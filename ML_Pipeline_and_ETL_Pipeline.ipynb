{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82a7cc0c-2834-41b0-8538-b3a679bed52d",
   "metadata": {},
   "source": [
    "An end-to-end machine learning (ML) model pipeline involves a series of steps that take raw data and transform it into a deployed, production-ready model. Here are the typical **end-to-end steps in an ML model pipeline**:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Problem Definition**\n",
    "- Understand the business problem.\n",
    "- Define the objective: classification, regression, clustering, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Data Collection**\n",
    "- Gather data from various sources (databases, APIs, web scraping, sensors).\n",
    "- Ensure data relevance and quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Data Preprocessing**\n",
    "- **Data Cleaning:** Handle missing values, remove duplicates, fix inconsistencies.\n",
    "- **Feature Engineering:** Create new features, encode categorical variables, normalize/scale data.\n",
    "- **Data Splitting:** Divide data into training, validation, and test sets.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Exploratory Data Analysis (EDA)**\n",
    "- Analyze data patterns and relationships using visualizations and statistics.\n",
    "- Understand feature distributions and correlations.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Model Selection**\n",
    "- Choose appropriate ML algorithms (e.g., Logistic Regression, Decision Trees, Random Forest, XGBoost, Neural Networks).\n",
    "- Consider baseline models first.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Model Training**\n",
    "- Train the model using the training dataset.\n",
    "- Monitor performance on validation data.\n",
    "- Use techniques like cross-validation if necessary.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Model Evaluation**\n",
    "- Evaluate using metrics like:\n",
    "  - **Classification:** Accuracy, Precision, Recall, F1-score, ROC-AUC\n",
    "  - **Regression:** RMSE, MAE, RÂ²\n",
    "- Tune hyperparameters for better performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Model Optimization**\n",
    "- **Hyperparameter tuning:** Grid search, Random search, Bayesian optimization.\n",
    "- **Feature selection/importance:** Reduce dimensionality.\n",
    "- **Regularization:** L1, L2 to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Model Deployment**\n",
    "- Use tools like **FastAPI**, **Flask**, or cloud platforms (Azure, AWS, GCP).\n",
    "- Deploy as:\n",
    "  - REST API\n",
    "  - Web service\n",
    "  - Embedded model in applications\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Monitoring & Maintenance**\n",
    "- Track model performance in production.\n",
    "- Retrain periodically with new data (model drift).\n",
    "- Set up alerting and logging.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Documentation & Reporting**\n",
    "- Document the entire process, assumptions, decisions.\n",
    "- Create dashboards for stakeholders (e.g., Power BI, Tableau).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234685ca-d07b-47d4-922e-83e7edeba7b5",
   "metadata": {},
   "source": [
    "Creating ETL (Extract, Transform, Load) pipelines using **Python**, **SQL**, and a **combination of both** is a common approach in data engineering. Let's break it down step by step with examples for each:\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. **ETL Pipeline Using Python Only**\n",
    "\n",
    "We'll use **Pandas**, **requests**, and **SQLAlchemy** for working with data and databases.\n",
    "\n",
    "### Example: Load CSV from web â†’ Transform â†’ Load to SQLite\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Step 1: Extract\n",
    "url = \"https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 2: Transform\n",
    "df.columns = ['Index', 'Height', 'Weight']\n",
    "df['Height'] = df['Height'].astype(float)\n",
    "df['Weight'] = df['Weight'].astype(float)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 3: Load\n",
    "engine = create_engine('sqlite:///students.db')\n",
    "df.to_sql('students_hw', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "print(\"ETL Pipeline with Python completed!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. **ETL Pipeline Using SQL Only**\n",
    "\n",
    "Use **stored procedures** or scripts to perform ETL directly inside a database.\n",
    "\n",
    "### Example (PostgreSQL Syntax):\n",
    "\n",
    "```sql\n",
    "-- Step 1: Extract (Assuming data already in raw_data table)\n",
    "-- Step 2: Transform and Load\n",
    "CREATE OR REPLACE FUNCTION etl_students_data()\n",
    "RETURNS void AS $$\n",
    "BEGIN\n",
    "    -- Create a clean table\n",
    "    DROP TABLE IF EXISTS students_cleaned;\n",
    "    CREATE TABLE students_cleaned AS\n",
    "    SELECT \n",
    "        id,\n",
    "        CAST(height AS FLOAT) AS height,\n",
    "        CAST(weight AS FLOAT) AS weight\n",
    "    FROM raw_data\n",
    "    WHERE height IS NOT NULL AND weight IS NOT NULL;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "-- Call the function\n",
    "SELECT etl_students_data();\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. **ETL Pipeline Using Python + SQL Together**\n",
    "\n",
    "This is a **hybrid** and most practical approach.\n",
    "\n",
    "### Example: Use Python for extract/transform and SQL for loading and further processing.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Step 1: Extract\n",
    "url = \"https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 2: Transform in Python\n",
    "df.columns = ['Index', 'Height', 'Weight']\n",
    "df['Height'] = pd.to_numeric(df['Height'], errors='coerce')\n",
    "df['Weight'] = pd.to_numeric(df['Weight'], errors='coerce')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 3: Load to SQL\n",
    "engine = create_engine('sqlite:///hybrid_pipeline.db')\n",
    "df.to_sql('students_raw', con=engine, if_exists='replace', index=False)\n",
    "\n",
    "# Step 4: Use SQL to create a cleaned version\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"DROP TABLE IF EXISTS students_cleaned\"))\n",
    "    conn.execute(text(\"\"\"\n",
    "        CREATE TABLE students_cleaned AS\n",
    "        SELECT Index, Height, Weight\n",
    "        FROM students_raw\n",
    "        WHERE Height > 0 AND Weight > 0\n",
    "    \"\"\"))\n",
    "\n",
    "print(\"Hybrid ETL Pipeline completed!\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary Table\n",
    "\n",
    "| Approach       | Tools Used            | Pros                                  | Use Case Example                     |\n",
    "|----------------|------------------------|---------------------------------------|--------------------------------------|\n",
    "| Python Only    | Pandas, SQLAlchemy     | Flexible, good for APIs/files         | ETL from CSV/JSON to DB              |\n",
    "| SQL Only       | Stored Procedures/SQL  | Fast, works inside DB                 | Transforming internal DB tables      |\n",
    "| Python + SQL   | Pandas + SQL Queries   | Best of both, scalable                | Complex logic + SQL performance      |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2393335-bf74-4e90-b489-ca02ba63bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
