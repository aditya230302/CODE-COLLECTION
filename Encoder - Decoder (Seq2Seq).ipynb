{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyODhwnkrTVdKfbpc2JLUSnP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Imports and Data Setup**"],"metadata":{"id":"gzYZ7Gftn89T"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0T1jKa-fxWn"},"outputs":[],"source":["import tensorflow as tf         # For building and training deep learning models\n","import numpy as np\n","from collections import Counter # Helps count word frequencies to build vocabulary.\n","import random                   # Used to decide whether to apply teacher forcing during training."]},{"cell_type":"code","source":["# This is a small parallel corpus of English-Hindi sentence pairs used for training.\n","data = [ (\"i am happy\",\"मैं खुश हूँ\"),\n","         (\"You are sad\",\"आप दुखी हैं\"),\n","         (\"she is tired\", \"वह थक गया है\"),\n","         (\"we are hungry\",\"हम भूखें है\"),\n","         (\"they are busy\",\"वे व्यस्त हैं\"),\n","         (\"i am cold\",\"मुझे ठंड लग रही है\"),\n","         (\"you are late\",\"तुम देरी से आए हो\"),\n","         (\"she is happy\", \"वह खुश है\"),\n","         (\"we are ready\",\"हम तैयार हैं\")]"],"metadata":{"id":"OqvzZxMjf4hZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[0][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"vrHrGN-pf7Rl","executionInfo":{"status":"ok","timestamp":1751030415825,"user_tz":-330,"elapsed":34,"user":{"displayName":"Aditya","userId":"16705183015007342629"}},"outputId":"0baf7b31-ed62-4ae9-fbd6-dc4192eaca3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'i am happy'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["data[0][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"B4LsuISKf9h7","executionInfo":{"status":"ok","timestamp":1751030415837,"user_tz":-330,"elapsed":8,"user":{"displayName":"Aditya","userId":"16705183015007342629"}},"outputId":"aba2b3e1-1f7a-421c-a08f-0ad8efac61f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'मैं खुश हूँ'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":24}]},{"cell_type":"markdown","source":["## **Vocabulary Builder**\n","\n","- Converts list of sentences into a vocabulary dictionary.\n","\n","- Adds special tokens:\n","  - `<PAD>`: for padding sequences to equal length.\n","  - `<SOS>`: start of sentence.\n","  - `<EOS>`: end of sentence."],"metadata":{"id":"vwrmA-qMoFkr"}},{"cell_type":"code","source":["# Function to build vocabulary from sentences\n","def build_vocab(sentences, lang):\n","  tokens = Counter()                # To count word frequencies\n","  for sentence in sentences:\n","    tokens.update(sentence.split()) # split by space and update token counts\n","\n","  vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2} # Initialize vocabulary with special tokens they will take the first 3 indices i.e 1,2,3\n","  for i,token in enumerate(tokens.keys(),3):   # assign index starting from 3 i.e. indice = 4\n","    vocab[token] = i\n","  return vocab"],"metadata":{"id":"oU4jv_4WhVw7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Build English and Hindi Vocabularies**\n","- Splits the parallel data into separate English and Hindi sentences and builds their respective vocabularies."],"metadata":{"id":"75Pp1zNeoXEM"}},{"cell_type":"code","source":["# Separate source (English) and target (Hindi) sentences\n","eng_sents = [pair[0] for pair in data]\n","hin_sent = [pair[1] for pair in data]\n","\n","# Build vocabularies\n","eng_vocab = build_vocab(eng_sents, \"eng\")\n","hin_vocab = build_vocab(hin_sent, \"hin\")\n","\n","print(f\"English Vocabulary size : {len(eng_vocab)}\")\n","print(f\"Hindi Vocabulary size : {len(hin_vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jJXqIVuCoXiw","executionInfo":{"status":"ok","timestamp":1751030415877,"user_tz":-330,"elapsed":32,"user":{"displayName":"Aditya","userId":"16705183015007342629"}},"outputId":"32a1df90-c5f3-447e-a6b2-f6e1063b0e51"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["English Vocabulary size : 20\n","Hindi Vocabulary size : 27\n"]}]},{"cell_type":"markdown","source":["## **Convert Sentences to Indices**\n","- Converts sentence into list of token IDs.\n","- Surrounds sentence with `<SOS>` and `<EOS>`.\n","- The EOS token signals to the encoder when it has processed the entire input sequence, and it allows the decoder to know when to stop generating output. The SOS token provides a starting point for the decoder, initiating the output generation process."],"metadata":{"id":"rxRWN_gcodSw"}},{"cell_type":"code","source":["# Function to convert sentence into list of word indices\n","def sentence_to_indices(sent, vocab):\n","  indices = [vocab.get(token, vocab.get(\"<UNK>\",0)) for token in sent.split()] # convert words to indices\n","  '''\n","  This looks up each word (token) in the vocabulary dictionary vocab.\n","  If the word is in the vocabulary, it returns its index.\n","  If the word is not in the vocabulary:\n","  It tries to return the index of \"<UNK>\" (unknown word token).\n","  If <UNK> is not even in the vocab, it defaults to 0 (usually <PAD> index).\n","  '''\n","  indices = [vocab['<SOS>']] + indices + [vocab['<EOS>']]  # add <SOS> and <EOS>\n","  return indices"],"metadata":{"id":"FUUwrU-WoicX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Padding Sequences**\n","- Converts all English (source) and Hindi (target) sentences into padded tensors"],"metadata":{"id":"8DVDTORdokXI"}},{"cell_type":"code","source":["# Function to convert all sentences and pad them to same length\n","def prepare_data(data, eng_vocab, hin_vocab):\n","  # convert english and hindi sentences to indices\n","  src_data = [sentence_to_indices(pair[0], eng_vocab) for pair in data]\n","  tgt_data = [sentence_to_indices(pair[1], hin_vocab) for pair in data]\n","\n","  # Pad the sequences with <PAD> token to make them equal length\n","  src_padded = tf.keras.preprocessing.sequence.pad_sequences(src_data, padding='post', value=eng_vocab['<PAD>'])\n","  tgt_padded = tf.keras.preprocessing.sequence.pad_sequences(tgt_data, padding='post', value=hin_vocab['<PAD>'])\n","  return src_padded, tgt_padded\n","\n","# Prepare padded data\n","src_data, tgt_data = prepare_data(data, eng_vocab, hin_vocab)\n","\n","print(f\"Source data shape : {src_data.shape}\")\n","print(f\"Target data shape : {tgt_data.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kMWnLU-Sok2d","executionInfo":{"status":"ok","timestamp":1751030415916,"user_tz":-330,"elapsed":16,"user":{"displayName":"Aditya","userId":"16705183015007342629"}},"outputId":"a4ab1f01-9f0d-4bd0-a8b2-de733c40bf9c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Source data shape : (9, 5)\n","Target data shape : (9, 7)\n"]}]},{"cell_type":"markdown","source":["## **ENCODER: Converts input sentence to context vector**\n","- Encodes input sentence into a context vector (hidden & cell state).\n","\n","**Encoder**\n","\n","* Reads the **input sequence** word by word.\n","* Produces a **context vector** (i.e., final hidden + cell state).\n","* Think of it like compressing all the meaning of the input into a fixed-size vector."],"metadata":{"id":"0r9f_ttHosQ5"}},{"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","    def __init__(self, input_size, embedding_size, hidden_size):\n","        super(Encoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(input_size, embedding_size)\n","        # Embedding layer for source (input) tokens\n","\n","        self.lstm = tf.keras.layers.LSTM(\n","            hidden_size, return_sequences=True, return_state=True\n","        )\n","        # LSTM processes embedded input sequence.\n","        # return_sequences=True: return output at all time steps (we don’t use it here).\n","        # return_state=True: return final hidden and cell state (context vector).\n","\n","    def call(self, x):\n","        \"\"\"\n","        Forward pass for the encoder.\n","\n","        x: input sequence [batch_size, seq_len] — token indices of source sentence\n","        \"\"\"\n","        embedded = self.embedding(x)\n","        # Convert input tokens to dense vector embeddings: [batch_size, seq_len, embedding_size]\n","\n","        _, hidden, cell = self.lstm(embedded)\n","        # LSTM processes the entire sequence and returns:\n","        #   - hidden: final hidden state\n","        #   - cell: final cell state\n","        #   - we discard the full sequence output (_) since decoder only needs the final context\n","\n","        return hidden, cell\n","        # Return the context vector (hidden & cell) for initializing decoder"],"metadata":{"id":"yAZwj6eQooJy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **DECODER: Generates target sentence from context vector**\n","- Takes in the previous token and generates the next token probability distribution.\n","\n","**Decoder**\n","\n","* Takes the **context vector** and generates the **output sequence**, one word at a time.\n","* It uses **previous outputs** to predict the **next word**."],"metadata":{"id":"Xl-ryvUQownF"}},{"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, output_size, embedding_size, hidden_size):\n","        super(Decoder, self).__init__()\n","        self.embedding = tf.keras.layers.Embedding(output_size, embedding_size)\n","        # Embedding layer converts target tokens into dense vectors of size [embedding_size]\n","\n","        self.lstm = tf.keras.layers.LSTM(\n","            hidden_size, return_sequences=True, return_state=True\n","        )\n","        # LSTM layer processes embeddings.\n","        # return_sequences=True: returns output at each time step\n","        # return_state=True: returns final hidden & cell state (needed for next decoding step)\n","\n","        self.fc = tf.keras.layers.Dense(output_size)\n","        # Final dense layer maps LSTM outputs to vocabulary size (used to predict next token)\n","\n","    def call(self, x, hidden, cell):\n","        \"\"\"\n","        Performs a single forward pass for one decoding step.\n","\n","        x      : input token(s) to the decoder [batch_size, 1]\n","        hidden : hidden state from previous time step or encoder\n","        cell   : cell state from previous time step or encoder\n","        \"\"\"\n","        embedded = self.embedding(x)\n","        # Converts token indices into dense vector representation: [batch_size, 1, embedding_size]\n","\n","        lstm_out, hidden, cell = self.lstm(embedded, initial_state=[hidden, cell])\n","        # LSTM processes the embedded input using the given hidden and cell state\n","        # Outputs:\n","        #   - lstm_out: output at current step for each batch\n","        #   - hidden, cell: updated states to be used in the next step\n","\n","        output = self.fc(lstm_out)\n","        # Apply linear layer to LSTM output to produce logits for each token in the vocabulary\n","\n","        return output, hidden, cell\n","        # Return predictions and updated LSTM states"],"metadata":{"id":"Y9Dd2IfFozek"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Summary till now**\n","\n","| Component                 | Purpose                                                                                   |\n","| ------------------------- | ----------------------------------------------------------------------------------------- |\n","| **Encoder**               | Converts the entire source sentence into a context vector (final hidden and cell states). |\n","| **Decoder**               | Generates the output sentence one token at a time, using the context from the encoder.    |\n","| **Embedding layers**      | Convert word indices to dense vector representations.                                     |\n","| **LSTM**                  | Captures temporal dependencies and remembers long-term patterns.                          |\n","| **Dense layer (Decoder)** | Maps LSTM output to vocabulary distribution for next-token prediction.                    |\n"],"metadata":{"id":"QPDcLMplh-sh"}},{"cell_type":"markdown","source":["---\n","## **SEQ2SEQ WRAPPER: Connects Encoder and Decoder**\n","- Implements auto-regressive decoding with optional teacher forcing.\n","- During inference, it feeds predicted token back to decoder.\n","\n","🔍 What is a **Seq2Seq (Sequence-to-Sequence) Model**?\n","\n","---\n","\n","A **Seq2Seq model** is a type of neural network architecture **designed to transform one sequence into another**. It's widely used in tasks where **input and output are both sequences**, but not necessarily of the same length.\n","\n","---\n","\n","🔄 **Basic Architecture: Encoder–Decoder**\n","\n","```text\n","INPUT SEQUENCE              OUTPUT SEQUENCE\n","-----------------          -------------------\n","\"I am happy\"    ───▶      [\"<SOS>\", \"मैं\", \"खुश\", \"हूँ\", \"<EOS>\"]\n","```\n","\n","---\n","\n","🧩 **Detailed Workflow**\n","\n","```text\n","[“i”, “am”, “happy”]\n","       |\n","     Encoder\n","       ↓\n","[context vector (hidden + cell)]\n","       ↓\n","     Decoder\n","       |\n","[\"<SOS>\" → \"मैं\" → \"खुश\" → \"हूँ\" → \"<EOS>\"]\n","```\n","\n","---\n","\n","🧠 **How It Works Under the Hood**\n","\n","1. **Input Processing:**\n","\n","   * Input is tokenized and converted to numbers.\n","   * Padded to fixed length (e.g., `[1, 3, 4, 2, 0]`).\n","\n","2. **Encoder (LSTM/RNN):**\n","\n","   * Reads input and produces `hidden_state` and `cell_state`.\n","\n","3. **Decoder (LSTM):**\n","\n","   * Uses `<SOS>` as first input.\n","   * Predicts one word at a time using:\n","\n","     * Previous output word\n","     * Previous hidden/cell state\n","     * Context vector from encoder\n","\n","4. **Training:**\n","\n","   * Uses **Teacher Forcing** to feed the correct previous word during training.\n","---\n","🔥 Enhancements Over Basic Seq2Seq\n","\n","| Technique                 | Purpose                                                                              |\n","| ------------------------- | ------------------------------------------------------------------------------------ |\n","| **Attention**             | Helps decoder focus on relevant parts of input (solves bottleneck of context vector) |\n","| **Beam Search**           | Generates better translations by considering multiple predictions                    |\n","| **Bidirectional Encoder** | Allows encoder to look at future as well as past                                     |\n","| **Transformer**           | Replaces RNNs/LSTMs with self-attention (used in BERT, GPT, etc.)                    |\n","\n","---\n","\n","✅ Summary\n","\n","| Term          | Meaning                                                  |\n","| ------------- | -------------------------------------------------------- |\n","| **Seq2Seq**   | Neural architecture for sequence input → sequence output |\n","| **Encoder**   | Reads input and summarizes as context vector             |\n","| **Decoder**   | Generates output sequence using context                  |\n","| **Use cases** | Translation, Chatbots, Summarization, etc.               |\n","\n","---\n"],"metadata":{"id":"UJ0twENJo054"}},{"cell_type":"code","source":["class Seq2Seq(tf.keras.Model):  # Creates a custom model by extending tf.keras.Model.\n","    def __init__(self, encoder, decoder):\n","        super(Seq2Seq, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        \"\"\"\n","        Accepts two prebuilt models: an Encoder and a Decoder.\n","        Stores them as class attributes.\n","        \"\"\"\n","\n","    def call(self, inputs, training=False, teacher_forcing_ratio=0.5):\n","        \"\"\"\n","        This is the forward pass.\n","        inputs: a tuple/list containing:\n","                src – source (input) sequences (English)\n","                tgt – target (output) sequences (Hindi)\n","        training: if True, apply teacher forcing\n","        teacher_forcing_ratio: controls how often we use actual ground truth instead of the model's previous prediction.\n","        \"\"\"\n","        src, tgt = inputs  # Unpack input pair\n","        batch_size = tf.shape(src)[0]\n","        tgt_len = tf.shape(tgt)[1]\n","        tgt_vocab_size = len(hin_vocab)\n","\n","        hidden, cell = self.encoder(src)  # Encode the source sentence\n","\n","        outputs = []\n","        input_token = tgt[:, 0:1]  # Start with <SOS>\n","\n","        for t in range(1, tgt_len): # Loop Through Each Time Step in Decoder\n","            # Skip the first token <SOS> and loop through rest of tgt\n","            output, hidden, cell = self.decoder(input_token, hidden, cell)  # Decode step\n","            outputs.append(output) # Store the decoder's output for the current time step.\n","\n","            # Teacher forcing: sometimes use ground truth next token\n","            if training: # If training is True\n","                teacher_force = random.random() < teacher_forcing_ratio\n","                input_token = tgt[:, t:t+1] if teacher_force else tf.argmax(output, axis=-1, output_type=tf.int32)\n","            else:\n","                input_token = tf.argmax(output, axis=-1, output_type=tf.int32)\n","\n","        return tf.concat(outputs, axis=1) if outputs else tf.zeros((batch_size, 1, tgt_vocab_size))\n","        \"\"\"\n","        Combine all time step outputs into a single tensor.\n","        Each output is of shape: [batch_size, 1, vocab_size]\n","        Final output shape: [batch_size, seq_len - 1, vocab_size]\n","        \"\"\"\n"],"metadata":{"id":"PZWuAdM9o5lf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Training Function**\n","- Standard training loop with masked loss to ignore `<PAD>` tokens."],"metadata":{"id":"VBIovLGro7FG"}},{"cell_type":"code","source":["def train(model, src_data, tgt_data, epochs=100):\n","    \"\"\"\n","    Trains the Seq2Seq model on the provided source and target data.\n","\n","    model      : instance of the Seq2Seq model\n","    src_data   : padded source sequences (e.g., English sentences as indices)\n","    tgt_data   : padded target sequences (e.g., Hindi sentences as indices)\n","    epochs     : number of training iterations\n","    \"\"\"\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n","    # Adam optimizer is used for updating weights\n","\n","    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","    \"\"\"\n","    Loss function:\n","    - SparseCategoricalCrossentropy is used because targets are integers (not one-hot vectors).\n","    - from_logits=True because the decoder's output layer does not use softmax.\n","    - reduction='none' allows us to compute loss per token and apply masking manually.\n","    \"\"\"\n","\n","    def train_step(src, tgt):\n","        \"\"\"\n","        A single training step for one batch (entire dataset here).\n","        src: batch of source sequences\n","        tgt: batch of target sequences\n","        \"\"\"\n","        with tf.GradientTape() as tape:\n","            outputs = model([src, tgt], training=True, teacher_forcing_ratio=1.0)\n","            # Forward pass with teacher forcing enabled (always use ground truth tokens).\n","\n","            target_labels = tgt[:, 1:]\n","            # Remove the <SOS> token from the targets (as decoder should predict from position 1 onward)\n","\n","            outputs = outputs[:, :tf.shape(target_labels)[1], :]\n","            # Ensure decoder outputs match the shape of the target (align dimensions)\n","\n","            mask = tf.cast(target_labels != hin_vocab[\"<PAD>\"], tf.float32)\n","            # Create a binary mask to ignore loss from <PAD> tokens.\n","\n","            loss = loss_fn(target_labels, outputs)\n","            # Compute the loss between actual tokens and predicted outputs.\n","\n","            loss *= mask\n","            # Apply the mask to ignore padding tokens in loss calculation.\n","\n","            total_loss = tf.reduce_sum(loss)\n","            total_tokens = tf.reduce_sum(mask)\n","            mean_loss = total_loss / (total_tokens + 1e-8)\n","            # Compute average loss per token (add small epsilon to avoid divide-by-zero)\n","\n","        gradients = tape.gradient(mean_loss, model.trainable_variables)\n","        # Compute gradients of loss w.r.t. model parameters\n","\n","        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","        # Update model parameters using optimizer and gradients\n","\n","        return mean_loss\n","        # Return average loss for logging\n","\n","    print(\"Starting Training....\")\n","    for epoch in range(epochs):\n","        loss = train_step(src_data, tgt_data)  # Train for one epoch on full data\n","        if epoch % 10 == 0:\n","            print(f\"Epoch {epoch}, Loss : {loss.numpy():.4f}\")  # Print loss every 10 epochs\n","\n","    print(\"Training completed\")  # Training loop ends"],"metadata":{"id":"BRs4TRlXo-dc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Model Initialization**"],"metadata":{"id":"VE88eApko_wZ"}},{"cell_type":"code","source":["input_size = len(eng_vocab)\n","output_size = len(hin_vocab)\n","embed_size = 50\n","hidden_size = 100\n","\n","print(f\"Initializing model with :\")\n","print(f\"Input vocab size : {input_size}\")\n","print(f\"Output vocab size : {output_size}\")\n","print(f\"Embedding size : {embed_size}\")\n","print(f\"Hidden size (lstm) : {hidden_size}\")\n","\n","# Create encoder-decoder and seq2seq model\n","encoder = Encoder(input_size, embed_size, hidden_size)\n","decoder = Decoder(output_size, embed_size, hidden_size)\n","model = Seq2Seq(encoder, decoder)\n","\n","# Dummy call to build the model\n","dummy_src = tf.zeros((1,5), dtype=tf.int32)\n","dummy_tgt = tf.zeros((1,5), dtype=tf.int32)\n","_ = model([dummy_src, dummy_tgt], training=False)\n","\n","# Train the model\n","train(model, src_data, tgt_data, epochs=50)"],"metadata":{"id":"psL9FenGpAo5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751030430512,"user_tz":-330,"elapsed":14546,"user":{"displayName":"Aditya","userId":"16705183015007342629"}},"outputId":"1314775e-b080-4651-cea7-0f57d3788155"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing model with :\n","Input vocab size : 20\n","Output vocab size : 27\n","Embedding size : 50\n","Hidden size (lstm) : 100\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:1387: UserWarning: Layer 'seq2_seq_1' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n","1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n","2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n","Exception encountered: '''SymbolicTensor' object cannot be interpreted as an integer''\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:393: UserWarning: `build()` was called on layer 'seq2_seq_1', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Starting Training....\n","Epoch 0, Loss : 3.2954\n","Epoch 10, Loss : 2.2147\n","Epoch 20, Loss : 0.8862\n","Epoch 30, Loss : 0.1943\n","Epoch 40, Loss : 0.0270\n","Training completed\n"]}]},{"cell_type":"markdown","source":["## **Translation Function**\n","- Takes an English sentence, converts it into tokens, encodes it, and decodes into Hindi using the trained model."],"metadata":{"id":"5s2Nezt0pCWN"}},{"cell_type":"code","source":["def translate(model, sentence, eng_vocab, hin_vocab, max_len=15):\n","    \"\"\"\n","    Translates an English sentence into Hindi using the trained Seq2Seq model.\n","\n","    model     : Trained Seq2Seq model containing encoder and decoder\n","    sentence  : English sentence as input string\n","    eng_vocab : Vocabulary mapping for English words to indices\n","    hin_vocab : Vocabulary mapping for Hindi words to indices\n","    max_len   : Maximum number of tokens to decode (prevents infinite loops)\n","    \"\"\"\n","\n","    print(f\"Translating your sentence ---> : {sentence}\")\n","\n","    tokens = sentence.split()\n","    # Tokenize the input sentence by splitting on spaces\n","\n","    indices = [eng_vocab[\"<SOS>\"]] + [eng_vocab.get(token, 0) for token in tokens] + [eng_vocab[\"<EOS>\"]]\n","    # Convert tokens to indices using English vocab\n","    # Add <SOS> at the start and <EOS> at the end\n","    # If word not found, use 0 (usually <PAD> or <UNK>)\n","\n","    print(f\"Input tokens :{tokens}\")\n","    print(f\"Input Indices : {indices}\")\n","\n","    src_tensor = tf.convert_to_tensor([indices], dtype=tf.int32)\n","    # Convert list of indices into a tensor of shape [1, sequence_length] (batch size = 1)\n","\n","    hidden, cell = model.encoder(src_tensor)\n","    # Pass the input tensor to the encoder to get the context vector (hidden and cell states)\n","\n","    print(f\"Encoded to context vector of shape : {hidden.shape}\")\n","\n","    input_token = tf.convert_to_tensor([[hin_vocab['<SOS>']]], dtype=tf.int32)\n","    # Initialize decoder input with <SOS> token from Hindi vocab (shape: [1,1])\n","\n","    output_tokens = []\n","    # List to store predicted token indices\n","\n","    print(\"Decoding Steps: \")\n","\n","    for step in range(max_len):\n","        # Loop to generate each token of the translation (max_len is the upper limit)\n","\n","        output, hidden, cell = model.decoder(input_token, hidden, cell)\n","        # Pass input token and context vector to decoder\n","        # Receive output logits and updated hidden, cell states\n","\n","        predicted_token = tf.argmax(output, axis=-1).numpy()[0, 0]\n","        # Choose the token index with highest probability from output logits\n","\n","        inv_hin_vocab = {v: k for k, v in hin_vocab.items()}\n","        # Create a reverse vocabulary to convert indices back to Hindi words\n","\n","        predicted_word = inv_hin_vocab.get(predicted_token, \"<UNK>\")\n","        # Get word corresponding to predicted token index (use <UNK> if missing)\n","\n","        print(f\"  Step {step+1}:{predicted_word} (token {predicted_token})\")\n","\n","        if predicted_token == hin_vocab[\"<EOS>\"]:\n","            # If end-of-sentence token is generated, stop decoding\n","            print(\"   Reached EOS token, stopping the process!\")\n","            break\n","\n","        output_tokens.append(predicted_token)\n","        # Add predicted token to the output sequence\n","\n","        input_token = tf.convert_to_tensor([[predicted_token]], dtype=tf.int32)\n","        # Set predicted token as input to the decoder for the next step\n","\n","    translated_words = [inv_hin_vocab.get(idx, \"<UNK>\") for idx in output_tokens]\n","    # Convert list of predicted token indices to words\n","\n","    translation = \" \".join(translated_words)\n","    # Join words to form final translated sentence\n","\n","    print(f\"Final Translation : {translation}\")\n","    return translation\n","    # Return the translated Hindi sentence as output"],"metadata":{"id":"1dDcadPvpC2z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**summary of above cell**\n","\n","| Feature                  | Description                                 |\n","| ------------------------ | ------------------------------------------- |\n","| `model.encoder()`        | Converts input sentence into context vector |\n","| `model.decoder()`        | Decodes one word at a time                  |\n","| `argmax`                 | Chooses highest probability word            |\n","| `input_token` loop       | Sequential decoding                         |\n","| `<SOS>` / `<EOS>` tokens | Manage sentence start/end                   |\n","| `max_len`                | Limits the translation length               |\n"],"metadata":{"id":"THwqLMHzi-EK"}},{"cell_type":"markdown","source":["**Testing Translation**"],"metadata":{"id":"TZOBGIqKpHQ2"}},{"cell_type":"code","source":["test_sentences = [\"i am happy\", \"You are sad\", \"we are ready\", \"tired is hungry\"]\n","\n","for sentence in test_sentences:\n","    expected = next((hin for eng, hin in data if eng == sentence), None)\n","    translation = translate(model, sentence, eng_vocab, hin_vocab)\n","    print(f\"Expected : '{expected}'\")\n","    print(f\"Got :      '{translation}'\")\n","    print(\"**\"*40)"],"metadata":{"id":"DGT3F5S1pHzi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751030430874,"user_tz":-330,"elapsed":320,"user":{"displayName":"Aditya","userId":"16705183015007342629"}},"outputId":"11e443c0-4971-4042-8e85-d5dc882c9a1b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Translating your sentence ---> : i am happy\n","Input tokens :['i', 'am', 'happy']\n","Input Indices : [1, 3, 4, 5, 2]\n","Encoded to context vector of shape : (1, 100)\n","Decoding Steps: \n","  Step 1:मैं (token 3)\n","  Step 2:खुश (token 4)\n","  Step 3:हूँ (token 5)\n","  Step 4:<EOS> (token 2)\n","   Reached EOS token, stopping the process!\n","Final Translation : मैं खुश हूँ\n","Expected : 'मैं खुश हूँ'\n","Got :      'मैं खुश हूँ'\n","********************************************************************************\n","Translating your sentence ---> : You are sad\n","Input tokens :['You', 'are', 'sad']\n","Input Indices : [1, 6, 7, 8, 2]\n","Encoded to context vector of shape : (1, 100)\n","Decoding Steps: \n","  Step 1:आप (token 6)\n","  Step 2:दुखी (token 7)\n","  Step 3:हैं (token 8)\n","  Step 4:<EOS> (token 2)\n","   Reached EOS token, stopping the process!\n","Final Translation : आप दुखी हैं\n","Expected : 'आप दुखी हैं'\n","Got :      'आप दुखी हैं'\n","********************************************************************************\n","Translating your sentence ---> : we are ready\n","Input tokens :['we', 'are', 'ready']\n","Input Indices : [1, 12, 7, 19, 2]\n","Encoded to context vector of shape : (1, 100)\n","Decoding Steps: \n","  Step 1:हम (token 13)\n","  Step 2:तैयार (token 26)\n","  Step 3:हैं (token 8)\n","  Step 4:<EOS> (token 2)\n","   Reached EOS token, stopping the process!\n","Final Translation : हम तैयार हैं\n","Expected : 'हम तैयार हैं'\n","Got :      'हम तैयार हैं'\n","********************************************************************************\n","Translating your sentence ---> : tired is hungry\n","Input tokens :['tired', 'is', 'hungry']\n","Input Indices : [1, 11, 10, 13, 2]\n","Encoded to context vector of shape : (1, 100)\n","Decoding Steps: \n","  Step 1:वह (token 9)\n","  Step 2:थक (token 10)\n","  Step 3:गया (token 11)\n","  Step 4:है (token 12)\n","  Step 5:<EOS> (token 2)\n","   Reached EOS token, stopping the process!\n","Final Translation : वह थक गया है\n","Expected : 'None'\n","Got :      'वह थक गया है'\n","********************************************************************************\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"n9tlkIfNhL3h"},"execution_count":null,"outputs":[]}]}