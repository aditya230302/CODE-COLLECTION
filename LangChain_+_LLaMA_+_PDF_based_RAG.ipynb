{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "2qFG7Gp32M2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ LangChain: Framework to build modular, chainable LLM pipelines (RAG, QA, chatbots, etc.)\n",
        "!pip install langchain -q\n",
        "\n",
        "# ‚úÖ langchain-community: Extra tools, connectors, and model wrappers not in core LangChain\n",
        "!pip install langchain-community -q\n",
        "\n",
        "# ‚úÖ pypdf: To read and extract text from PDF documents\n",
        "!pip install pypdf -q\n",
        "\n",
        "# ‚úÖ docarray: A lightweight vector storage library for storing and retrieving document embeddings\n",
        "!pip install docarray -q\n",
        "\n",
        "# ‚úÖ sentence-transformers: Provides pretrained models like MiniLM to convert sentences into dense vectors (embeddings)\n",
        "!pip install sentence-transformers -q\n",
        "\n",
        "# ‚úÖ huggingface_hub: Interface to download models/files from Hugging Face model hub (e.g., GGUF models)\n",
        "!pip install huggingface_hub -q\n",
        "\n",
        "# ‚úÖ llama-cpp-python: Python bindings to run local GGUF-format LLaMA models using C++ backend (llama.cpp)\n",
        "!pip install llama-cpp-python -q\n",
        "\n",
        "# ‚úÖ apt-get update: Updates package index\n",
        "# ‚úÖ git: Required to clone the llama.cpp GitHub repository\n",
        "# ‚úÖ cmake + build-essential: Required to build the C++ code for llama.cpp\n",
        "!apt-get update && apt-get install -y git cmake build-essential -q\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3oX3KyCMtQd",
        "outputId": "af9a370d-66ad-40ee-ebd5-e396330e4860"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.83)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Connected to cloud.r-project.org (108.157.173.89)] [W\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rGet:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 133 kB in 2s (69.8 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 40 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Clone and compile LLaMa.cpp**"
      ],
      "metadata": {
        "id": "WtMZ5w752RgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üì• Clone the llama.cpp repository from GitHub (required for running local LLaMA models efficiently)\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "# üìÅ Move into the llama.cpp directory\n",
        "%cd llama.cpp\n",
        "\n",
        "# üõ†Ô∏è Compile llama.cpp using all available processor cores (parallel build)\n",
        "!make -j$(nproc)\n",
        "\n",
        "# üîô Move back to the root project directory\n",
        "%cd .."
      ],
      "metadata": {
        "id": "9z8Nj2EkTSg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a69994-a74e-4b20-cac2-cd5ea041d383"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
            "/content/llama.cpp\n",
            "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ For downloading the LLaMA GGUF model from Hugging Face\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# üß† You are downloading a quantized 3B parameter instruction-tuned model (Q4_K_M)\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=\"bartowski/Llama-3.2-3B-Instruct-GGUF\",\n",
        "    filename=\"Llama-3.2-3B-Instruct-Q4_K_M.gguf\"\n",
        ")"
      ],
      "metadata": {
        "id": "urxJROPNTcnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bae694e-09c2-495c-8a05-3672cfa2e032"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Uploading Pdf**"
      ],
      "metadata": {
        "id": "ScPLASTj2d9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ For uploading files (PDF) from your local system in Google Colab\n",
        "from google.colab import files\n",
        "\n",
        "# üßæ User uploads the file ‚Üí returned as a dict {filename: file_data}\n",
        "uploaded = files.upload()\n",
        "\n",
        "# üõ£Ô∏è Get the uploaded PDF file path\n",
        "pdf_path = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "LBIEIq4kTdz_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "2a62f135-d976-4c9e-b11a-9391be697ad8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-342d3d3f-e818-4e71-bb9f-874b4dccc732\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-342d3d3f-e818-4e71-bb9f-874b4dccc732\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving blood test.pdf to blood test.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initializing LLaMA model**"
      ],
      "metadata": {
        "id": "SYDACAlA2lM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ For checking if GPU is available (used in LLaMA config)\n",
        "import torch\n",
        "\n",
        "# üß† Import LlamaCpp from langchain_community (to use local LLaMA model)\n",
        "# ‚úÖ LangChain interface to LLaMA.cpp (loads and queries GGUF models locally)\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# üß† Initialize the LLaMA model with context size, GPU settings, temperature, etc.\n",
        "llm = LlamaCpp(\n",
        "    model_path=model_path,\n",
        "    n_ctx=5000,  # Max context window size (prompt + response)\n",
        "    n_gpu_layers=33 if torch.cuda.is_available() else 0,  # Use GPU if available\n",
        "    temperature=0.7,  # 0 = deterministic; 0.7 = more creative/random\n",
        "    verbose=False  # Less logging noise\n",
        ")"
      ],
      "metadata": {
        "id": "EuzfKSCo0Ynm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "194d9c28-e163-4134-a92c-3964e7c7abee"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_ctx_per_seq (5000) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **PDF to Doc and pages extraction**"
      ],
      "metadata": {
        "id": "oGF-Xsrb2wgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ LangChain PDF loader: Splits and converts PDF pages into documents\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# ‚úÖ LangChain vector storage: Stores document embeddings in memory (no DB required)\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "# ‚úÖ Sentence embedding wrapper using Hugging Face (MiniLM, BERT, etc.)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# ‚úÖ Used to define the prompt format used by the LLM\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# ‚úÖ Pass-through node for dynamic chain input (like a question)\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# ‚úÖ Parses the LLM output and returns as a plain string\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# üìÑ Load your PDF and split it page-wise (each page becomes a document)\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "MP9yfMtO0eO-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Embeddings, VectorStore, and Retriever**"
      ],
      "metadata": {
        "id": "1WiMUlmU2_TG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üî§ Convert each sentence/page into embeddings using MiniLM (fast + accurate)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# üóÉÔ∏è Store vectorized documents in memory (no external DB needed)\n",
        "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
        "\n",
        "# üîé Convert the vector store into a retriever\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "xf64NUqK0kMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef5c0d4-e295-42c4-8f10-567890b3d94b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-7-3636932763.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìã **Define the prompt template for LLM**"
      ],
      "metadata": {
        "id": "fvV9gmDN3IUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This tells the LLM: \"Use this context to answer this question\"\n",
        "template = \"\"\"Use the following context to answer the question:\n",
        "{context}\n",
        "\n",
        "Question : {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "\n",
        "# Create a LangChain PromptTemplate object\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "d85NcsDs1c0_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîó **Build a RAG chain**\n",
        "Steps:\n",
        "1. Use retriever to get documents using question\n",
        "2. Format the prompt\n",
        "3. Run through the LLaMA model\n",
        "4. Parse the output as clean text"
      ],
      "metadata": {
        "id": "I1HaiT2y3OEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()  # Clean output (strip, format)\n",
        ")"
      ],
      "metadata": {
        "id": "nNIVaN-t1e4o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üí¨ Try a simple test question (control prompt)"
      ],
      "metadata": {
        "id": "WH6bHgSE3fwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"how to make pizza\"\n",
        "\n",
        "# üöÄ Run the RAG pipeline\n",
        "response = chain.invoke(question) # Runs the full Retrieval + Generation pipeline\n",
        "\n",
        "# üì¢ Print the answer\n",
        "print(f\"Answer: {response}\")"
      ],
      "metadata": {
        "id": "415Z3Nax1hD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a42d94e-c707-4b25-94d4-b3fa0d7f9655"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: 1. Make the dough: In a large mixing bowl, combine 2 cups of warm water, 2 teaspoons of active dry yeast, and 3 tablespoons of olive oil. Mix until the yeast is dissolved, then add 3 cups of all-purpose flour and continue to mix until the dough becomes smooth and elastic. Knead the dough for 5-10 minutes, until it becomes smooth and elastic.\n",
            "\n",
            "2. Prepare the sauce: In a blender or food processor, combine 1 cup of crushed tomatoes, 1 tablespoon of olive oil, 2 cloves of minced garlic, and 1 teaspoon of dried oregano. Blend until smooth.\n",
            "\n",
            "3. Add cheese to the pizza: Grate 8 ounces of mozzarella cheese and sprinkle it evenly over the sauce.\n",
            "\n",
            "4. Add toppings to your pizza: Choose your favorite toppings, such as pepperoni, sausage, mushrooms, onions, bell peppers, olives, etc. Sprinkle these toppings evenly over the cheese.\n",
            "\n",
            "5. Bake the pizza in the oven: Preheat your oven to 425 degrees Fahrenheit (220 degrees Celsius). Place the pizza on a baking sheet or pizza stone and bake for 12-15 minutes, until the crust is golden brown and the cheese is melted and bubbly.\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚è±Ô∏è Performance Benchmark: Ask a deeper question & measure response time\n",
        "import time\n",
        "\n",
        "st = time.time()\n",
        "\n",
        "# üß† Real semantic question based on PDF content\n",
        "question = \"What is the main topic of the PDF? Can you tell me the top 2 important points from the document?\"\n",
        "\n",
        "# üöÄ Invoke the same RAG chain\n",
        "response = chain.invoke(question)\n",
        "\n",
        "# üßæ Output the answer\n",
        "print(f\"Answer: {response}\")\n",
        "\n",
        "ed = time.time()\n",
        "print(f\"Inference took: {(ed - st):.3f} seconds\")"
      ],
      "metadata": {
        "id": "I83D6hA61i5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3c3ab0-3a2d-4b0b-d0dd-1e0643156321"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  The main topic of the PDF is a \"Complete Blood Count (CBC)\" report.  Based on this, I can tell you that the top 2 important points from the document are:\n",
            "\n",
            "1.  **Test Results**:  The blood test results show the following values:\n",
            "    *   Hemoglobin: 12.00 - 15.00 g/dL\n",
            "    *   Packed Cell Volume (PCV): 36.00 - 46.00%\n",
            "2.  **Treatment Goals**:  According to the Lipid Association of India 2020 guidelines, the treatment goals for patients with high cholesterol levels are:\n",
            "    *   LDL Cholesterol: <100 mg/dL\n",
            "    *   Non-HDL Cholesterol: <130 mg/dL\n",
            "Inference took: 846.309 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Adding GuardRails**\n",
        "\n",
        "* **Guardrails** add safety, limit abuse, and increase trustworthiness.\n",
        "* The **diagram** gives a high-level overview of the complete data flow in your app.\n"
      ],
      "metadata": {
        "id": "ZIiy9q1H1yVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_guardrails(response: str, keywords=None, max_length=1000):\n",
        "    \"\"\"\n",
        "    Guardrails for basic safety and quality control.\n",
        "    - Check if response is empty\n",
        "    - Check for restricted keywords (toxic/offensive)\n",
        "    - Check for hallucination patterns\n",
        "    \"\"\"\n",
        "    # Empty or null response\n",
        "    if not response.strip():\n",
        "        return \"‚ö†Ô∏è The model did not generate a response. Please try a different question.\"\n",
        "\n",
        "    # Keyword filtering (basic safety)\n",
        "    if keywords:\n",
        "        for word in keywords:\n",
        "            if word.lower() in response.lower():\n",
        "                return f\"‚ö†Ô∏è Unsafe content detected: '{word}'. Response blocked.\"\n",
        "\n",
        "    # Response too long (length control)\n",
        "    if len(response) > max_length:\n",
        "        return response[:max_length] + \"\\n\\n‚ö†Ô∏è Truncated due to length.\"\n",
        "\n",
        "    # If passed all guardrails\n",
        "    return response"
      ],
      "metadata": {
        "id": "jrF_hGh910WP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your unsafe or banned words (basic example)\n",
        "banned_keywords = [\"kill\", \"hate\", \"bomb\", \"suicide\", \"terrorist\"]\n",
        "\n",
        "# Run RAG pipeline\n",
        "question = \"What is the main topic of the PDF? Can you tell me the top 2 important points?\"\n",
        "raw_response = chain.invoke(question)\n",
        "\n",
        "# Apply guardrails before showing the response\n",
        "safe_response = apply_guardrails(raw_response, keywords=banned_keywords)\n",
        "\n",
        "print(f\"Answer: {safe_response}\")"
      ],
      "metadata": {
        "id": "m-aT3U9M14Vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0373066d-8577-4bd6-b9fa-a58f6338927b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  The main topic of the PDF is \"Blood Test Report\".\n",
            "\n",
            "The top 2 important points are:\n",
            "\n",
            "1. **Cholesterol Levels**: The report provides the results for Cholesterol, Total, which falls within the normal range (115.00 mg/dL).\n",
            "\n",
            "2. **Electrolyte Balance**: The report also provides the results for Electrolytes: Phosphorus, Sodium, Potassium, and Chloride, all of which fall within the normal ranges (2.40 - 5.10 mg/dL for Phosphorus, 136.00 - 145.00 mEq/L for Sodium, 3.50 - 5.10 mEq/L for Potassium, and 98.00 - 107.00 mEq/L for Chloride).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìä **Diagram: LangChain RAG with LLaMA + PDF + Guardrails**\n",
        "\n",
        "```text\n",
        "                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                ‚îÇ  üìÅ  Uploaded PDF     ‚îÇ\n",
        "                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                         ‚Üì\n",
        "              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "              ‚îÇ Load & Split Pages (Loader)‚îÇ\n",
        "              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                       ‚Üì\n",
        "         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "         ‚îÇ Convert to Embeddings (MiniLM)‚îÇ\n",
        "         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                  ‚Üì\n",
        "       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "       ‚îÇ In-Memory Vector Store (DocArray)‚îÇ\n",
        "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                ‚Üì\n",
        " ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        " ‚îÇ     QUESTION from USER                     ‚îÇ\n",
        " ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                ‚Üì\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ  Retrieve Relevant Chunks    ‚îÇ  ‚Üê (from vector store)\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "              ‚Üì\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ  Format Prompt (LangChain)   ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "              ‚Üì\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ   Local LLaMA Model (llama.cpp) ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "              ‚Üì\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ  Extract LLM Response (Parser)‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "              ‚Üì\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ üõ°Ô∏è Guardrails (Validation)     ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "              ‚Üì\n",
        "     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "     ‚îÇ  ‚úÖ Safe Answer to User       ‚îÇ\n",
        "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n"
      ],
      "metadata": {
        "id": "t0AHijvO17dh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7FHRLktr2Ekv"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}