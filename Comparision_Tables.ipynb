{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üîç **Types of Machine Learning ‚Äì Comparison Table**\n",
        "\n",
        "| Aspect | **Supervised Learning** | **Unsupervised Learning** | **Semi-Supervised Learning** | **Reinforcement Learning** |\n",
        "|--------|--------------------------|----------------------------|-------------------------------|-----------------------------|\n",
        "| **Definition** | Model learns from labeled data. | Model learns from unlabeled data by identifying patterns. | Uses a small amount of labeled data with a large amount of unlabeled data. | Agent learns through interaction with the environment using rewards and penalties. |\n",
        "| **Data Type** | Labeled | Unlabeled | Mix of Labeled & Unlabeled | Sequential, reward-based |\n",
        "| **Goal** | Predict outcomes or classify data | Find hidden patterns or structures | Improve accuracy with minimal labeled data | Learn a policy to maximize cumulative reward |\n",
        "| **Common Algorithms** | Linear Regression, Logistic Regression, Decision Trees, SVM, Random Forest | K-Means, Hierarchical Clustering, PCA, Autoencoders | Self-training, Semi-Supervised SVM, Graph-based methods | Q-Learning, Deep Q-Network (DQN), Policy Gradient, SARSA |\n",
        "| **Examples** | Email spam detection, credit scoring, disease prediction | Market segmentation, anomaly detection, topic modeling | Web content classification with few labeled pages | Game playing (Chess, Go), robotic control, recommendation engines |\n",
        "| **Input Data** | X (features) and y (labels) | Only X (features) | Small y with large X | States, actions, and rewards |\n",
        "| **Feedback Type** | Direct (ground truth labels) | No feedback | Partial feedback | Delayed feedback (rewards/punishments) |\n",
        "| **Training Complexity** | Moderate | Low to High (depends on algorithm) | Moderate | High (due to trial-and-error learning) |\n",
        "| **Output** | Predict class/values | Discover structure (clusters, dimensions) | Improved supervised performance | Optimal actions/policies |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **Quick Recap**\n",
        "\n",
        "- **Supervised Learning** ‚Üí Labeled data ‚Üí Prediction  \n",
        "- **Unsupervised Learning** ‚Üí No labels ‚Üí Pattern discovery  \n",
        "- **Semi-Supervised Learning** ‚Üí Small labeled + large unlabeled ‚Üí Better predictions  \n",
        "- **Reinforcement Learning** ‚Üí Learn by doing ‚Üí Optimal strategy via reward/penalty\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "8TCbUeFp7Zk3"
      },
      "id": "8TCbUeFp7Zk3"
    },
    {
      "cell_type": "markdown",
      "id": "63ea6a91-8ddf-4808-83b7-41163b2a5ac4",
      "metadata": {
        "id": "63ea6a91-8ddf-4808-83b7-41163b2a5ac4"
      },
      "source": [
        "\n",
        "\n",
        "### **Comparison of Batch Gradient Descent (BGD), Stochastic Gradient Descent (SGD), and Mini-batch Gradient Descent (MBGD)**\n",
        "\n",
        "| **Characteristic**              | **Batch Gradient Descent (BGD)**         | **Stochastic Gradient Descent (SGD)**   | **Mini-batch Gradient Descent (MBGD)**  |\n",
        "|---------------------------------|------------------------------------------|-----------------------------------------|------------------------------------------|\n",
        "| **Update Frequency**            | After processing **entire dataset**      | After **each training example**        | After each **mini-batch** (subset of data) |\n",
        "| **Memory Requirement**          | **High** (entire dataset)                | **Low** (one example)                  | **Medium** (mini-batch)                  |\n",
        "| **Speed per Update**            | **Slow** (needs entire dataset)          | **Fast** (one example at a time)       | **Medium** (mini-batch size)             |\n",
        "| **Convergence**                 | **Smooth, more stable**                  | **Noisy, can be erratic**              | **Balanced**, smoother than SGD         |\n",
        "| **Handling Large Datasets**      | **Inefficient**                          | **Efficient**                           | **Efficient** (most popular choice)      |\n",
        "| **Jumping out of Local Minima**  | **Difficult** ‚Üí affects model performance | **Easier** (due to noise)              | **Easier than BGD**                      |\n",
        "| **Application**                  | **Small datasets** (can't be improvised) | **Large datasets**, online learning   | **Deep learning, large datasets**       |\n",
        "\n",
        "---\n",
        "\n",
        "This table enhances readability while maintaining all the key information. Let me know if you need further refinements! üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae32e112-e420-41f5-b48e-32574efc003c",
      "metadata": {
        "id": "ae32e112-e420-41f5-b48e-32574efc003c"
      },
      "source": [
        "## **üìå LDA vs PCA**\n",
        "Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are both dimensionality reduction techniques but serve different purposes.  \n",
        "\n",
        "| Feature | **PCA (Principal Component Analysis)** | **LDA (Linear Discriminant Analysis)** |\n",
        "|---------|--------------------------------------|--------------------------------------|\n",
        "| **Purpose** | Reduces dimensionality by finding directions (principal components) that maximize variance in the data. | Reduces dimensionality while maximizing class separability for classification tasks. |\n",
        "| **Supervised/Unsupervised** | Unsupervised | Supervised (requires class labels) |\n",
        "| **How It Works** | Finds new axes (principal components) that capture maximum variance in the dataset. | Finds axes that maximize the separation between different classes. |\n",
        "| **Mathematical Basis** | Uses eigenvalues and eigenvectors of the covariance matrix to identify principal components. | Uses the scatter matrices to maximize the ratio of inter-class variance to intra-class variance. |\n",
        "| **Usage** | General-purpose dimensionality reduction for any data type. | Best suited for classification problems where class labels are available. |\n",
        "| **Interpretability** | Captures variance but does not consider class information. | Considers class labels, making it better for classification tasks. |\n",
        "| **Common Applications** | Image compression, noise reduction, exploratory data analysis. | Face recognition, pattern classification. |\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Feature Selection vs Feature Extraction vs Feature Elimination**\n",
        "Feature engineering involves choosing the right set of features for machine learning models. Here‚Äôs how **feature selection, feature extraction, and feature elimination** differ:\n",
        "\n",
        "| Feature | **Feature Selection** | **Feature Extraction** | **Feature Elimination** |\n",
        "|---------|----------------------|----------------------|----------------------|\n",
        "| **Definition** | Selecting the most relevant features from the dataset. | Transforming existing features into a new feature space. | Removing irrelevant or redundant features from the dataset. |\n",
        "| **Goal** | Improve model performance by removing noisy or redundant features. | Create new features that better represent the data. | Reduce overfitting and improve model efficiency. |\n",
        "| **How It Works** | Uses statistical tests, mutual information, or algorithms like Recursive Feature Elimination (RFE). | Uses mathematical transformations (PCA, LDA, Autoencoders). | Iteratively removes less important features based on model performance. |\n",
        "| **Techniques** | Filter methods (Chi-Square, ANOVA), Wrapper methods (RFE), Embedded methods (LASSO). | PCA, LDA, Autoencoders, Word Embeddings in NLP. | Recursive Feature Elimination (RFE), Univariate selection, Dropout in deep learning. |\n",
        "| **When to Use?** | When raw features have meaningful information. | When raw features are too high-dimensional or correlated. | When many features do not contribute significantly to model performance. |\n",
        "| **Example** | Choosing top 10 most correlated features from a dataset. | Using PCA to reduce 100 features into 10 principal components. | Using decision tree importance scores to drop unimportant features. |\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Summary**\n",
        "- **Use PCA when**: You need to reduce dimensionality without considering class labels.  \n",
        "- **Use LDA when**: You need dimensionality reduction while preserving class separability.  \n",
        "- **Use Feature Selection when**: You want to keep only the best features.  \n",
        "- **Use Feature Extraction when**: You want to create new features from existing data.  \n",
        "- **Use Feature Elimination when**: You want to remove unnecessary or redundant features.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2199612-0208-4f54-ae0f-53bf63ddeef7",
      "metadata": {
        "id": "f2199612-0208-4f54-ae0f-53bf63ddeef7"
      },
      "source": [
        "### **üìå Comprehensive Comparison Tables**  \n",
        "Below are the **detailed comparison tables** for **ML vs DL**, **NumPy vs Pandas**, **Python vs SQL for Data Manipulation**, and **Matplotlib vs Seaborn**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Machine Learning (ML) vs Deep Learning (DL)**  \n",
        "\n",
        "| **Feature**             | **Machine Learning (ML)**                                              | **Deep Learning (DL)**                                          |\n",
        "|-------------------------|------------------------------------------------------------------------|----------------------------------------------------------------|\n",
        "| **Definition**          | Algorithms that learn patterns from data without explicit programming. | Subset of ML that uses deep neural networks to model complex patterns. |\n",
        "| **Data Requirements**   | Works well with **small to medium datasets**.                         | Requires **large datasets** for better performance.            |\n",
        "| **Feature Engineering** | Requires **manual feature extraction** by domain experts.            | **Automatically extracts features** from raw data.             |\n",
        "| **Model Complexity**    | Uses simpler models like **Decision Trees, SVM, Random Forests**.    | Uses **complex neural networks** (CNNs, RNNs, Transformers).   |\n",
        "| **Training Time**       | **Faster training** (minutes to hours).                              | **Slower training** (hours to weeks).                         |\n",
        "| **Computational Power** | Works on **CPUs**; does not require high-end GPUs.                   | Requires **GPUs/TPUs** due to heavy computations.              |\n",
        "| **Interpretability**    | **More explainable and interpretable**.                              | **Harder to interpret** (black-box nature).                   |\n",
        "| **Generalization**      | Works well on **structured/tabular data**.                          | Works best on **unstructured data** (text, images, audio).    |\n",
        "| **Cost**               | **Lower cost** due to lower hardware needs.                          | **Higher cost** due to expensive training and hardware.       |\n",
        "| **Common Algorithms**  | **Linear Regression, Decision Trees, Random Forest, SVM, KNN**.      | **CNN, RNN, LSTM, Transformers, GANs**.                       |\n",
        "| **Use Cases**          | Fraud detection, recommendation systems, structured data analysis.   | Image recognition, speech processing, NLP, self-driving cars. |\n",
        "| **Popular Libraries**  | **Scikit-learn, XGBoost, LightGBM, H2O.ai**.                          | **TensorFlow, PyTorch, Keras, Theano**.                       |\n",
        "| **Real-World Examples** | **Netflix recommendations, spam filtering, credit scoring**.        | **Face recognition, self-driving cars, chatbots (ChatGPT)**.  |\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ NumPy vs Pandas**  \n",
        "| Feature | NumPy | Pandas |\n",
        "|---------|------|--------|\n",
        "| **Purpose** | Used for numerical computations and array-based operations. | Used for data manipulation and analysis. |\n",
        "| **Data Structure** | Works with **ndarrays** (multi-dimensional arrays). | Works with **Series** (1D) and **DataFrames** (2D). |\n",
        "| **Performance** | Faster for numerical operations. | Slightly slower as it adds additional functionalities. |\n",
        "| **Data Handling** | Works with homogeneous data (same data type). | Works with heterogeneous data (multiple types). |\n",
        "| **Indexing** | Uses integer-based indexing like arrays. | Uses labeled indexing with row/column names. |\n",
        "| **Operations** | Supports vectorized operations like matrix multiplication. | Supports SQL-like operations (merge, groupby, pivot). |\n",
        "| **Ease of Use** | Requires more effort for tabular data manipulation. | More intuitive for handling structured data. |\n",
        "| **Use Cases** | Scientific computing, linear algebra, statistics. | Data cleaning, manipulation, and analysis. |\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Python vs SQL for Data Manipulation**  \n",
        "| Feature | Python | SQL |\n",
        "|---------|--------|-----|\n",
        "| **Purpose** | General-purpose programming language for data analysis, ML, and AI. | Query language for managing relational databases. |\n",
        "| **Data Handling** | Uses Pandas, NumPy for data manipulation. | Uses tables, joins, and queries for structured data. |\n",
        "| **Performance** | Efficient for small to medium datasets. | Optimized for large datasets and relational operations. |\n",
        "| **Complexity** | More flexibility but requires coding for complex tasks. | Simpler for structured data operations like filtering and joins. |\n",
        "| **Data Storage** | Works with in-memory data. | Stores and manages data in relational databases. |\n",
        "| **Operations** | Supports complex mathematical and statistical operations. | Best for querying, aggregating, and filtering tabular data. |\n",
        "| **Common Use Cases** | Data preprocessing, ML model building, automation. | Data retrieval, database management, reporting. |\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Matplotlib vs Seaborn**  \n",
        "| Feature | Matplotlib | Seaborn |\n",
        "|---------|-----------|---------|\n",
        "| **Purpose** | General-purpose plotting library for static visualizations. | Built on top of Matplotlib, designed for statistical data visualization. |\n",
        "| **Ease of Use** | Requires more manual customization. | More concise and comes with better default styles. |\n",
        "| **Customization** | Highly customizable with detailed control over every aspect. | Less customizable but provides elegant default themes. |\n",
        "| **Data Handling** | Works well with NumPy arrays and lists. | Works well with Pandas DataFrames. |\n",
        "| **Types of Plots** | Line plots, bar charts, scatter plots, histograms. | Heatmaps, violin plots, pair plots, categorical plots. |\n",
        "| **Performance** | Faster for basic plotting. | Optimized for statistical visualization. |\n",
        "| **Common Use Cases** | Basic data visualization for reports, exploratory analysis. | Advanced data exploration with statistical insights. |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81f600e0-f7b7-43a7-bdd1-3f60461816b8",
      "metadata": {
        "id": "81f600e0-f7b7-43a7-bdd1-3f60461816b8"
      },
      "source": [
        "---\n",
        "\n",
        "### **Hyperparameter tuning search Comparison**\n",
        "| Method | Exploration | Computation Time | Efficiency |\n",
        "|--------|------------|-----------------|------------|\n",
        "| **Grid Search** | Exhaustive | High | Inefficient for large spaces |\n",
        "| **Random Search** | Random | Medium | Good for large spaces |\n",
        "| **Bayesian Search** | Adaptive | Low-Medium | Best for optimizing efficiently |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ab1e91-b901-4f9b-a8b9-893919755f70",
      "metadata": {
        "id": "58ab1e91-b901-4f9b-a8b9-893919755f70"
      },
      "source": [
        "---\n",
        " ## **Lasso Regression** and **Ridge Regression**:\n",
        "\n",
        "| Feature            | Lasso Regression (L1) | Ridge Regression (L2) |\n",
        "|--------------------|----------------------|----------------------|\n",
        "| **Regularization Type** | L1 (Lasso) | L2 (Ridge) |\n",
        "| **Mathematical Penalty** | sum of absolute values of coefficients | sum of squared coefficients|\n",
        "| **Feature Selection** | Can shrink some coefficients to **zero**, performing feature selection | Shrinks coefficients close to zero but **does not eliminate** them |\n",
        "| **Effect on Coefficients** | Some become **exactly zero**, reducing the number of features | All features contribute, but coefficients become smaller |\n",
        "| **Best Use Case** | When feature selection is needed (high-dimensional datasets) | When multicollinearity is present (correlated features) |\n",
        "| **Computational Cost** | Can be higher due to non-differentiability at zero | Generally lower as the gradient is well-behaved |\n",
        "| **Bias-Variance Tradeoff** | Higher bias, lower variance (due to feature selection) | Lower bias, higher variance (since all features contribute) |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51cd406-6ee7-49d6-a6fb-cfaf1111dcea",
      "metadata": {
        "id": "f51cd406-6ee7-49d6-a6fb-cfaf1111dcea"
      },
      "source": [
        "---\n",
        "Here‚Äôs a detailed comparison of all the commonly asked ML topics in interviews:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Model Comparisons**\n",
        "\n",
        "### **Linear Regression vs. Logistic Regression**  \n",
        "| Feature | Linear Regression | Logistic Regression |\n",
        "|---------|-----------------|------------------|\n",
        "| **Type of Problem** | Regression | Classification |\n",
        "| **Output** | Continuous values (e.g., price, temperature) | Probability (0-1) |\n",
        "| **Equation** | \\( Y = \\beta_0 + \\beta_1 X_1 + \\dots + \\beta_n X_n \\) | \\( P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X_1 + \\dots + \\beta_n X_n)}} \\) |\n",
        "| **Cost Function** | Mean Squared Error (MSE) | Log Loss (Cross-Entropy) |\n",
        "| **Use Cases** | Predicting house prices, stock prices | Spam detection, disease classification |\n",
        "\n",
        "---\n",
        "\n",
        "### **Decision Tree vs. Random Forest**  \n",
        "| Feature | Decision Tree | Random Forest |\n",
        "|---------|-------------|--------------|\n",
        "| **Concept** | Single tree structure | Multiple decision trees (ensemble) |\n",
        "| **Overfitting Risk** | High (prone to overfitting) | Low (ensemble averaging) |\n",
        "| **Accuracy** | Moderate | High |\n",
        "| **Computational Complexity** | Low | High (because of multiple trees) |\n",
        "| **Interpretability** | High (easy to understand) | Low (complex model) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Random Forest vs. Gradient Boosting**  \n",
        "| Feature | Random Forest | Gradient Boosting |\n",
        "|---------|--------------|-------------------|\n",
        "| **Concept** | Bagging (parallel trees) | Boosting (sequential weak learners) |\n",
        "| **Training Speed** | Faster | Slower |\n",
        "| **Performance on Small Data** | Good | Excellent |\n",
        "| **Robust to Outliers** | Yes | No (sensitive to outliers) |\n",
        "\n",
        "---\n",
        "\n",
        "### **SVM vs. Logistic Regression**  \n",
        "| Feature | SVM | Logistic Regression |\n",
        "|---------|-----|--------------------|\n",
        "| **Use Case** | Complex boundary classification | Simple binary classification |\n",
        "| **Kernel Trick** | Yes | No |\n",
        "| **Interpretability** | Low | High |\n",
        "| **Scalability** | Slow for large data | Fast |\n",
        "\n",
        "---\n",
        "\n",
        "### **SVM vs. Random Forest**  \n",
        "| Feature | SVM | Random Forest |\n",
        "|---------|-----|--------------|\n",
        "| **Non-Linearity** | Supports kernels | Works with decision trees |\n",
        "| **Handling of Large Datasets** | Computationally expensive | Handles large data well |\n",
        "| **Overfitting** | Can overfit with improper kernel choice | Less prone to overfitting |\n",
        "\n",
        "---\n",
        "\n",
        "### **XGBoost vs. LightGBM vs. CatBoost**  \n",
        "| Feature | XGBoost | LightGBM | CatBoost |\n",
        "|---------|--------|----------|---------|\n",
        "| **Speed** | Moderate | Faster | Fast |\n",
        "| **Handling Categorical Data** | Needs encoding | Needs encoding | Handles directly |\n",
        "| **Tree Structure** | Level-wise | Leaf-wise | Ordered boosting |\n",
        "\n",
        "---\n",
        "\n",
        "### **KNN vs. SVM**  \n",
        "| Feature | KNN | SVM |\n",
        "|---------|----|-----|\n",
        "| **Training Time** | Fast | Slow |\n",
        "| **Prediction Time** | Slow (distance calculations) | Fast |\n",
        "| **Best for** | Small datasets | Large, complex datasets |\n",
        "\n",
        "---\n",
        "\n",
        "### **Lasso vs. Ridge Regression**  \n",
        "| Feature | Lasso (L1) | Ridge (L2) |\n",
        "|---------|-----------|-----------|\n",
        "| **Regularization** | L1 (absolute value) | L2 (squared value) |\n",
        "| **Feature Selection** | Yes (eliminates some features) | No (reduces coefficients) |\n",
        "| **Use Case** | Sparse data | Multicollinear data |\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Deep Learning Comparisons**\n",
        "\n",
        "### **CNN vs. RNN**  \n",
        "| Feature | CNN | RNN |\n",
        "|---------|----|-----|\n",
        "| **Best for** | Images | Sequential data (text, speech) |\n",
        "| **Memory** | No memory | Keeps memory of past inputs |\n",
        "| **Processing** | Parallel | Sequential |\n",
        "\n",
        "---\n",
        "\n",
        "### **LSTM vs. GRU**  \n",
        "| Feature | LSTM | GRU |\n",
        "|---------|----|-----|\n",
        "| **Gates** | 3 (Forget, Input, Output) | 2 (Reset, Update) |\n",
        "| **Training Speed** | Slow | Faster |\n",
        "\n",
        "---\n",
        "\n",
        "### **Transformers vs. RNNs**  \n",
        "| Feature | Transformers | RNNs |\n",
        "|---------|-------------|------|\n",
        "| **Parallel Processing** | Yes | No |\n",
        "| **Best for** | Large text datasets | Short sequential data |\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Optimization and Training Comparisons**\n",
        "\n",
        "### **Gradient Descent Variants**  \n",
        "| Feature | Batch GD | Mini-Batch GD | Stochastic GD |\n",
        "|---------|---------|-------------|-------------|\n",
        "| **Update per step** | Full dataset | Small batch | One sample |\n",
        "| **Speed** | Slow | Fast | Fastest |\n",
        "| **Stability** | Stable | Moderate | Noisy |\n",
        "\n",
        "---\n",
        "\n",
        "### **Adam vs. RMSprop vs. SGD**  \n",
        "| Feature | Adam | RMSprop | SGD |\n",
        "|---------|-----|--------|-----|\n",
        "| **Learning Rate Adaptation** | Yes | Yes | No |\n",
        "| **Best for** | General ML/DL | RNNs | Simple problems |\n",
        "\n",
        "---\n",
        "\n",
        "### **ReLU vs. Sigmoid vs. Tanh**  \n",
        "| Feature | ReLU | Sigmoid | Tanh |\n",
        "|---------|-----|--------|-----|\n",
        "| **Best for** | Deep networks | Probability outputs | Symmetric data |\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Regularization and Feature Selection**\n",
        "\n",
        "### **PCA vs. t-SNE vs. UMAP**  \n",
        "| Feature | PCA | t-SNE | UMAP |\n",
        "|---------|-----|------|------|\n",
        "| **Type** | Linear | Non-linear | Non-linear |\n",
        "| **Best for** | Large datasets | Clustering | Embeddings |\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Unsupervised Learning Comparisons**\n",
        "\n",
        "### **K-Means vs. DBSCAN vs. Hierarchical Clustering**  \n",
        "| Feature | K-Means | DBSCAN | Hierarchical |\n",
        "|---------|--------|--------|-------------|\n",
        "| **Cluster Shape** | Spherical | Arbitrary | Tree-based |\n",
        "| **Scalability** | High | Moderate | Slow |\n",
        "\n",
        "---\n",
        "\n",
        "### **PCA vs. LDA**  \n",
        "| Feature | PCA | LDA |\n",
        "|---------|----|----|\n",
        "| **Type** | Unsupervised | Supervised |\n",
        "| **Best for** | Any data | Classification |\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Bias-Variance Tradeoff & Generalization**\n",
        "\n",
        "### **Overfitting vs. Underfitting**  \n",
        "| Feature | Overfitting | Underfitting |\n",
        "|---------|-----------|-------------|\n",
        "| **Training Error** | Low | High |\n",
        "| **Test Error** | High | High |\n",
        "\n",
        "---\n",
        "\n",
        "### **Bias vs. Variance**  \n",
        "| Feature | Bias | Variance |\n",
        "|---------|-----|--------|\n",
        "| **Error Type** | Systematic | Sensitivity to training data |\n",
        "| **Fix** | More complex model | Simplify model |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç **Difference Between Single Layer Perceptron and Multi-Layer Perceptron (MLP)**\n",
        "\n",
        "Both are types of **artificial neural networks**, but they differ in **architecture, capability, and use cases**. Here's a detailed comparison:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "| Aspect | **Single Layer Perceptron** | **Multi-Layer Perceptron** |\n",
        "|--------|-----------------------------|-----------------------------|\n",
        "| **Definition** | A neural network with only one layer between input and output. | A neural network with one or more **hidden layers** between input and output. |\n",
        "| Layers | 1 (Input ‚Üí Output) | ‚â•3 (Input ‚Üí Hidden(s) ‚Üí Output) |\n",
        "| Hidden Layer | No | Yes |\n",
        "| Linearity | Can only learn linearly separable patterns. | Can learn non-linear and complex decision boundaries. |\n",
        "| Learning Rule | Typically uses the Perceptron Learning Rule. | Uses Backpropagation and Gradient Descent. |\n",
        "| Activation Function | Step, Linear | ReLU, Sigmoid, Tanh, etc. |\n",
        "| Handles Non-Linearity | ‚ùå No | ‚úÖ Yes |\n",
        "| Forward Pass| Input ‚Üí Output directly. | Input ‚Üí Hidden ‚Üí Output with nonlinear transformations. |\n",
        "| Uses Backpropagation | ‚ùå No (no hidden layers)| ‚úÖ Yes |\n",
        "| Solves XOR | ‚ùå No | ‚úÖ Yes |\n",
        "| Complexity | Low | High |\n",
        "| Use Case | Basic linear classification | Complex classification and regression |\n",
        "| Scalability | Not scalable for complex problems. | Highly scalable with multiple layers and neurons. |\n",
        "| Capability | Limited to simple tasks (e.g., AND, OR gates). | Can solve complex tasks (e.g., image classification, NLP). |\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can show a **Python implementation of both SLP and MLP**, or even visualize the **decision boundaries**. Let me know! üöÄ"
      ],
      "metadata": {
        "id": "rvsYDUO95_OW"
      },
      "id": "rvsYDUO95_OW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SQL Language Categories (DDL, DML, DCL, TCL, DQL)** with their purpose and commonly used commands:\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ **SQL Language Classification Table**\n",
        "\n",
        "| **Category** | **Full Form**                    | **Purpose**                                             | **Key Commands**                         |\n",
        "|--------------|----------------------------------|----------------------------------------------------------|------------------------------------------|\n",
        "| **DDL**      | Data Definition Language         | Defines and modifies **structure/schema** of DB objects | `CREATE`, `ALTER`, `DROP`, `TRUNCATE`    |\n",
        "| **DML**      | Data Manipulation Language       | Performs **data operations** (insert, update, delete)   | `INSERT`, `UPDATE`, `DELETE`             |\n",
        "| **DCL**      | Data Control Language            | Manages **permissions and access control**              | `GRANT`, `REVOKE`                        |\n",
        "| **TCL**      | Transaction Control Language     | Controls **transactions** and ensures data integrity    | `COMMIT`, `ROLLBACK`, `SAVEPOINT`        |\n",
        "| **DQL**      | Data Query Language              | Used to **fetch/query data** from the database          | `SELECT`                                 |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Quick Summary**\n",
        "\n",
        "- **DDL** ‚Äì Structure-related  \n",
        "- **DML** ‚Äì Data manipulation  \n",
        "- **DCL** ‚Äì Access control  \n",
        "- **TCL** ‚Äì Transaction management  \n",
        "- **DQL** ‚Äì Data retrieval  "
      ],
      "metadata": {
        "id": "tlGupm2A-qgJ"
      },
      "id": "tlGupm2A-qgJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## üìä **Key SQL Commands Table**\n",
        "\n",
        "| **Command**   | **Category** | **Purpose**                                                  |\n",
        "|---------------|--------------|---------------------------------------------------------------|\n",
        "| `CREATE`      | DDL          | Creates a new database object (e.g., table, view, index).     |\n",
        "| `ALTER`       | DDL          | Modifies the structure of an existing database object.        |\n",
        "| `DROP`        | DDL          | Deletes a database object permanently.                        |\n",
        "| `TRUNCATE`    | DDL          | Removes all records from a table (faster than DELETE).        |\n",
        "| `INSERT`      | DML          | Adds new data into a table.                                   |\n",
        "| `UPDATE`      | DML          | Modifies existing data in a table.                            |\n",
        "| `DELETE`      | DML          | Removes specific records from a table.                        |\n",
        "| `GRANT`       | DCL          | Gives user access privileges to database objects.             |\n",
        "| `REVOKE`      | DCL          | Removes access privileges granted to users.                   |\n",
        "| `COMMIT`      | TCL          | Saves all changes made by the transaction.                    |\n",
        "| `ROLLBACK`    | TCL          | Undoes changes made in the current transaction.               |\n",
        "| `SAVEPOINT`   | TCL          | Sets a point within a transaction to which you can rollback.  |\n",
        "| `SELECT`      | DQL          | Retrieves data from one or more tables.                       |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Tip:\n",
        "- Use `DDL` carefully ‚Äì **DROP** and **TRUNCATE** are irreversible.\n",
        "- Combine `TCL` with `DML` for safe transaction control.\n",
        "- `DCL` is essential in multi-user environments.\n"
      ],
      "metadata": {
        "id": "s2SXaxxI-7_G"
      },
      "id": "s2SXaxxI-7_G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287439b9-e792-4382-830f-49225d5e4ea1",
      "metadata": {
        "id": "287439b9-e792-4382-830f-49225d5e4ea1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}