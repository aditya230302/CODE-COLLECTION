{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tiny Transformer**\n",
        "complete the training pipeline, including:\n",
        "- Preprocessing (tokenization & vocab creation)\n",
        "- Sequence preparation (adding <SOS> and <EOS>)\n",
        "- Dataset & DataLoader\n",
        "- Training loop\n",
        "- Inference function"
      ],
      "metadata": {
        "id": "jPIF6b-SLSaB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-QrMc5uLMPu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Data**"
      ],
      "metadata": {
        "id": "THrQ5MsPLkPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å\", \"i am happy\"),\n",
        "    (\"‡§Ü‡§™ ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•à‡§Ç\", \"you are sad\"),\n",
        "    (\"‡§µ‡§π ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à\", \"he is tired\"),\n",
        "    (\"‡§µ‡§π ‡§•‡§ï‡•Ä ‡§π‡•Å‡§à ‡§π‡•à\", \"she is tired\"),\n",
        "    (\"‡§π‡§Æ ‡§≠‡•Ç‡§ñ‡•á‡§Ç ‡§π‡•à‡§Ç\", \"we are hungry\"),\n",
        "    (\"‡§µ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç\", \"they are busy\"),\n",
        "    (\"‡§Æ‡•Å‡§ù‡•á ‡§†‡§Ç‡§° ‡§≤‡§ó ‡§∞‡§π‡•Ä ‡§π‡•à\", \"i am cold\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§Ü‡§è ‡§π‡•ã\", \"you are late\"),\n",
        "    (\"‡§µ‡§π ‡§ñ‡•Å‡§∂ ‡§π‡•à\", \"she is happy\"),\n",
        "    (\"‡§π‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç\", \"we are ready\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•Ç‡§Å\", \"i am sad\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ñ‡•Å‡§∂ ‡§π‡•ã\", \"you are happy\"),\n",
        "    (\"‡§µ‡•á ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç\", \"they are ready\"),\n",
        "    (\"‡§µ‡§π ‡§•‡§ï‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à\", \"he is tired\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã\", \"you are busy\"),\n",
        "    (\"‡§π‡§Æ ‡§†‡§Ç‡§°‡•á ‡§π‡•à‡§Ç\", \"we are cold\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç?\", \"are you ready?\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§π ‡§Ü ‡§∞‡§π‡•Ä ‡§π‡•à?\", \"is she coming?\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§†‡•Ä‡§ï ‡§π‡•ã?\", \"are you okay?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\", \"i am fine\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã?\", \"are you busy?\"),\n",
        "    (\"‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\", \"he is going to school\"),\n",
        "    (\"‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\", \"she is going to the market\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§™‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å\", \"i am studying\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã?\", \"what are you doing?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§∏‡•ã ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å\", \"i am sleeping\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•ã?\", \"where are you going?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å\", \"i am going home\"),\n",
        "    (\"‡§µ‡§π ‡§ü‡•Ä‡§µ‡•Ä ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à\", \"he is watching TV\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§ö‡•Å‡§ï‡•á ‡§π‡•ã?\", \"have you eaten?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à\", \"i have eaten\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ï‡•å‡§® ‡§π‡•ã?\", \"who are you?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å\", \"i am a student\"),\n",
        "    (\"‡§µ‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§π‡•à\", \"he is a doctor\"),\n",
        "    (\"‡§µ‡§π ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à\", \"she is a teacher\"),\n",
        "    (\"‡§π‡§Æ ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç\", \"we are friends\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§Æ‡•á‡§∞‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã\", \"you are my friend\"),\n",
        "    (\"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\", \"the weather is good today\"),\n",
        "    (\"‡§¨‡§æ‡§π‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à\", \"it is raining outside\"),\n",
        "    (\"‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§†‡§Ç‡§° ‡§π‡•à\", \"it is very cold today\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç?\", \"do you speak english?\"),\n",
        "    (\"‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Å\", \"yes, i speak english\"),\n",
        "    (\"‡§®‡§π‡•Ä‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¨‡•ã‡§≤‡§§‡§æ\", \"no, i do not speak english\"),\n",
        "    (\"‡§ï‡•É‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡§ø‡§è\", \"please speak slowly\"),\n",
        "    (\"‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç\", \"i am sorry\"),\n",
        "    (\"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\", \"thank you\"),\n",
        "    (\"‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à\", \"you are welcome\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§Ø ‡§π‡•Å‡§Ü ‡§π‡•à?\", \"what time is it?\"),\n",
        "    (\"‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ\", \"i do not know\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡§æ\", \"i did not understand\"),\n",
        "    (\"‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§π‡§∞‡§æ‡§á‡§è\", \"please repeat\"),\n",
        "]\n"
      ],
      "metadata": {
        "id": "xx4xTxINLgfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Vocab building**"
      ],
      "metadata": {
        "id": "z67SoSZKLsi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "bNTpNOBILqjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    idx = 4\n",
        "    for sent in sentences:\n",
        "        for word in sent.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "    return vocab\n"
      ],
      "metadata": {
        "id": "0Rl_Nx9_L2RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hindi_sentences = [h for h, e in pairs]\n",
        "english_sentences = [e for h, e in pairs]\n",
        "\n",
        "hindi_vocab = build_vocab(hindi_sentences)\n",
        "english_vocab = build_vocab(english_sentences)\n",
        "\n",
        "inv_english_vocab = {v: k for k, v in english_vocab.items()}"
      ],
      "metadata": {
        "id": "parCzyWPL37X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Tokenizer helpers**"
      ],
      "metadata": {
        "id": "3rpyxJFCL-kt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence, vocab):\n",
        "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in sentence.split()]\n",
        "\n",
        "def add_tokens(seq, vocab):\n",
        "    return [vocab[\"<SOS>\"]] + seq + [vocab[\"<EOS>\"]]"
      ],
      "metadata": {
        "id": "mShFz_uUL6ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Dataset**"
      ],
      "metadata": {
        "id": "fEom2rJyMCuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, hindi_vocab, english_vocab):\n",
        "        self.data = []\n",
        "        for hin, eng in pairs:\n",
        "            hin_tokens = add_tokens(tokenize(hin, hindi_vocab), hindi_vocab)\n",
        "            eng_tokens = add_tokens(tokenize(eng, english_vocab), english_vocab)\n",
        "            self.data.append((torch.tensor(hin_tokens), torch.tensor(eng_tokens)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "0S81EgehMBI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    hindi, english = zip(*batch)\n",
        "    return (\n",
        "        pad_sequence(hindi, batch_first=True, padding_value=hindi_vocab[\"<PAD>\"]),\n",
        "        pad_sequence(english, batch_first=True, padding_value=english_vocab[\"<PAD>\"])\n",
        "    )"
      ],
      "metadata": {
        "id": "BFKrAH0CMFlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = TranslationDataset(pairs, hindi_vocab, english_vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "m6rXjaCCMHCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Model**"
      ],
      "metadata": {
        "id": "6nYpsDO8MKxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#helper modules - way srp\n",
        "\n",
        "class TinyTransformer(nn.Module):\n",
        "  def __init__(self,hindi_vocab_size, english_vocab_size, d_model=32):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "\n",
        "    #Embeddings\n",
        "    self.hindi_emb = nn.Embedding(hindi_vocab_size, d_model)\n",
        "    self.english_emb = nn.Embedding(english_vocab_size, d_model)\n",
        "\n",
        "    #Single attention layer\n",
        "    self.attention = nn.MultiheadAttention(d_model, num_heads=1, batch_first=True)\n",
        "\n",
        "    #feedforard later\n",
        "    # hindi - englis\n",
        "    #inp - 32 op-vocab size -? english _vocab size?\n",
        "    self.ffn = nn.Linear(d_model, english_vocab_size)\n",
        "\n",
        "  def forward(self, hindi, english):\n",
        "    #embed\n",
        "    h_emb = self.hindi_emb(hindi)\n",
        "    e_emb = self.english_emb(english)\n",
        "\n",
        "    #attention                         @query,key,value\n",
        "    attented, weights = self.attention(e_emb, h_emb, h_emb)\n",
        "\n",
        "    output = self.ffn(attented)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "_wgn8zrgMIoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Training**"
      ],
      "metadata": {
        "id": "Xr6o-D0GMbyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CUDA stands for Compute Unified Device Architecture ‚Äî it is a parallel computing platform and API developed by NVIDIA that allows your GPU (Graphics Processing Unit) to perform general-purpose computations (not just graphics rendering).`"
      ],
      "metadata": {
        "id": "yGvh6dkFc1j0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = TinyTransformer(len(hindi_vocab), len(english_vocab)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"])"
      ],
      "metadata": {
        "id": "XYEMKrjcMRsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(50):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for hindi, english in dataloader:\n",
        "        hindi, english = hindi.to(device), english.to(device)\n",
        "\n",
        "        # Shift English for teacher forcing\n",
        "        input_english = english[:, :-1]\n",
        "        target_english = english[:, 1:]\n",
        "\n",
        "        output = model(hindi, input_english)\n",
        "        output = output.reshape(-1, output.shape[-1])\n",
        "        target = target_english.reshape(-1)\n",
        "\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPDo2EfaMez5",
        "outputId": "4d256893-5b14-42f3-f763-dfa7b7e3cf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 100.5661\n",
            "Epoch 2, Loss: 78.6104\n",
            "Epoch 3, Loss: 59.3238\n",
            "Epoch 4, Loss: 54.4675\n",
            "Epoch 5, Loss: 43.3650\n",
            "Epoch 6, Loss: 38.0139\n",
            "Epoch 7, Loss: 35.9755\n",
            "Epoch 8, Loss: 30.1930\n",
            "Epoch 9, Loss: 32.3335\n",
            "Epoch 10, Loss: 31.3307\n",
            "Epoch 11, Loss: 27.9239\n",
            "Epoch 12, Loss: 27.0539\n",
            "Epoch 13, Loss: 26.5243\n",
            "Epoch 14, Loss: 25.7373\n",
            "Epoch 15, Loss: 26.8565\n",
            "Epoch 16, Loss: 27.4223\n",
            "Epoch 17, Loss: 25.4458\n",
            "Epoch 18, Loss: 22.4163\n",
            "Epoch 19, Loss: 20.4710\n",
            "Epoch 20, Loss: 20.8281\n",
            "Epoch 21, Loss: 21.2567\n",
            "Epoch 22, Loss: 19.1312\n",
            "Epoch 23, Loss: 18.7586\n",
            "Epoch 24, Loss: 20.7685\n",
            "Epoch 25, Loss: 22.7718\n",
            "Epoch 26, Loss: 18.9869\n",
            "Epoch 27, Loss: 20.7499\n",
            "Epoch 28, Loss: 21.4003\n",
            "Epoch 29, Loss: 22.3462\n",
            "Epoch 30, Loss: 21.4046\n",
            "Epoch 31, Loss: 19.1117\n",
            "Epoch 32, Loss: 18.7097\n",
            "Epoch 33, Loss: 19.2413\n",
            "Epoch 34, Loss: 18.6041\n",
            "Epoch 35, Loss: 15.9459\n",
            "Epoch 36, Loss: 14.8069\n",
            "Epoch 37, Loss: 16.8873\n",
            "Epoch 38, Loss: 17.2674\n",
            "Epoch 39, Loss: 16.8566\n",
            "Epoch 40, Loss: 15.5341\n",
            "Epoch 41, Loss: 18.4584\n",
            "Epoch 42, Loss: 20.6820\n",
            "Epoch 43, Loss: 18.3003\n",
            "Epoch 44, Loss: 17.3698\n",
            "Epoch 45, Loss: 19.0655\n",
            "Epoch 46, Loss: 22.3186\n",
            "Epoch 47, Loss: 24.4760\n",
            "Epoch 48, Loss: 27.5603\n",
            "Epoch 49, Loss: 28.9887\n",
            "Epoch 50, Loss: 25.6823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Inference**"
      ],
      "metadata": {
        "id": "SHO1Kj6PMmcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(hindi_sentence):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        hin = add_tokens(tokenize(hindi_sentence, hindi_vocab), hindi_vocab)\n",
        "        hin_tensor = torch.tensor(hin).unsqueeze(0).to(device)\n",
        "\n",
        "        eng_input = torch.tensor([english_vocab[\"<SOS>\"]], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(10):\n",
        "            output = model(hin_tensor, eng_input)\n",
        "            next_token = output[0, -1].argmax(-1).item()\n",
        "            if next_token == english_vocab[\"<EOS>\"]:\n",
        "                break\n",
        "            result.append(inv_english_vocab.get(next_token, \"<UNK>\"))\n",
        "            eng_input = torch.cat([eng_input, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "\n",
        "        return ' '.join(result)"
      ],
      "metadata": {
        "id": "5FKQOSF_MiOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Test**"
      ],
      "metadata": {
        "id": "H-gZ_8wCMsxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nTranslation Examples:\\n\")\n",
        "for hindi, _ in pairs:\n",
        "    print(f\"{hindi} -> {translate_sentence(hindi)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEIPTZCaMqx2",
        "outputId": "ebe1e3d8-ba4b-466c-eff3-01ca3ec3a268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translation Examples:\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å -> i am happy\n",
            "‡§Ü‡§™ ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•à‡§Ç -> you are we\n",
            "‡§µ‡§π ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à -> is tired\n",
            "‡§µ‡§π ‡§•‡§ï‡•Ä ‡§π‡•Å‡§à ‡§π‡•à -> is tired\n",
            "‡§π‡§Æ ‡§≠‡•Ç‡§ñ‡•á‡§Ç ‡§π‡•à‡§Ç -> we\n",
            "‡§µ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç -> they\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§†‡§Ç‡§° ‡§≤‡§ó ‡§∞‡§π‡•Ä ‡§π‡•à -> i is are i is are i is are i\n",
            "‡§§‡•Å‡§Æ ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§Ü‡§è ‡§π‡•ã -> you are late\n",
            "‡§µ‡§π ‡§ñ‡•Å‡§∂ ‡§π‡•à -> is happy\n",
            "‡§π‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç -> we\n",
            "‡§Æ‡•à‡§Ç ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•Ç‡§Å -> i am sad\n",
            "‡§§‡•Å‡§Æ ‡§ñ‡•Å‡§∂ ‡§π‡•ã -> you are happy\n",
            "‡§µ‡•á ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç -> they\n",
            "‡§µ‡§π ‡§•‡§ï‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à -> is tired\n",
            "‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã -> you are late\n",
            "‡§π‡§Æ ‡§†‡§Ç‡§°‡•á ‡§π‡•à‡§Ç -> we\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç? -> you ready?\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§π ‡§Ü ‡§∞‡§π‡•Ä ‡§π‡•à? -> is it?\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§†‡•Ä‡§ï ‡§π‡•ã? -> you ready?\n",
            "‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å -> i am fine\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã? -> you ready?\n",
            "‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à -> is going going going going going going going going going\n",
            "‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à -> is going going going going going going going going going\n",
            "‡§Æ‡•à‡§Ç ‡§™‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å -> i am studying\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã? -> you ready?\n",
            "‡§Æ‡•à‡§Ç ‡§∏‡•ã ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å -> i am sleeping\n",
            "‡§§‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•ã? -> you doing?\n",
            "‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å -> i am going going going going going going going going\n",
            "‡§µ‡§π ‡§ü‡•Ä‡§µ‡•Ä ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à -> is watching TV is watching TV is watching TV is\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§ö‡•Å‡§ï‡•á ‡§π‡•ã? -> you eaten?\n",
            "‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à -> eaten\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•å‡§® ‡§π‡•ã? -> you are you are you are you are you are\n",
            "‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å -> i am a student\n",
            "‡§µ‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§π‡•à -> is doctor\n",
            "‡§µ‡§π ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à -> is teacher\n",
            "‡§π‡§Æ ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç -> we\n",
            "‡§§‡•Å‡§Æ ‡§Æ‡•á‡§∞‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã -> you are late\n",
            "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à -> the weather is weather is weather is weather is weather\n",
            "‡§¨‡§æ‡§π‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à -> it is late\n",
            "‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§†‡§Ç‡§° ‡§π‡•à -> it is very it is very it is very it\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç? -> you speak ready?\n",
            "‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Å -> english\n",
            "‡§®‡§π‡•Ä‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¨‡•ã‡§≤‡§§‡§æ -> english\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡§ø‡§è -> please speak please speak please speak please speak please speak\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç -> i am sorry\n",
            "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ -> are you are you are you are you are you\n",
            "‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à -> welcome\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§Ø ‡§π‡•Å‡§Ü ‡§π‡•à? -> is it?\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ -> i do not do not do not do not do\n",
            "‡§Æ‡•à‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡§æ -> not understand\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§π‡§∞‡§æ‡§á‡§è -> please are please are please are please are please are\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ‚úÖ Notes:\n",
        "\n",
        "* It uses **only 1 attention head**, so it's a *very tiny* model‚Äîgreat for learning.\n",
        "* You can improve it by:\n",
        "\n",
        "  * Using positional encodings\n",
        "  * Adding encoder-decoder layers\n",
        "  * Using a larger dataset\n",
        "* This model works because the dataset is tiny and deterministic.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B5viXNxFM7Nl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Add positional encodings and Use multi-heads and layers**"
      ],
      "metadata": {
        "id": "tlKXLnZPO0nZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Positional Encoding ‚Äî Adds position awareness to the model.\n",
        "\n",
        "2. Multi-head Attention ‚Äî Already exists; we‚Äôll expand it.\n",
        "\n",
        "3. Stacked Layers ‚Äî Multiple encoder and decoder layers."
      ],
      "metadata": {
        "id": "sukRQFzmO6Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Data Setup\n",
        "# ----------------------------\n",
        "\n",
        "pairs = [\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å\", \"i am happy\"),\n",
        "    (\"‡§Ü‡§™ ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•à‡§Ç\", \"you are sad\"),\n",
        "    (\"‡§µ‡§π ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à\", \"he is tired\"),\n",
        "    (\"‡§µ‡§π ‡§•‡§ï‡•Ä ‡§π‡•Å‡§à ‡§π‡•à\", \"she is tired\"),\n",
        "    (\"‡§π‡§Æ ‡§≠‡•Ç‡§ñ‡•á‡§Ç ‡§π‡•à‡§Ç\", \"we are hungry\"),\n",
        "    (\"‡§µ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç\", \"they are busy\"),\n",
        "    (\"‡§Æ‡•Å‡§ù‡•á ‡§†‡§Ç‡§° ‡§≤‡§ó ‡§∞‡§π‡•Ä ‡§π‡•à\", \"i am cold\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§Ü‡§è ‡§π‡•ã\", \"you are late\"),\n",
        "    (\"‡§µ‡§π ‡§ñ‡•Å‡§∂ ‡§π‡•à\", \"she is happy\"),\n",
        "    (\"‡§π‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç\", \"we are ready\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•Ç‡§Å\", \"i am sad\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ñ‡•Å‡§∂ ‡§π‡•ã\", \"you are happy\"),\n",
        "    (\"‡§µ‡•á ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç\", \"they are ready\"),\n",
        "    (\"‡§µ‡§π ‡§•‡§ï‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à\", \"he is tired\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã\", \"you are busy\"),\n",
        "    (\"‡§π‡§Æ ‡§†‡§Ç‡§°‡•á ‡§π‡•à‡§Ç\", \"we are cold\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç?\", \"are you ready?\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§π ‡§Ü ‡§∞‡§π‡•Ä ‡§π‡•à?\", \"is she coming?\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§†‡•Ä‡§ï ‡§π‡•ã?\", \"are you okay?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å\", \"i am fine\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã?\", \"are you busy?\"),\n",
        "    (\"‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à\", \"he is going to school\"),\n",
        "    (\"‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à\", \"she is going to the market\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§™‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å\", \"i am studying\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã?\", \"what are you doing?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§∏‡•ã ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å\", \"i am sleeping\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•ã?\", \"where are you going?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å\", \"i am going home\"),\n",
        "    (\"‡§µ‡§π ‡§ü‡•Ä‡§µ‡•Ä ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à\", \"he is watching TV\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§ö‡•Å‡§ï‡•á ‡§π‡•ã?\", \"have you eaten?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à\", \"i have eaten\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§ï‡•å‡§® ‡§π‡•ã?\", \"who are you?\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å\", \"i am a student\"),\n",
        "    (\"‡§µ‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§π‡•à\", \"he is a doctor\"),\n",
        "    (\"‡§µ‡§π ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à\", \"she is a teacher\"),\n",
        "    (\"‡§π‡§Æ ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç\", \"we are friends\"),\n",
        "    (\"‡§§‡•Å‡§Æ ‡§Æ‡•á‡§∞‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã\", \"you are my friend\"),\n",
        "    (\"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à\", \"the weather is good today\"),\n",
        "    (\"‡§¨‡§æ‡§π‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à\", \"it is raining outside\"),\n",
        "    (\"‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§†‡§Ç‡§° ‡§π‡•à\", \"it is very cold today\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç?\", \"do you speak english?\"),\n",
        "    (\"‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Å\", \"yes, i speak english\"),\n",
        "    (\"‡§®‡§π‡•Ä‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¨‡•ã‡§≤‡§§‡§æ\", \"no, i do not speak english\"),\n",
        "    (\"‡§ï‡•É‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡§ø‡§è\", \"please speak slowly\"),\n",
        "    (\"‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç\", \"i am sorry\"),\n",
        "    (\"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶\", \"thank you\"),\n",
        "    (\"‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à\", \"you are welcome\"),\n",
        "    (\"‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§Ø ‡§π‡•Å‡§Ü ‡§π‡•à?\", \"what time is it?\"),\n",
        "    (\"‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ\", \"i do not know\"),\n",
        "    (\"‡§Æ‡•à‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡§æ\", \"i did not understand\"),\n",
        "    (\"‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§π‡§∞‡§æ‡§á‡§è\", \"please repeat\"),\n",
        "]\n",
        "\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "    idx = 4\n",
        "    for sent in sentences:\n",
        "        for word in sent.split():\n",
        "            if word not in vocab:\n",
        "                vocab[word] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "hindi_sentences = [h for h, e in pairs]\n",
        "english_sentences = [e for h, e in pairs]\n",
        "\n",
        "hindi_vocab = build_vocab(hindi_sentences)\n",
        "english_vocab = build_vocab(english_sentences)\n",
        "inv_english_vocab = {v: k for k, v in english_vocab.items()}\n",
        "\n",
        "def tokenize(sentence, vocab):\n",
        "    return [vocab.get(w, vocab[\"<UNK>\"]) for w in sentence.split()]\n",
        "\n",
        "def add_tokens(seq, vocab):\n",
        "    return [vocab[\"<SOS>\"]] + seq + [vocab[\"<EOS>\"]]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, hin_vocab, eng_vocab):\n",
        "        self.data = []\n",
        "        for h, e in pairs:\n",
        "            hin_tokens = add_tokens(tokenize(h, hin_vocab), hin_vocab)\n",
        "            eng_tokens = add_tokens(tokenize(e, eng_vocab), eng_vocab)\n",
        "            self.data.append((torch.tensor(hin_tokens), torch.tensor(eng_tokens)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    hin, eng = zip(*batch)\n",
        "    return (\n",
        "        pad_sequence(hin, batch_first=True, padding_value=hindi_vocab[\"<PAD>\"]),\n",
        "        pad_sequence(eng, batch_first=True, padding_value=english_vocab[\"<PAD>\"])\n",
        "    )\n",
        "\n",
        "dataset = TranslationDataset(pairs, hindi_vocab, english_vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Positional Encoding\n",
        "# ----------------------------\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Transformer Architecture\n",
        "# ----------------------------\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.norm2(x + ff_out)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        tgt_len = tgt.size(1)\n",
        "        mask = torch.triu(torch.ones((tgt_len, tgt_len), device=tgt.device), 1).bool()\n",
        "        self_out, _ = self.self_attn(tgt, tgt, tgt, attn_mask=mask)\n",
        "        tgt = self.norm1(tgt + self_out)\n",
        "\n",
        "        cross_out, _ = self.cross_attn(tgt, memory, memory)\n",
        "        tgt = self.norm2(tgt + cross_out)\n",
        "\n",
        "        ff_out = self.ff(tgt)\n",
        "        tgt = self.norm3(tgt + ff_out)\n",
        "        return tgt\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, src):\n",
        "        x = self.pos(self.embed(src))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, n_heads, d_ff, n_layers):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, tgt, memory):\n",
        "        x = self.pos(self.embed(tgt))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory)\n",
        "        return self.out(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, hin_vocab_size, eng_vocab_size, d_model=32, n_heads=2, d_ff=64, n_layers=1):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(hin_vocab_size, d_model, n_heads, d_ff, n_layers)\n",
        "        self.decoder = Decoder(eng_vocab_size, d_model, n_heads, d_ff, n_layers)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        memory = self.encoder(src)\n",
        "        return self.decoder(tgt, memory)\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Train the Model\n",
        "# ----------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TransformerModel(len(hindi_vocab), len(english_vocab)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=english_vocab[\"<PAD>\"])\n",
        "\n",
        "for epoch in range(90):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for hin, eng in dataloader:\n",
        "        hin, eng = hin.to(device), eng.to(device)\n",
        "        input_eng = eng[:, :-1]\n",
        "        target_eng = eng[:, 1:]\n",
        "\n",
        "        out = model(hin, input_eng)\n",
        "        out = out.reshape(-1, out.shape[-1])\n",
        "        target = target_eng.reshape(-1)\n",
        "\n",
        "        loss = criterion(out, target)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Inference\n",
        "# ----------------------------\n",
        "\n",
        "def translate(hindi_sentence, max_len=12):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        hin = add_tokens(tokenize(hindi_sentence, hindi_vocab), hindi_vocab)\n",
        "        hin_tensor = torch.tensor(hin).unsqueeze(0).to(device)\n",
        "\n",
        "        tgt = torch.tensor([[english_vocab[\"<SOS>\"]]], device=device)\n",
        "        result = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            out = model(hin_tensor, tgt)\n",
        "            next_token = out[0, -1].argmax(-1).item()\n",
        "            if next_token == english_vocab[\"<EOS>\"]:\n",
        "                break\n",
        "            result.append(inv_english_vocab.get(next_token, \"<UNK>\"))\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]], device=device)], dim=1)\n",
        "\n",
        "        return ' '.join(result)\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Evaluate Translations\n",
        "# ----------------------------\n",
        "\n",
        "print(\"\\nTranslations:\\n\")\n",
        "for h, _ in pairs:\n",
        "    print(f\"{h} ‚Üí {translate(h)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMxAzUYTQDU-",
        "outputId": "fd43c2df-1640-4543-dde5-e7c60055951a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Loss: 68.5514\n",
            "Epoch 10, Loss: 40.3118\n",
            "Epoch 15, Loss: 24.7517\n",
            "Epoch 20, Loss: 14.5274\n",
            "Epoch 25, Loss: 8.3308\n",
            "Epoch 30, Loss: 4.5063\n",
            "Epoch 35, Loss: 3.2067\n",
            "Epoch 40, Loss: 1.5605\n",
            "Epoch 45, Loss: 1.0506\n",
            "Epoch 50, Loss: 0.7618\n",
            "Epoch 55, Loss: 0.5921\n",
            "Epoch 60, Loss: 0.4638\n",
            "Epoch 65, Loss: 0.3820\n",
            "Epoch 70, Loss: 0.3142\n",
            "Epoch 75, Loss: 0.2628\n",
            "Epoch 80, Loss: 0.2204\n",
            "Epoch 85, Loss: 0.1898\n",
            "Epoch 90, Loss: 0.1635\n",
            "\n",
            "Translations:\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å ‚Üí i am happy\n",
            "‡§Ü‡§™ ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•à‡§Ç ‚Üí you are sad\n",
            "‡§µ‡§π ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à ‚Üí he is tired\n",
            "‡§µ‡§π ‡§•‡§ï‡•Ä ‡§π‡•Å‡§à ‡§π‡•à ‚Üí she is tired\n",
            "‡§π‡§Æ ‡§≠‡•Ç‡§ñ‡•á‡§Ç ‡§π‡•à‡§Ç ‚Üí we are hungry\n",
            "‡§µ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç ‚Üí they are busy\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§†‡§Ç‡§° ‡§≤‡§ó ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí i am cold\n",
            "‡§§‡•Å‡§Æ ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§Ü‡§è ‡§π‡•ã ‚Üí you are late\n",
            "‡§µ‡§π ‡§ñ‡•Å‡§∂ ‡§π‡•à ‚Üí she is happy\n",
            "‡§π‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç ‚Üí we are ready\n",
            "‡§Æ‡•à‡§Ç ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•Ç‡§Å ‚Üí i am sad\n",
            "‡§§‡•Å‡§Æ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‚Üí you are happy\n",
            "‡§µ‡•á ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç ‚Üí they are ready\n",
            "‡§µ‡§π ‡§•‡§ï‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à ‚Üí he is tired\n",
            "‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã ‚Üí you are busy\n",
            "‡§π‡§Æ ‡§†‡§Ç‡§°‡•á ‡§π‡•à‡§Ç ‚Üí we are cold\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç? ‚Üí are you ready?\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§π ‡§Ü ‡§∞‡§π‡•Ä ‡§π‡•à? ‚Üí is she coming?\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§†‡•Ä‡§ï ‡§π‡•ã? ‚Üí are you okay?\n",
            "‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å ‚Üí i am fine\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã? ‚Üí are you busy?\n",
            "‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‚Üí he is going to school\n",
            "‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí she is going to the market\n",
            "‡§Æ‡•à‡§Ç ‡§™‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am studying\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã? ‚Üí what are you doing?\n",
            "‡§Æ‡•à‡§Ç ‡§∏‡•ã ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am sleeping\n",
            "‡§§‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•ã? ‚Üí where are you going?\n",
            "‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am going home\n",
            "‡§µ‡§π ‡§ü‡•Ä‡§µ‡•Ä ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à ‚Üí he is watching TV\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§ö‡•Å‡§ï‡•á ‡§π‡•ã? ‚Üí have you eaten?\n",
            "‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à ‚Üí i have eaten\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•å‡§® ‡§π‡•ã? ‚Üí who are you?\n",
            "‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å ‚Üí i am a student\n",
            "‡§µ‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§π‡•à ‚Üí he is a doctor\n",
            "‡§µ‡§π ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à ‚Üí she is a teacher\n",
            "‡§π‡§Æ ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç ‚Üí we are friends\n",
            "‡§§‡•Å‡§Æ ‡§Æ‡•á‡§∞‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã ‚Üí you are my friend\n",
            "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à ‚Üí the weather is good today\n",
            "‡§¨‡§æ‡§π‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí it is raining outside\n",
            "‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§†‡§Ç‡§° ‡§π‡•à ‚Üí it is very cold today\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç? ‚Üí do you speak english?\n",
            "‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Å ‚Üí yes, i speak english\n",
            "‡§®‡§π‡•Ä‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¨‡•ã‡§≤‡§§‡§æ ‚Üí no, i do not speak english\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡§ø‡§è ‚Üí please speak slowly\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç ‚Üí i am sorry\n",
            "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ ‚Üí thank you\n",
            "‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à ‚Üí you are welcome\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§Ø ‡§π‡•Å‡§Ü ‡§π‡•à? ‚Üí what time is it?\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‚Üí i do not know\n",
            "‡§Æ‡•à‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡§æ ‚Üí i did not understand\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§π‡§∞‡§æ‡§á‡§è ‚Üí please repeat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvViEKEgRraC",
        "outputId": "20ea1194-cdb5-4ae1-a182-c4367af25924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu():\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    scores = []\n",
        "\n",
        "    for hin, eng in pairs:\n",
        "        reference = eng.split()\n",
        "        prediction = translate(hin).split()\n",
        "        score = sentence_bleu([reference], prediction, smoothing_function=smoothie)\n",
        "        scores.append(score)\n",
        "        print(f\"{hin} ‚Üí {translate(hin)}\")\n",
        "        print(f\"Ref: {reference}\")\n",
        "        print(f\"Hyp: {prediction}\")\n",
        "        print(f\"BLEU: {score:.4f}\\n\")\n",
        "\n",
        "    avg_bleu = sum(scores) / len(scores)\n",
        "    print(f\"\\nüîµ Average BLEU Score: {avg_bleu:.4f} ({avg_bleu*100:.2f})\")\n",
        "\n",
        "evaluate_bleu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7iiEnffR2gB",
        "outputId": "25f47da4-bef1-48d7-922a-a88da4f50f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å ‚Üí i am happy\n",
            "Ref: ['i', 'am', 'happy']\n",
            "Hyp: ['i', 'am', 'happy']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Ü‡§™ ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•à‡§Ç ‚Üí you are sad\n",
            "Ref: ['you', 'are', 'sad']\n",
            "Hyp: ['you', 'are', 'sad']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡§π ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à ‚Üí he is tired\n",
            "Ref: ['he', 'is', 'tired']\n",
            "Hyp: ['he', 'is', 'tired']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡§π ‡§•‡§ï‡•Ä ‡§π‡•Å‡§à ‡§π‡•à ‚Üí she is tired\n",
            "Ref: ['she', 'is', 'tired']\n",
            "Hyp: ['she', 'is', 'tired']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§π‡§Æ ‡§≠‡•Ç‡§ñ‡•á‡§Ç ‡§π‡•à‡§Ç ‚Üí we are hungry\n",
            "Ref: ['we', 'are', 'hungry']\n",
            "Hyp: ['we', 'are', 'hungry']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç ‚Üí they are busy\n",
            "Ref: ['they', 'are', 'busy']\n",
            "Hyp: ['they', 'are', 'busy']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§†‡§Ç‡§° ‡§≤‡§ó ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí i am cold\n",
            "Ref: ['i', 'am', 'cold']\n",
            "Hyp: ['i', 'am', 'cold']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§Ü‡§è ‡§π‡•ã ‚Üí you are late\n",
            "Ref: ['you', 'are', 'late']\n",
            "Hyp: ['you', 'are', 'late']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡§π ‡§ñ‡•Å‡§∂ ‡§π‡•à ‚Üí she is happy\n",
            "Ref: ['she', 'is', 'happy']\n",
            "Hyp: ['she', 'is', 'happy']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§π‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç ‚Üí we are ready\n",
            "Ref: ['we', 'are', 'ready']\n",
            "Hyp: ['we', 'are', 'ready']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•Ç‡§Å ‚Üí i am sad\n",
            "Ref: ['i', 'am', 'sad']\n",
            "Hyp: ['i', 'am', 'sad']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‚Üí you are happy\n",
            "Ref: ['you', 'are', 'happy']\n",
            "Hyp: ['you', 'are', 'happy']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡•á ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç ‚Üí they are ready\n",
            "Ref: ['they', 'are', 'ready']\n",
            "Hyp: ['they', 'are', 'ready']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡§π ‡§•‡§ï‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à ‚Üí he is tired\n",
            "Ref: ['he', 'is', 'tired']\n",
            "Hyp: ['he', 'is', 'tired']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã ‚Üí you are busy\n",
            "Ref: ['you', 'are', 'busy']\n",
            "Hyp: ['you', 'are', 'busy']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§π‡§Æ ‡§†‡§Ç‡§°‡•á ‡§π‡•à‡§Ç ‚Üí we are cold\n",
            "Ref: ['we', 'are', 'cold']\n",
            "Hyp: ['we', 'are', 'cold']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç? ‚Üí are you ready?\n",
            "Ref: ['are', 'you', 'ready?']\n",
            "Hyp: ['are', 'you', 'ready?']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§π ‡§Ü ‡§∞‡§π‡•Ä ‡§π‡•à? ‚Üí is she coming?\n",
            "Ref: ['is', 'she', 'coming?']\n",
            "Hyp: ['is', 'she', 'coming?']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§†‡•Ä‡§ï ‡§π‡•ã? ‚Üí are you okay?\n",
            "Ref: ['are', 'you', 'okay?']\n",
            "Hyp: ['are', 'you', 'okay?']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å ‚Üí i am fine\n",
            "Ref: ['i', 'am', 'fine']\n",
            "Hyp: ['i', 'am', 'fine']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã? ‚Üí are you busy?\n",
            "Ref: ['are', 'you', 'busy?']\n",
            "Hyp: ['are', 'you', 'busy?']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‚Üí he is going to school\n",
            "Ref: ['he', 'is', 'going', 'to', 'school']\n",
            "Hyp: ['he', 'is', 'going', 'to', 'school']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí she is going to the market\n",
            "Ref: ['she', 'is', 'going', 'to', 'the', 'market']\n",
            "Hyp: ['she', 'is', 'going', 'to', 'the', 'market']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§™‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am studying\n",
            "Ref: ['i', 'am', 'studying']\n",
            "Hyp: ['i', 'am', 'studying']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã? ‚Üí what are you doing?\n",
            "Ref: ['what', 'are', 'you', 'doing?']\n",
            "Hyp: ['what', 'are', 'you', 'doing?']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§∏‡•ã ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am sleeping\n",
            "Ref: ['i', 'am', 'sleeping']\n",
            "Hyp: ['i', 'am', 'sleeping']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•ã? ‚Üí where are you going?\n",
            "Ref: ['where', 'are', 'you', 'going?']\n",
            "Hyp: ['where', 'are', 'you', 'going?']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am going home\n",
            "Ref: ['i', 'am', 'going', 'home']\n",
            "Hyp: ['i', 'am', 'going', 'home']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§µ‡§π ‡§ü‡•Ä‡§µ‡•Ä ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à ‚Üí he is watching TV\n",
            "Ref: ['he', 'is', 'watching', 'TV']\n",
            "Hyp: ['he', 'is', 'watching', 'TV']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§ö‡•Å‡§ï‡•á ‡§π‡•ã? ‚Üí have you eaten?\n",
            "Ref: ['have', 'you', 'eaten?']\n",
            "Hyp: ['have', 'you', 'eaten?']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à ‚Üí i have eaten\n",
            "Ref: ['i', 'have', 'eaten']\n",
            "Hyp: ['i', 'have', 'eaten']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•å‡§® ‡§π‡•ã? ‚Üí who are you?\n",
            "Ref: ['who', 'are', 'you?']\n",
            "Hyp: ['who', 'are', 'you?']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å ‚Üí i am a student\n",
            "Ref: ['i', 'am', 'a', 'student']\n",
            "Hyp: ['i', 'am', 'a', 'student']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§µ‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§π‡•à ‚Üí he is a doctor\n",
            "Ref: ['he', 'is', 'a', 'doctor']\n",
            "Hyp: ['he', 'is', 'a', 'doctor']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§µ‡§π ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à ‚Üí she is a teacher\n",
            "Ref: ['she', 'is', 'a', 'teacher']\n",
            "Hyp: ['she', 'is', 'a', 'teacher']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§π‡§Æ ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç ‚Üí we are friends\n",
            "Ref: ['we', 'are', 'friends']\n",
            "Hyp: ['we', 'are', 'friends']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§Æ‡•á‡§∞‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã ‚Üí you are my friend\n",
            "Ref: ['you', 'are', 'my', 'friend']\n",
            "Hyp: ['you', 'are', 'my', 'friend']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à ‚Üí the weather is good today\n",
            "Ref: ['the', 'weather', 'is', 'good', 'today']\n",
            "Hyp: ['the', 'weather', 'is', 'good', 'today']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§¨‡§æ‡§π‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí it is raining outside\n",
            "Ref: ['it', 'is', 'raining', 'outside']\n",
            "Hyp: ['it', 'is', 'raining', 'outside']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§†‡§Ç‡§° ‡§π‡•à ‚Üí it is very cold today\n",
            "Ref: ['it', 'is', 'very', 'cold', 'today']\n",
            "Hyp: ['it', 'is', 'very', 'cold', 'today']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç? ‚Üí do you speak english?\n",
            "Ref: ['do', 'you', 'speak', 'english?']\n",
            "Hyp: ['do', 'you', 'speak', 'english?']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Å ‚Üí yes, i speak english\n",
            "Ref: ['yes,', 'i', 'speak', 'english']\n",
            "Hyp: ['yes,', 'i', 'speak', 'english']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§®‡§π‡•Ä‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¨‡•ã‡§≤‡§§‡§æ ‚Üí no, i do not speak english\n",
            "Ref: ['no,', 'i', 'do', 'not', 'speak', 'english']\n",
            "Hyp: ['no,', 'i', 'do', 'not', 'speak', 'english']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡§ø‡§è ‚Üí please speak slowly\n",
            "Ref: ['please', 'speak', 'slowly']\n",
            "Hyp: ['please', 'speak', 'slowly']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç ‚Üí i am sorry\n",
            "Ref: ['i', 'am', 'sorry']\n",
            "Hyp: ['i', 'am', 'sorry']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ ‚Üí thank you\n",
            "Ref: ['thank', 'you']\n",
            "Hyp: ['thank', 'you']\n",
            "BLEU: 0.2214\n",
            "\n",
            "‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à ‚Üí you are welcome\n",
            "Ref: ['you', 'are', 'welcome']\n",
            "Hyp: ['you', 'are', 'welcome']\n",
            "BLEU: 0.5757\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§Ø ‡§π‡•Å‡§Ü ‡§π‡•à? ‚Üí what time is it?\n",
            "Ref: ['what', 'time', 'is', 'it?']\n",
            "Hyp: ['what', 'time', 'is', 'it?']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‚Üí i do not know\n",
            "Ref: ['i', 'do', 'not', 'know']\n",
            "Hyp: ['i', 'do', 'not', 'know']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡§æ ‚Üí i did not understand\n",
            "Ref: ['i', 'did', 'not', 'understand']\n",
            "Hyp: ['i', 'did', 'not', 'understand']\n",
            "BLEU: 1.0000\n",
            "\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§π‡§∞‡§æ‡§á‡§è ‚Üí please repeat\n",
            "Ref: ['please', 'repeat']\n",
            "Hyp: ['please', 'repeat']\n",
            "BLEU: 0.2214\n",
            "\n",
            "\n",
            "üîµ Average BLEU Score: 0.7199 (71.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction"
      ],
      "metadata": {
        "id": "3y7xzRV3R4TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_corpus_bleu():\n",
        "    references = []  # List of [reference] for each sentence\n",
        "    hypotheses = []  # List of predicted tokens\n",
        "\n",
        "    for hin, eng in pairs:\n",
        "        reference = eng.split()\n",
        "        prediction = translate(hin).split()\n",
        "\n",
        "        references.append([reference])  # each ref wrapped in another list\n",
        "        hypotheses.append(prediction)\n",
        "\n",
        "        print(f\"{hin} ‚Üí {translate(hin)}\")\n",
        "        print(f\"Ref: {reference}\")\n",
        "        print(f\"Hyp: {prediction}\\n\")\n",
        "\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothie)\n",
        "    print(f\"üîµ Corpus BLEU Score: {bleu:.4f} ({bleu*100:.2f})\")"
      ],
      "metadata": {
        "id": "M0fnrVCFSGLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_corpus_bleu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9Ai4p2WSI7K",
        "outputId": "3fc1d449-58e3-4983-edf3-6085b3213a93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‡§Æ‡•à‡§Ç ‡§ñ‡•Å‡§∂ ‡§π‡•Ç‡§Å ‚Üí i am happy\n",
            "Ref: ['i', 'am', 'happy']\n",
            "Hyp: ['i', 'am', 'happy']\n",
            "\n",
            "‡§Ü‡§™ ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•à‡§Ç ‚Üí you are sad\n",
            "Ref: ['you', 'are', 'sad']\n",
            "Hyp: ['you', 'are', 'sad']\n",
            "\n",
            "‡§µ‡§π ‡§•‡§ï ‡§ó‡§Ø‡§æ ‡§π‡•à ‚Üí he is tired\n",
            "Ref: ['he', 'is', 'tired']\n",
            "Hyp: ['he', 'is', 'tired']\n",
            "\n",
            "‡§µ‡§π ‡§•‡§ï‡•Ä ‡§π‡•Å‡§à ‡§π‡•à ‚Üí she is tired\n",
            "Ref: ['she', 'is', 'tired']\n",
            "Hyp: ['she', 'is', 'tired']\n",
            "\n",
            "‡§π‡§Æ ‡§≠‡•Ç‡§ñ‡•á‡§Ç ‡§π‡•à‡§Ç ‚Üí we are hungry\n",
            "Ref: ['we', 'are', 'hungry']\n",
            "Hyp: ['we', 'are', 'hungry']\n",
            "\n",
            "‡§µ‡•á ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç ‚Üí they are busy\n",
            "Ref: ['they', 'are', 'busy']\n",
            "Hyp: ['they', 'are', 'busy']\n",
            "\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§†‡§Ç‡§° ‡§≤‡§ó ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí i am cold\n",
            "Ref: ['i', 'am', 'cold']\n",
            "Hyp: ['i', 'am', 'cold']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§¶‡•á‡§∞‡•Ä ‡§∏‡•á ‡§Ü‡§è ‡§π‡•ã ‚Üí you are late\n",
            "Ref: ['you', 'are', 'late']\n",
            "Hyp: ['you', 'are', 'late']\n",
            "\n",
            "‡§µ‡§π ‡§ñ‡•Å‡§∂ ‡§π‡•à ‚Üí she is happy\n",
            "Ref: ['she', 'is', 'happy']\n",
            "Hyp: ['she', 'is', 'happy']\n",
            "\n",
            "‡§π‡§Æ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç ‚Üí we are ready\n",
            "Ref: ['we', 'are', 'ready']\n",
            "Hyp: ['we', 'are', 'ready']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§¶‡•Å‡§ñ‡•Ä ‡§π‡•Ç‡§Å ‚Üí i am sad\n",
            "Ref: ['i', 'am', 'sad']\n",
            "Hyp: ['i', 'am', 'sad']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ñ‡•Å‡§∂ ‡§π‡•ã ‚Üí you are happy\n",
            "Ref: ['you', 'are', 'happy']\n",
            "Hyp: ['you', 'are', 'happy']\n",
            "\n",
            "‡§µ‡•á ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç ‚Üí they are ready\n",
            "Ref: ['they', 'are', 'ready']\n",
            "Hyp: ['they', 'are', 'ready']\n",
            "\n",
            "‡§µ‡§π ‡§•‡§ï‡§æ ‡§π‡•Å‡§Ü ‡§π‡•à ‚Üí he is tired\n",
            "Ref: ['he', 'is', 'tired']\n",
            "Hyp: ['he', 'is', 'tired']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã ‚Üí you are busy\n",
            "Ref: ['you', 'are', 'busy']\n",
            "Hyp: ['you', 'are', 'busy']\n",
            "\n",
            "‡§π‡§Æ ‡§†‡§Ç‡§°‡•á ‡§π‡•à‡§Ç ‚Üí we are cold\n",
            "Ref: ['we', 'are', 'cold']\n",
            "Hyp: ['we', 'are', 'cold']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•à‡§Ç? ‚Üí are you ready?\n",
            "Ref: ['are', 'you', 'ready?']\n",
            "Hyp: ['are', 'you', 'ready?']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§µ‡§π ‡§Ü ‡§∞‡§π‡•Ä ‡§π‡•à? ‚Üí is she coming?\n",
            "Ref: ['is', 'she', 'coming?']\n",
            "Hyp: ['is', 'she', 'coming?']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§†‡•Ä‡§ï ‡§π‡•ã? ‚Üí are you okay?\n",
            "Ref: ['are', 'you', 'okay?']\n",
            "Hyp: ['are', 'you', 'okay?']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§†‡•Ä‡§ï ‡§π‡•Ç‡§Å ‚Üí i am fine\n",
            "Ref: ['i', 'am', 'fine']\n",
            "Hyp: ['i', 'am', 'fine']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§π‡•ã? ‚Üí are you busy?\n",
            "Ref: ['are', 'you', 'busy?']\n",
            "Hyp: ['are', 'you', 'busy?']\n",
            "\n",
            "‡§µ‡§π ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•à ‚Üí he is going to school\n",
            "Ref: ['he', 'is', 'going', 'to', 'school']\n",
            "Hyp: ['he', 'is', 'going', 'to', 'school']\n",
            "\n",
            "‡§µ‡§π ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§ú‡§æ ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí she is going to the market\n",
            "Ref: ['she', 'is', 'going', 'to', 'the', 'market']\n",
            "Hyp: ['she', 'is', 'going', 'to', 'the', 'market']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§™‡§¢‡§º ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am studying\n",
            "Ref: ['i', 'am', 'studying']\n",
            "Hyp: ['i', 'am', 'studying']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞ ‡§∞‡§π‡•á ‡§π‡•ã? ‚Üí what are you doing?\n",
            "Ref: ['what', 'are', 'you', 'doing?']\n",
            "Hyp: ['what', 'are', 'you', 'doing?']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§∏‡•ã ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am sleeping\n",
            "Ref: ['i', 'am', 'sleeping']\n",
            "Hyp: ['i', 'am', 'sleeping']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•ã? ‚Üí where are you going?\n",
            "Ref: ['where', 'are', 'you', 'going?']\n",
            "Hyp: ['where', 'are', 'you', 'going?']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§ò‡§∞ ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å ‚Üí i am going home\n",
            "Ref: ['i', 'am', 'going', 'home']\n",
            "Hyp: ['i', 'am', 'going', 'home']\n",
            "\n",
            "‡§µ‡§π ‡§ü‡•Ä‡§µ‡•Ä ‡§¶‡•á‡§ñ ‡§∞‡§π‡§æ ‡§π‡•à ‚Üí he is watching TV\n",
            "Ref: ['he', 'is', 'watching', 'TV']\n",
            "Hyp: ['he', 'is', 'watching', 'TV']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§§‡•Å‡§Æ ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§ö‡•Å‡§ï‡•á ‡§π‡•ã? ‚Üí have you eaten?\n",
            "Ref: ['have', 'you', 'eaten?']\n",
            "Hyp: ['have', 'you', 'eaten?']\n",
            "\n",
            "‡§Æ‡•à‡§Ç‡§®‡•á ‡§ñ‡§æ‡§®‡§æ ‡§ñ‡§æ ‡§≤‡§ø‡§Ø‡§æ ‡§π‡•à ‚Üí i have eaten\n",
            "Ref: ['i', 'have', 'eaten']\n",
            "Hyp: ['i', 'have', 'eaten']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§ï‡•å‡§® ‡§π‡•ã? ‚Üí who are you?\n",
            "Ref: ['who', 'are', 'you?']\n",
            "Hyp: ['who', 'are', 'you?']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§è‡§ï ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§π‡•Ç‡§Å ‚Üí i am a student\n",
            "Ref: ['i', 'am', 'a', 'student']\n",
            "Hyp: ['i', 'am', 'a', 'student']\n",
            "\n",
            "‡§µ‡§π ‡§°‡•â‡§ï‡•ç‡§ü‡§∞ ‡§π‡•à ‚Üí he is a doctor\n",
            "Ref: ['he', 'is', 'a', 'doctor']\n",
            "Hyp: ['he', 'is', 'a', 'doctor']\n",
            "\n",
            "‡§µ‡§π ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•à ‚Üí she is a teacher\n",
            "Ref: ['she', 'is', 'a', 'teacher']\n",
            "Hyp: ['she', 'is', 'a', 'teacher']\n",
            "\n",
            "‡§π‡§Æ ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•à‡§Ç ‚Üí we are friends\n",
            "Ref: ['we', 'are', 'friends']\n",
            "Hyp: ['we', 'are', 'friends']\n",
            "\n",
            "‡§§‡•Å‡§Æ ‡§Æ‡•á‡§∞‡•á ‡§¶‡•ã‡§∏‡•ç‡§§ ‡§π‡•ã ‚Üí you are my friend\n",
            "Ref: ['you', 'are', 'my', 'friend']\n",
            "Hyp: ['you', 'are', 'my', 'friend']\n",
            "\n",
            "‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§Ö‡§ö‡•ç‡§õ‡§æ ‡§π‡•à ‚Üí the weather is good today\n",
            "Ref: ['the', 'weather', 'is', 'good', 'today']\n",
            "Hyp: ['the', 'weather', 'is', 'good', 'today']\n",
            "\n",
            "‡§¨‡§æ‡§π‡§∞ ‡§¨‡§æ‡§∞‡§ø‡§∂ ‡§π‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à ‚Üí it is raining outside\n",
            "Ref: ['it', 'is', 'raining', 'outside']\n",
            "Hyp: ['it', 'is', 'raining', 'outside']\n",
            "\n",
            "‡§Ü‡§ú ‡§¨‡§π‡•Å‡§§ ‡§†‡§Ç‡§° ‡§π‡•à ‚Üí it is very cold today\n",
            "Ref: ['it', 'is', 'very', 'cold', 'today']\n",
            "Hyp: ['it', 'is', 'very', 'cold', 'today']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§Ü‡§™ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡•á ‡§π‡•à‡§Ç? ‚Üí do you speak english?\n",
            "Ref: ['do', 'you', 'speak', 'english?']\n",
            "Hyp: ['do', 'you', 'speak', 'english?']\n",
            "\n",
            "‡§π‡§æ‡§Å, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§¨‡•ã‡§≤‡§§‡§æ ‡§π‡•Ç‡§Å ‚Üí yes, i speak english\n",
            "Ref: ['yes,', 'i', 'speak', 'english']\n",
            "Hyp: ['yes,', 'i', 'speak', 'english']\n",
            "\n",
            "‡§®‡§π‡•Ä‡§Ç, ‡§Æ‡•à‡§Ç ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§¨‡•ã‡§≤‡§§‡§æ ‚Üí no, i do not speak english\n",
            "Ref: ['no,', 'i', 'do', 'not', 'speak', 'english']\n",
            "Hyp: ['no,', 'i', 'do', 'not', 'speak', 'english']\n",
            "\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§ß‡•Ä‡§∞‡•á ‡§¨‡•ã‡§≤‡§ø‡§è ‚Üí please speak slowly\n",
            "Ref: ['please', 'speak', 'slowly']\n",
            "Hyp: ['please', 'speak', 'slowly']\n",
            "\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§Æ‡§æ‡§´‡§º ‡§ï‡§∞‡•á‡§Ç ‚Üí i am sorry\n",
            "Ref: ['i', 'am', 'sorry']\n",
            "Hyp: ['i', 'am', 'sorry']\n",
            "\n",
            "‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ ‚Üí thank you\n",
            "Ref: ['thank', 'you']\n",
            "Hyp: ['thank', 'you']\n",
            "\n",
            "‡§∏‡•ç‡§µ‡§æ‡§ó‡§§ ‡§π‡•à ‚Üí you are welcome\n",
            "Ref: ['you', 'are', 'welcome']\n",
            "Hyp: ['you', 'are', 'welcome']\n",
            "\n",
            "‡§ï‡•ç‡§Ø‡§æ ‡§∏‡§Æ‡§Ø ‡§π‡•Å‡§Ü ‡§π‡•à? ‚Üí what time is it?\n",
            "Ref: ['what', 'time', 'is', 'it?']\n",
            "Hyp: ['what', 'time', 'is', 'it?']\n",
            "\n",
            "‡§Æ‡•Å‡§ù‡•á ‡§®‡§π‡•Ä‡§Ç ‡§™‡§§‡§æ ‚Üí i do not know\n",
            "Ref: ['i', 'do', 'not', 'know']\n",
            "Hyp: ['i', 'do', 'not', 'know']\n",
            "\n",
            "‡§Æ‡•à‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§∏‡§Æ‡§ù‡§æ ‚Üí i did not understand\n",
            "Ref: ['i', 'did', 'not', 'understand']\n",
            "Hyp: ['i', 'did', 'not', 'understand']\n",
            "\n",
            "‡§ï‡•É‡§™‡§Ø‡§æ ‡§¶‡•ã‡§π‡§∞‡§æ‡§á‡§è ‚Üí please repeat\n",
            "Ref: ['please', 'repeat']\n",
            "Hyp: ['please', 'repeat']\n",
            "\n",
            "üîµ Corpus BLEU Score: 0.8129 (81.29)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Here's a simple example of how to use a **pre-trained Transformer model** using the **Hugging Face Transformers** library. We'll use a **BERT-based model for sentiment analysis**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Install Requirements\n",
        "\n",
        "If you haven‚Äôt already installed `transformers` and `torch`, run this:\n",
        "\n",
        "```bash\n",
        "pip install transformers torch\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Load and Use Pre-trained Transformer (e.g., `distilbert-base-uncased`)\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained sentiment analysis pipeline\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Test the model\n",
        "text = \"I love using transformers! They make NLP so easy.\"\n",
        "result = classifier(text)\n",
        "\n",
        "print(result)\n",
        "```\n",
        "\n",
        "**Output:**\n",
        "\n",
        "```python\n",
        "[{'label': 'POSITIVE', 'score': 0.9998}]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Custom Example: Load Pre-trained Model and Tokenizer\n",
        "\n",
        "You can also manually load the model and tokenizer:\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Load pre-trained tokenizer and model\n",
        "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Tokenize input\n",
        "inputs = tokenizer(\"Transformers are powerful!\", return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get prediction\n",
        "logits = outputs.logits\n",
        "predicted_class_id = torch.argmax(logits).item()\n",
        "\n",
        "# Convert class ID to label\n",
        "label = model.config.id2label[predicted_class_id]\n",
        "print(f\"Predicted Label: {label}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Common Tasks with Transformers\n",
        "\n",
        "You can load different tasks by changing the `pipeline()` type:\n",
        "\n",
        "| Task               | Pipeline Type            | Example Model                                           |\n",
        "| ------------------ | ------------------------ | ------------------------------------------------------- |\n",
        "| Sentiment Analysis | `\"sentiment-analysis\"`   | `distilbert-base-uncased`                               |\n",
        "| Question Answering | `\"question-answering\"`   | `bert-large-uncased-whole-word-masking-finetuned-squad` |\n",
        "| Text Generation    | `\"text-generation\"`      | `gpt2`                                                  |\n",
        "| Translation        | `\"translation_en_to_fr\"` | `t5-small`                                              |\n",
        "| Summarization      | `\"summarization\"`        | `facebook/bart-large-cnn`                               |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "K7TvIttBJSUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perfect! Here's a collection of **ready-to-run code snippets** using **popular Transformer models** from the [Hugging Face ü§ó `transformers`](https://huggingface.co/models) library ‚Äî for BERT, GPT-2, RoBERTa, T5, and BART ‚Äî covering the **most common NLP tasks**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 1. **BERT for Text Classification (Sentiment Analysis)**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment-analysis pipeline with BERT fine-tuned on SST-2\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "# Predict\n",
        "text = \"This is a fantastic movie!\"\n",
        "result = classifier(text)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 2. **GPT-2 for Text Generation**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load GPT-2 text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "# Generate text\n",
        "prompt = \"In the future, AI will\"\n",
        "result = generator(prompt, max_length=50, num_return_sequences=1)\n",
        "print(result[0]['generated_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 3. **RoBERTa for Sentiment Classification**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load RoBERTa fine-tuned for sentiment\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "\n",
        "# Predict\n",
        "text = \"I'm feeling awesome today!\"\n",
        "result = classifier(text)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 4. **T5 for Text-to-Text Tasks (e.g., Translation, Summarization)**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load summarization pipeline with T5\n",
        "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
        "\n",
        "# Summarize\n",
        "text = \"The Transformers library by Hugging Face makes it easy to use pretrained models for NLP tasks.\"\n",
        "result = summarizer(text, max_length=30, min_length=5, do_sample=False)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 5. **BART for Summarization**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load BART model for summarization\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "text = \"\"\"\n",
        "Machine learning is a method of data analysis that automates analytical model building.\n",
        "It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns and make decisions with minimal human intervention.\n",
        "\"\"\"\n",
        "summary = summarizer(text, max_length=50, min_length=25, do_sample=False)\n",
        "print(summary)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 6. **T5 for English to French Translation**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# T5 expects prompts for tasks\n",
        "translator = pipeline(\"translation_en_to_fr\", model=\"t5-small\")\n",
        "\n",
        "text = \"The weather is nice today.\"\n",
        "result = translator(text)\n",
        "print(result[0]['translation_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ 7. **BERT for Question Answering**\n",
        "\n",
        "```python\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load BERT QA pipeline\n",
        "qa = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
        "\n",
        "# Ask question\n",
        "context = \"Hugging Face is a company that develops tools for natural language processing.\"\n",
        "question = \"What does Hugging Face do?\"\n",
        "\n",
        "result = qa(question=question, context=context)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary Table\n",
        "\n",
        "| Model       | Task                       | Pretrained Example                          |\n",
        "| ----------- | -------------------------- | ------------------------------------------- |\n",
        "| **BERT**    | Classification, QA         | `bert-base-uncased`, SST-2, SQuAD           |\n",
        "| **GPT-2**   | Text Generation            | `gpt2`                                      |\n",
        "| **RoBERTa** | Classification             | `cardiffnlp/twitter-roberta-base-sentiment` |\n",
        "| **T5**      | Translation, Summarization | `t5-small`                                  |\n",
        "| **BART**    | Summarization              | `facebook/bart-large-cnn`                   |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Zu3Zc0h6JWXr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ubxqyY6ZSKi7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}