{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "You're absolutely right â€” and great observation!\n",
        "\n",
        "âœ… **Yes, feature engineering absolutely includes**:\n",
        "\n",
        "1. **Feature Selection** â€“ Choosing which features to keep\n",
        "2. **Feature Elimination** â€“ Dropping irrelevant or redundant features\n",
        "3. **Feature Importance** â€“ Measuring how useful a feature is for prediction\n",
        "\n",
        "These are **critical components of the feature engineering process**, and I did briefly include them under **section 9**, but let's now **expand and group them clearly** with **full code examples**.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”Ž FEATURE ENGINEERING: Selection, Elimination & Importance\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 1. **Feature Selection (Keep Most Relevant Features)**\n",
        "\n",
        "#### âœ… a. **Univariate Selection (Filter Method)**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Select top 10 features with highest ANOVA F-score\n",
        "selector = SelectKBest(score_func=f_classif, k=10)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "selected_features = X.columns[selector.get_support()]\n",
        "print(selected_features)\n",
        "```\n",
        "\n",
        "#### âœ… b. **Recursive Feature Elimination (Wrapper Method)**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "rfe = RFE(estimator=model, n_features_to_select=5)\n",
        "X_rfe = rfe.fit_transform(X, y)\n",
        "\n",
        "selected = X.columns[rfe.support_]\n",
        "print(selected)\n",
        "```\n",
        "\n",
        "#### âœ… c. **Variance Threshold (Remove Low-Variance Features)**\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_high_variance = selector.fit_transform(X)\n",
        "selected = X.columns[selector.get_support()]\n",
        "print(selected)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 2. **Feature Elimination (Remove Redundant/Correlated Features)**\n",
        "\n",
        "#### âœ… a. **Using Correlation Matrix**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "corr_matrix = df.corr().abs()\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
        "df_reduced = df.drop(columns=to_drop)\n",
        "print(\"Dropped due to high correlation:\", to_drop)\n",
        "```\n",
        "\n",
        "#### âœ… b. **Using VIF (Variance Inflation Factor)**\n",
        "\n",
        "```python\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = X.columns\n",
        "vif[\"VIF\"] = [variance_inflation_factor(X_scaled, i) for i in range(X_scaled.shape[1])]\n",
        "print(vif)\n",
        "\n",
        "# Drop features with VIF > 10 (rule of thumb)\n",
        "high_vif = vif[vif[\"VIF\"] > 10][\"Feature\"]\n",
        "X_reduced = X.drop(columns=high_vif)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”¹ 3. **Feature Importance (Measure Feature Utility)**\n",
        "\n",
        "#### âœ… a. **Tree-Based Models**\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X, y)\n",
        "importances = model.feature_importances_\n",
        "\n",
        "feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=False)\n",
        "feat_imp.plot(kind='bar', title='Feature Importance (Random Forest)')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "#### âœ… b. **Using SHAP Values (Advanced, Model-Agnostic Explainability)**\n",
        "\n",
        "```python\n",
        "import shap\n",
        "\n",
        "model = RandomForestClassifier().fit(X, y)\n",
        "explainer = shap.Explainer(model, X)\n",
        "shap_values = explainer(X)\n",
        "\n",
        "shap.plots.beeswarm(shap_values)  # visualization\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Summary Table\n",
        "\n",
        "| Method                | Category       | Library          | Good For                                 |\n",
        "| --------------------- | -------------- | ---------------- | ---------------------------------------- |\n",
        "| ANOVA F-test          | Filter         | `sklearn`        | Classification with numeric features     |\n",
        "| RFE                   | Wrapper        | `sklearn`        | General-purpose models                   |\n",
        "| Variance Threshold    | Filter         | `sklearn`        | Removing uninformative features          |\n",
        "| Correlation Matrix    | Filter         | `pandas + numpy` | Remove multicollinearity                 |\n",
        "| VIF                   | Filter         | `statsmodels`    | Remove multicollinearity (linear models) |\n",
        "| Tree-based Importance | Embedded       | `sklearn`        | Random Forest, XGBoost, etc.             |\n",
        "| SHAP / LIME           | Model-Agnostic | `shap`, `lime`   | Explainability & interpretability        |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "R-hJIMEIUdIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely! Here's a comprehensive guide to **feature engineering methods**, grouped by type, along with **Python code examples** for each. This covers:\n",
        "\n",
        "1. **Basic Feature Engineering**\n",
        "2. **Encoding Techniques**\n",
        "3. **Handling Missing Values**\n",
        "4. **Scaling and Normalization**\n",
        "5. **Interaction and Polynomial Features**\n",
        "6. **Datetime Features**\n",
        "7. **Text Features (NLP)**\n",
        "8. **Binning/Discretization**\n",
        "9. **Feature Selection**\n",
        "10. **Dimensionality Reduction**\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 1. **Basic Feature Engineering**\n",
        "\n",
        "### âœ… Creating New Features\n",
        "\n",
        "```python\n",
        "df['BMI'] = df['Weight_kg'] / (df['Height_m'] ** 2)\n",
        "df['Speed'] = df['Distance'] / df['Time']\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 2. **Encoding Categorical Features**\n",
        "\n",
        "### âœ… One-Hot Encoding (Nominal)\n",
        "\n",
        "```python\n",
        "pd.get_dummies(df['Gender'], prefix='Gender')\n",
        "```\n",
        "\n",
        "### âœ… Label Encoding (Ordinal)\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['Education_Encoded'] = le.fit_transform(df['Education'])\n",
        "```\n",
        "\n",
        "### âœ… Target/Mean Encoding\n",
        "\n",
        "```python\n",
        "mean_target = df.groupby('City')['Price'].mean()\n",
        "df['City_encoded'] = df['City'].map(mean_target)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 3. **Handling Missing Values**\n",
        "\n",
        "### âœ… Fill with Mean/Median/Mode\n",
        "\n",
        "```python\n",
        "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
        "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
        "```\n",
        "\n",
        "### âœ… Fill with Predictive Model\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "train = df[df['Age'].notnull()]\n",
        "test = df[df['Age'].isnull()]\n",
        "model = RandomForestRegressor().fit(train[['Feature1']], train['Age'])\n",
        "df.loc[df['Age'].isnull(), 'Age'] = model.predict(test[['Feature1']])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 4. **Scaling and Normalization**\n",
        "\n",
        "### âœ… Standard Scaling (Z-score)\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "df[['Age', 'Income']] = scaler.fit_transform(df[['Age', 'Income']])\n",
        "```\n",
        "\n",
        "### âœ… Min-Max Scaling\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df[['Income']] = scaler.fit_transform(df[['Income']])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 5. **Interaction and Polynomial Features**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
        "X_poly = poly.fit_transform(df[['Feature1', 'Feature2']])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 6. **Datetime Feature Engineering**\n",
        "\n",
        "```python\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df['Year'] = df['Date'].dt.year\n",
        "df['Month'] = df['Date'].dt.month\n",
        "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 7. **Text Feature Engineering (NLP)**\n",
        "\n",
        "### âœ… Basic Features\n",
        "\n",
        "```python\n",
        "df['Text_Length'] = df['Review'].str.len()\n",
        "df['Word_Count'] = df['Review'].str.split().apply(len)\n",
        "```\n",
        "\n",
        "### âœ… TF-IDF Vectorization\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=100)\n",
        "X_tfidf = tfidf.fit_transform(df['Review'])\n",
        "```\n",
        "\n",
        "### âœ… Word Embeddings (e.g., Word2Vec, BERT)\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(df['Tokenized_Text'], vector_size=100, window=5, min_count=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 8. **Binning / Discretization**\n",
        "\n",
        "### âœ… Quantile Binning\n",
        "\n",
        "```python\n",
        "df['Income_bin'] = pd.qcut(df['Income'], q=4, labels=False)\n",
        "```\n",
        "\n",
        "### âœ… Custom Bins\n",
        "\n",
        "```python\n",
        "bins = [0, 18, 35, 60, 100]\n",
        "labels = ['Teen', 'Young Adult', 'Adult', 'Senior']\n",
        "df['AgeGroup'] = pd.cut(df['Age'], bins=bins, labels=labels)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 9. **Feature Selection**\n",
        "\n",
        "### âœ… Correlation Matrix\n",
        "\n",
        "```python\n",
        "import seaborn as sns\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, annot=True)\n",
        "```\n",
        "\n",
        "### âœ… Variance Threshold\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "X_reduced = selector.fit_transform(X)\n",
        "```\n",
        "\n",
        "### âœ… Recursive Feature Elimination (RFE)\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "rfe = RFE(model, n_features_to_select=5)\n",
        "X_rfe = rfe.fit_transform(X, y)\n",
        "```\n",
        "\n",
        "### âœ… Mutual Information\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "mi = mutual_info_classif(X, y)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 10. **Dimensionality Reduction**\n",
        "\n",
        "### âœ… PCA (Principal Component Analysis)\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "```\n",
        "\n",
        "### âœ… UMAP / t-SNE (for visualization)\n",
        "\n",
        "```python\n",
        "from umap import UMAP\n",
        "umap = UMAP(n_components=2)\n",
        "X_umap = umap.fit_transform(X)\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "tsne = TSNE(n_components=2)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Final Tip:\n",
        "\n",
        "* Always split data before applying **target-aware** feature engineering (like target encoding).\n",
        "* Use **pipelines** (with `scikit-learn`) to automate and cross-validate all preprocessing steps safely.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KV_-f-ljUnb6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Mf-1PwCUkHo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}