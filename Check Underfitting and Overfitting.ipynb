{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c649eea-b6bd-4f05-a7cb-86746769c91d",
   "metadata": {},
   "source": [
    "To check whether the **Logistic Regression model** is **overfitting or underfitting**, we can compare its **performance on training and test data** using key metrics like:\r\n",
    "\r\n",
    "1. **Accuracy Score** â€“ If training accuracy is much higher than test accuracy, the model is overfitting.\r\n",
    "2. **Precision, Recall, F1-score** â€“ Helps check performance on different aspects.\r\n",
    "3. **ROC-AUC Score** â€“ Measures overall model discrimination ability.\r\n",
    "4. **Learning Curves** â€“ A graphical way to check overfitting.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **ðŸ”¹ Python Code to Detect Overfitting/Underfitting**\r\n",
    "```python\r\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import learning_curve\r\n",
    "\r\n",
    "# **1. Evaluate Training and Testing Accuracy**\r\n",
    "train_acc = accuracy_score(y_train, model.predict(X_train))\r\n",
    "test_acc = accuracy_score(y_test, model.predict(X_test))\r\n",
    "\r\n",
    "print(f\"Training Accuracy: {train_acc:.2f}\")\r\n",
    "print(f\"Testing Accuracy: {test_acc:.2f}\")\r\n",
    "\r\n",
    "# **2. Classification Report (Precision, Recall, F1-score)**\r\n",
    "print(\"Classification Report (Test Set):\")\r\n",
    "print(classification_report(y_test, y_pred))\r\n",
    "\r\n",
    "# **3. ROC-AUC Score**\r\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\r\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\r\n",
    "\r\n",
    "# **4. Learning Curve (To visualize Overfitting/Underfitting)**\r\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_selected, y, cv=5, scoring=\"accuracy\")\r\n",
    "\r\n",
    "# Calculate mean and std deviation of training and test scores\r\n",
    "train_mean = np.mean(train_scores, axis=1)\r\n",
    "train_std = np.std(train_scores, axis=1)\r\n",
    "test_mean = np.mean(test_scores, axis=1)\r\n",
    "test_std = np.std(test_scores, axis=1)\r\n",
    "\r\n",
    "# Plot Learning Curve\r\n",
    "plt.figure(figsize=(8, 5))\r\n",
    "plt.plot(train_sizes, train_mean, 'o-', label=\"Training Score\", color=\"blue\")\r\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\r\n",
    "plt.plot(train_sizes, test_mean, 'o-', label=\"Cross-validation Score\", color=\"red\")\r\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"red\")\r\n",
    "plt.xlabel(\"Training Examples\")\r\n",
    "plt.ylabel(\"Score\")\r\n",
    "plt.title(\"Learning Curve\")\r\n",
    "plt.legend()\r\n",
    "plt.show()\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **ðŸ”¹ How to Interpret the Results?**\r\n",
    "âœ… **If Training Accuracy >> Test Accuracy (Low Test Accuracy)** â†’ **Overfitting**  \r\n",
    "âœ… **If Training and Test Accuracy are both Low** â†’ **Underfitting**  \r\n",
    "âœ… **If Training and Test Accuracy are close & high (~80%+)** â†’ **Good Fit**  \r\n",
    "\r\n",
    "- **Learning Curve:**\r\n",
    "  - If **training score is high, but validation score is low** â†’ **Overfitting**.\r\n",
    "  - If **both training and validation scores are low** â†’ **Underfitting**.\r\n",
    "  - If **both curves converge at a high value** â†’ **Good Fit**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **ðŸ”¹ How to Fix Overfitting or Underfitting?**\r\n",
    "**ðŸ”¸ If Overfitting:**  \r\n",
    "- **Reduce model complexity** (e.g., Regularization, Feature Selection)  \r\n",
    "- **Get more training data**  \r\n",
    "- **Use dropout (for deep learning models)**  \r\n",
    "\r\n",
    "**ðŸ”¸ If Underfitting:**  \r\n",
    "- **Use a more complex model**  \r\n",
    "- **Feature Engineering** (e.g., polynomial features)  \r\n",
    "- **Increase training time or reduce regularization**  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **âœ… Final Thoughts**\r\n",
    "This method helps determine if your **Heart Disease Prediction Model** is overfitting or underfitting. Let me know if you need improvements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a89c62-96bd-41c8-bf5e-a7ac2ae325c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# **1. Evaluate Training and Testing Accuracy**\n",
    "train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# **2. Classification Report (Precision, Recall, F1-score)**\n",
    "print(\"Classification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# **3. ROC-AUC Score**\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "print(f\"ROC-AUC Score: {roc_auc:.2f}\")\n",
    "\n",
    "# **4. Learning Curve (To visualize Overfitting/Underfitting)**\n",
    "train_sizes, train_scores, test_scores = learning_curve(model, X_selected, y, cv=5, scoring=\"accuracy\")\n",
    "\n",
    "# Calculate mean and std deviation of training and test scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot Learning Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label=\"Training Score\", color=\"blue\")\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"blue\")\n",
    "plt.plot(train_sizes, test_mean, 'o-', label=\"Cross-validation Score\", color=\"red\")\n",
    "plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"red\")\n",
    "plt.xlabel(\"Training Examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
