{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Here's a complete breakdown of **LangChain** ‚Äî one of the most powerful frameworks for building applications with LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "# üî∑ **LangChain: Everything You Need to Know**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Definition**\n",
        "\n",
        "> **LangChain** is an open-source **framework** designed to help developers build powerful **applications using Large Language Models (LLMs)** by connecting them with **external data sources**, **tools**, **memory**, and **multi-step workflows**.\n",
        "\n",
        "---\n",
        "\n",
        "## üèõÔ∏è **Architecture Overview**\n",
        "\n",
        "LangChain provides a modular, composable architecture for LLM applications. Here's the **typical stack**:\n",
        "\n",
        "```\n",
        "                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                     ‚îÇ     LangChain Application    ‚îÇ\n",
        "                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                  ‚îÇ\n",
        "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ                         ‚îÇ                          ‚îÇ\n",
        "    Chains & Agents         Memory Modules            Tool Integration\n",
        "        ‚îÇ                         ‚îÇ                          ‚îÇ\n",
        "        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ> Prompts       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "        ‚îÇ                    ‚îÇ Short-Term ‚îÇ       ‚îÇ APIs, Python, Google ‚îÇ\n",
        "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ> LLMs          ‚îÇ  Memory    ‚îÇ       ‚îÇ Search, DBs, Tools   ‚îÇ\n",
        "                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                  ‚îÇ                          ‚îÇ\n",
        "                             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "                             ‚îÇ Vector DB ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ Documents ‚îÇ\n",
        "                             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß± **Key Components**\n",
        "\n",
        "| Component            | Description                                                          |\n",
        "| -------------------- | -------------------------------------------------------------------- |\n",
        "| **Prompt Templates** | Predefined prompt formats with dynamic variables                     |\n",
        "| **LLM Wrappers**     | Abstract access to OpenAI, Hugging Face, Cohere, etc.                |\n",
        "| **Chains**           | Sequential or branching workflows combining LLM calls and tools      |\n",
        "| **Agents**           | Dynamic decision-makers that choose which tools to use at runtime    |\n",
        "| **Memory**           | Short-term or long-term memory for stateful interactions             |\n",
        "| **Tools**            | Plugins like Web Search, Calculators, Python, Zapier, APIs           |\n",
        "| **Retrievers**       | Interfaces to search knowledge from documents (RAG)                  |\n",
        "| **Document Loaders** | Extract content from PDFs, web, databases, etc.                      |\n",
        "| **Vector Stores**    | Store embeddings for semantic search (e.g., FAISS, Pinecone, Chroma) |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è **How LangChain Works (Step-by-Step)**\n",
        "\n",
        "### üß† Example: Question Answering with PDFs\n",
        "\n",
        "1. **Input**: User asks a question\n",
        "2. **Document Loading**: PDFs loaded using `PyMuPDF`, `pdfplumber`, etc.\n",
        "3. **Embedding**: Sentences converted to vectors using OpenAI/HF embeddings\n",
        "4. **Vector Store**: Embeddings stored in FAISS or Pinecone\n",
        "5. **Retriever**: LangChain retrieves most relevant chunks\n",
        "6. **Prompting**: Retrieved chunks inserted into a prompt template\n",
        "7. **LLM Call**: OpenAI (or other LLM) answers the question\n",
        "8. **Output**: Answer returned to user\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **Use Cases of LangChain**\n",
        "\n",
        "| Use Case                                 | Description                                                           |\n",
        "| ---------------------------------------- | --------------------------------------------------------------------- |\n",
        "| **Retrieval-Augmented Generation (RAG)** | Combines LLMs with external document knowledge                        |\n",
        "| **Chatbots / Virtual Assistants**        | Stateful conversation with tools and memory                           |\n",
        "| **Document QA**                          | Ask questions over PDFs, websites, internal docs                      |\n",
        "| **Agentic Workflows**                    | LLM decides which tools to call and in what order (LangGraph, Agents) |\n",
        "| **Summarization Pipelines**              | Summarize emails, reports, meeting notes                              |\n",
        "| **Code Explanation Tools**               | Explain code with language models + syntax highlighters               |\n",
        "| **Data QA Bots**                         | Combine SQL agents with natural language                              |\n",
        "| **Auto Report Generation**               | LLM generates summaries/reports from structured + unstructured data   |\n",
        "\n",
        "---\n",
        "\n",
        "## üí° **Code Example (Simple QA over PDF)**\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Step 1: Load PDF\n",
        "loader = PyMuPDFLoader(\"invoice.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Step 2: Convert to vectors\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "# Step 3: Create QA Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(),\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "# Step 4: Ask question\n",
        "response = qa_chain.run(\"What is the invoice number?\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Integration Ecosystem\n",
        "\n",
        "LangChain works seamlessly with:\n",
        "\n",
        "* **Vector DBs**: FAISS, Pinecone, Chroma, Weaviate\n",
        "* **LLMs**: OpenAI, Cohere, HuggingFace, Claude\n",
        "* **Embeddings**: OpenAI, HuggingFace, SentenceTransformers\n",
        "* **Memory**: Redis, ConversationBufferMemory, ConversationSummaryMemory\n",
        "* **Agents**: Tool usage, multi-step reasoning with LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Pro Tip:\n",
        "\n",
        "LangChain = \"Glue\" framework üß©\n",
        "It **doesn't train LLMs**, but it **lets you orchestrate, control, and deploy** smart applications on top of them.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B6Xa8rP9Jt7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question ‚Äî understanding the connection between **LangGraph** and **LangChain (Chains)** will help you build more **advanced and modular LLM apps**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÑ **LangGraph vs LangChain: How They‚Äôre Connected**\n",
        "\n",
        "### ‚úÖ Summary:\n",
        "\n",
        "> **LangGraph is a *new extension built on top of LangChain*** that allows you to create **stateful, dynamic, multi-step LLM workflows** using a **graph-based architecture** instead of just sequential chains.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. **LangChain Chains** ‚Äì Traditional (Sequential) Workflow\n",
        "\n",
        "### üß± What is a Chain?\n",
        "\n",
        "A **Chain** is a **fixed sequence** of steps (like: input ‚Üí prompt ‚Üí LLM ‚Üí output).\n",
        "\n",
        "### üîÅ Example:\n",
        "\n",
        "```plaintext\n",
        "Input ‚Üí Prompt Template ‚Üí LLM ‚Üí Output\n",
        "```\n",
        "\n",
        "### ‚úÖ Good for:\n",
        "\n",
        "* Simple tasks (Q\\&A, summarization, translation)\n",
        "* RAG pipelines\n",
        "* Static workflows\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. **LangGraph** ‚Äì Graph-Based Dynamic Workflow\n",
        "\n",
        "### üîó What is LangGraph?\n",
        "\n",
        "LangGraph is a **multi-node, stateful framework** built **on top of LangChain** that lets you define:\n",
        "\n",
        "* Multiple **nodes** (steps/agents/tools)\n",
        "* **Edges** that control flow (if/else, looping, dynamic branching)\n",
        "* **Shared state** (memory, context)\n",
        "\n",
        "### üîÅ Example:\n",
        "\n",
        "```plaintext\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "   Input ‚Üí‚îÇ Node A (LLM)‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ (based on output)\n",
        "       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "       ‚îÇ Node B (Tool) ‚îÇ ‚Üê‚Äî Loop if needed\n",
        "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "               ‚îÇ\n",
        "          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "          ‚îÇ Final Out‚îÇ\n",
        "          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîó How LangGraph and Chains Are Connected\n",
        "\n",
        "| Feature                | LangChain (Chains)       | LangGraph (Built on LangChain)             |\n",
        "| ---------------------- | ------------------------ | ------------------------------------------ |\n",
        "| Flow Type              | Linear / sequential      | Graph-based / dynamic / stateful           |\n",
        "| State Tracking         | Minimal / memory objects | Full shared state (via graph input/output) |\n",
        "| Reusability            | Limited                  | High (reusable nodes, dynamic routing)     |\n",
        "| Complexity Handling    | Low                      | High (loops, retries, branching)           |\n",
        "| Tool/Agent Integration | Supported                | Strongly supported                         |\n",
        "| Underlying Engine      | LangChain                | LangChain + NetworkX (graph engine)        |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Real-Life Analogy:\n",
        "\n",
        "### LangChain (Chain)\n",
        "\n",
        "üß± Like an **assembly line** ‚Äî step-by-step process, same for every input.\n",
        "\n",
        "### LangGraph\n",
        "\n",
        "üï∏Ô∏è Like a **brain or flowchart** ‚Äî makes decisions, loops, handles multiple tools, and uses memory.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Code Snippet (LangGraph with LangChain Nodes)\n",
        "\n",
        "```python\n",
        "import langgraph\n",
        "\n",
        "# Define LangChain-powered node\n",
        "def qa_node(state):\n",
        "    question = state[\"question\"]\n",
        "    answer = qa_chain.run(question)\n",
        "    return {\"answer\": answer}\n",
        "\n",
        "# Define graph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "graph = StateGraph()\n",
        "\n",
        "graph.add_node(\"qa_node\", qa_node)\n",
        "graph.set_entry_point(\"qa_node\")\n",
        "graph.set_finish_point(\"qa_node\")\n",
        "\n",
        "# Compile and run\n",
        "app = graph.compile()\n",
        "result = app.invoke({\"question\": \"Who is the CEO of OpenAI?\"})\n",
        "print(result[\"answer\"])\n",
        "```\n",
        "\n",
        "Here, `qa_chain` is a **LangChain chain** used inside a **LangGraph node**. This shows how **LangGraph uses LangChain inside it**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Final Takeaway:\n",
        "\n",
        "| If you're building...                | Use this               |\n",
        "| ------------------------------------ | ---------------------- |\n",
        "| Simple pipelines (QA, summarization) | **LangChain (Chains)** |\n",
        "| Multi-step agents with logic/memory  | **LangGraph**          |\n",
        "| Stateful, branching flows            | **LangGraph**          |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vy78tcjEJ6aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs a complete, beginner-friendly implementation of **RAG (Retrieval-Augmented Generation)** using **LangChain**, **OpenAI**, and **FAISS** ‚Äî step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "# üî∑ Retrieval-Augmented Generation (RAG) with LangChain\n",
        "\n",
        "## ‚úÖ Objective:\n",
        "\n",
        "Use an LLM to answer questions based on **your own documents** (e.g., PDFs, text files) instead of just its training data.\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ Libraries Required:\n",
        "\n",
        "```bash\n",
        "pip install langchain openai faiss-cpu tiktoken PyMuPDF\n",
        "```\n",
        "\n",
        "> You‚Äôll also need your **OpenAI API key**.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Project Structure:\n",
        "\n",
        "```\n",
        "/rag_project\n",
        "  ‚îú‚îÄ‚îÄ document.pdf\n",
        "  ‚îî‚îÄ‚îÄ rag_script.py\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Step-by-Step Code (`rag_script.py`):\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "\n",
        "# 1. Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "\n",
        "# 2. Load the document\n",
        "loader = PyMuPDFLoader(\"document.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# 3. Split text into manageable chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 4. Convert chunks into embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# 5. Create the RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    chain_type=\"stuff\",  # or map_reduce, refine\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "# 6. Ask a question\n",
        "question = \"What is the main topic discussed in the document?\"\n",
        "response = qa_chain.run(question)\n",
        "\n",
        "# 7. Output the answer\n",
        "print(\"üìò Answer:\", response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ What Each Step Does:\n",
        "\n",
        "| Step | Description                                        |\n",
        "| ---- | -------------------------------------------------- |\n",
        "| 1    | Sets your OpenAI key for accessing the API         |\n",
        "| 2    | Loads the PDF and extracts text                    |\n",
        "| 3    | Splits long texts into smaller chunks              |\n",
        "| 4    | Embeds each chunk into a vector (semantic meaning) |\n",
        "| 5    | Builds a retrieval-based QA chain                  |\n",
        "| 6    | Sends your question, retrieves relevant docs       |\n",
        "| 7    | Prints the answer generated by the LLM             |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Sample Output:\n",
        "\n",
        "```\n",
        "üìò Answer: The document discusses invoice generation and payment terms...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Use Cases:\n",
        "\n",
        "* Chat over PDFs / manuals / research papers\n",
        "* Internal knowledge base Q\\&A\n",
        "* Legal & policy document assistants\n",
        "* Customer support on product guides\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Want to Extend This?\n",
        "\n",
        "* Use **Chroma** instead of FAISS\n",
        "* Swap `OpenAI()` with `ChatOpenAI(model=\"gpt-4\")`\n",
        "* Add **LangGraph** for multi-step reasoning\n",
        "* Use **Guardrails AI** for safe responses\n",
        "* Build a UI using **Streamlit or FastAPI**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LztO0XrlKIIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5kSRl7_KJln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}