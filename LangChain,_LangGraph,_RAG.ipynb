{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sure! Here's a complete breakdown of **LangChain** — one of the most powerful frameworks for building applications with LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "# 🔷 **LangChain: Everything You Need to Know**\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Definition**\n",
        "\n",
        "> **LangChain** is an open-source **framework** designed to help developers build powerful **applications using Large Language Models (LLMs)** by connecting them with **external data sources**, **tools**, **memory**, and **multi-step workflows**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🏛️ **Architecture Overview**\n",
        "\n",
        "LangChain provides a modular, composable architecture for LLM applications. Here's the **typical stack**:\n",
        "\n",
        "```\n",
        "                     ┌─────────────────────────────┐\n",
        "                     │     LangChain Application    │\n",
        "                     └────────────┬────────────────┘\n",
        "                                  │\n",
        "        ┌─────────────────────────┼──────────────────────────┐\n",
        "        │                         │                          │\n",
        "    Chains & Agents         Memory Modules            Tool Integration\n",
        "        │                         │                          │\n",
        "        ├────> Prompts       ┌────▼────┐         ┌──────────▼───────────┐\n",
        "        │                    │ Short-Term │       │ APIs, Python, Google │\n",
        "        └────> LLMs          │  Memory    │       │ Search, DBs, Tools   │\n",
        "                             └────┬───────┘       └──────────┬───────────┘\n",
        "                                  │                          │\n",
        "                             ┌────▼──────┐            ┌──────▼────┐\n",
        "                             │ Vector DB │◄───────────┤ Documents │\n",
        "                             └───────────┘            └──────────┘\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🧱 **Key Components**\n",
        "\n",
        "| Component            | Description                                                          |\n",
        "| -------------------- | -------------------------------------------------------------------- |\n",
        "| **Prompt Templates** | Predefined prompt formats with dynamic variables                     |\n",
        "| **LLM Wrappers**     | Abstract access to OpenAI, Hugging Face, Cohere, etc.                |\n",
        "| **Chains**           | Sequential or branching workflows combining LLM calls and tools      |\n",
        "| **Agents**           | Dynamic decision-makers that choose which tools to use at runtime    |\n",
        "| **Memory**           | Short-term or long-term memory for stateful interactions             |\n",
        "| **Tools**            | Plugins like Web Search, Calculators, Python, Zapier, APIs           |\n",
        "| **Retrievers**       | Interfaces to search knowledge from documents (RAG)                  |\n",
        "| **Document Loaders** | Extract content from PDFs, web, databases, etc.                      |\n",
        "| **Vector Stores**    | Store embeddings for semantic search (e.g., FAISS, Pinecone, Chroma) |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚙️ **How LangChain Works (Step-by-Step)**\n",
        "\n",
        "### 🧠 Example: Question Answering with PDFs\n",
        "\n",
        "1. **Input**: User asks a question\n",
        "2. **Document Loading**: PDFs loaded using `PyMuPDF`, `pdfplumber`, etc.\n",
        "3. **Embedding**: Sentences converted to vectors using OpenAI/HF embeddings\n",
        "4. **Vector Store**: Embeddings stored in FAISS or Pinecone\n",
        "5. **Retriever**: LangChain retrieves most relevant chunks\n",
        "6. **Prompting**: Retrieved chunks inserted into a prompt template\n",
        "7. **LLM Call**: OpenAI (or other LLM) answers the question\n",
        "8. **Output**: Answer returned to user\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 **Use Cases of LangChain**\n",
        "\n",
        "| Use Case                                 | Description                                                           |\n",
        "| ---------------------------------------- | --------------------------------------------------------------------- |\n",
        "| **Retrieval-Augmented Generation (RAG)** | Combines LLMs with external document knowledge                        |\n",
        "| **Chatbots / Virtual Assistants**        | Stateful conversation with tools and memory                           |\n",
        "| **Document QA**                          | Ask questions over PDFs, websites, internal docs                      |\n",
        "| **Agentic Workflows**                    | LLM decides which tools to call and in what order (LangGraph, Agents) |\n",
        "| **Summarization Pipelines**              | Summarize emails, reports, meeting notes                              |\n",
        "| **Code Explanation Tools**               | Explain code with language models + syntax highlighters               |\n",
        "| **Data QA Bots**                         | Combine SQL agents with natural language                              |\n",
        "| **Auto Report Generation**               | LLM generates summaries/reports from structured + unstructured data   |\n",
        "\n",
        "---\n",
        "\n",
        "## 💡 **Code Example (Simple QA over PDF)**\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Step 1: Load PDF\n",
        "loader = PyMuPDFLoader(\"invoice.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "# Step 2: Convert to vectors\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embedding_model)\n",
        "\n",
        "# Step 3: Create QA Chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(),\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "# Step 4: Ask question\n",
        "response = qa_chain.run(\"What is the invoice number?\")\n",
        "print(response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔗 Integration Ecosystem\n",
        "\n",
        "LangChain works seamlessly with:\n",
        "\n",
        "* **Vector DBs**: FAISS, Pinecone, Chroma, Weaviate\n",
        "* **LLMs**: OpenAI, Cohere, HuggingFace, Claude\n",
        "* **Embeddings**: OpenAI, HuggingFace, SentenceTransformers\n",
        "* **Memory**: Redis, ConversationBufferMemory, ConversationSummaryMemory\n",
        "* **Agents**: Tool usage, multi-step reasoning with LangGraph\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Pro Tip:\n",
        "\n",
        "LangChain = \"Glue\" framework 🧩\n",
        "It **doesn't train LLMs**, but it **lets you orchestrate, control, and deploy** smart applications on top of them.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B6Xa8rP9Jt7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great question — understanding the connection between **LangGraph** and **LangChain (Chains)** will help you build more **advanced and modular LLM apps**.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔄 **LangGraph vs LangChain: How They’re Connected**\n",
        "\n",
        "### ✅ Summary:\n",
        "\n",
        "> **LangGraph is a *new extension built on top of LangChain*** that allows you to create **stateful, dynamic, multi-step LLM workflows** using a **graph-based architecture** instead of just sequential chains.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 1. **LangChain Chains** – Traditional (Sequential) Workflow\n",
        "\n",
        "### 🧱 What is a Chain?\n",
        "\n",
        "A **Chain** is a **fixed sequence** of steps (like: input → prompt → LLM → output).\n",
        "\n",
        "### 🔁 Example:\n",
        "\n",
        "```plaintext\n",
        "Input → Prompt Template → LLM → Output\n",
        "```\n",
        "\n",
        "### ✅ Good for:\n",
        "\n",
        "* Simple tasks (Q\\&A, summarization, translation)\n",
        "* RAG pipelines\n",
        "* Static workflows\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 2. **LangGraph** – Graph-Based Dynamic Workflow\n",
        "\n",
        "### 🔗 What is LangGraph?\n",
        "\n",
        "LangGraph is a **multi-node, stateful framework** built **on top of LangChain** that lets you define:\n",
        "\n",
        "* Multiple **nodes** (steps/agents/tools)\n",
        "* **Edges** that control flow (if/else, looping, dynamic branching)\n",
        "* **Shared state** (memory, context)\n",
        "\n",
        "### 🔁 Example:\n",
        "\n",
        "```plaintext\n",
        "          ┌────────────┐\n",
        "   Input →│ Node A (LLM)│\n",
        "          └────┬───────┘\n",
        "               │ (based on output)\n",
        "       ┌───────▼───────┐\n",
        "       │ Node B (Tool) │ ←— Loop if needed\n",
        "       └───────┬───────┘\n",
        "               │\n",
        "          ┌────▼─────┐\n",
        "          │ Final Out│\n",
        "          └──────────┘\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔗 How LangGraph and Chains Are Connected\n",
        "\n",
        "| Feature                | LangChain (Chains)       | LangGraph (Built on LangChain)             |\n",
        "| ---------------------- | ------------------------ | ------------------------------------------ |\n",
        "| Flow Type              | Linear / sequential      | Graph-based / dynamic / stateful           |\n",
        "| State Tracking         | Minimal / memory objects | Full shared state (via graph input/output) |\n",
        "| Reusability            | Limited                  | High (reusable nodes, dynamic routing)     |\n",
        "| Complexity Handling    | Low                      | High (loops, retries, branching)           |\n",
        "| Tool/Agent Integration | Supported                | Strongly supported                         |\n",
        "| Underlying Engine      | LangChain                | LangChain + NetworkX (graph engine)        |\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Real-Life Analogy:\n",
        "\n",
        "### LangChain (Chain)\n",
        "\n",
        "🧱 Like an **assembly line** — step-by-step process, same for every input.\n",
        "\n",
        "### LangGraph\n",
        "\n",
        "🕸️ Like a **brain or flowchart** — makes decisions, loops, handles multiple tools, and uses memory.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧪 Code Snippet (LangGraph with LangChain Nodes)\n",
        "\n",
        "```python\n",
        "import langgraph\n",
        "\n",
        "# Define LangChain-powered node\n",
        "def qa_node(state):\n",
        "    question = state[\"question\"]\n",
        "    answer = qa_chain.run(question)\n",
        "    return {\"answer\": answer}\n",
        "\n",
        "# Define graph\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "graph = StateGraph()\n",
        "\n",
        "graph.add_node(\"qa_node\", qa_node)\n",
        "graph.set_entry_point(\"qa_node\")\n",
        "graph.set_finish_point(\"qa_node\")\n",
        "\n",
        "# Compile and run\n",
        "app = graph.compile()\n",
        "result = app.invoke({\"question\": \"Who is the CEO of OpenAI?\"})\n",
        "print(result[\"answer\"])\n",
        "```\n",
        "\n",
        "Here, `qa_chain` is a **LangChain chain** used inside a **LangGraph node**. This shows how **LangGraph uses LangChain inside it**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Final Takeaway:\n",
        "\n",
        "| If you're building...                | Use this               |\n",
        "| ------------------------------------ | ---------------------- |\n",
        "| Simple pipelines (QA, summarization) | **LangChain (Chains)** |\n",
        "| Multi-step agents with logic/memory  | **LangGraph**          |\n",
        "| Stateful, branching flows            | **LangGraph**          |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vy78tcjEJ6aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a complete, beginner-friendly implementation of **RAG (Retrieval-Augmented Generation)** using **LangChain**, **OpenAI**, and **FAISS** — step-by-step.\n",
        "\n",
        "---\n",
        "\n",
        "# 🔷 Retrieval-Augmented Generation (RAG) with LangChain\n",
        "\n",
        "## ✅ Objective:\n",
        "\n",
        "Use an LLM to answer questions based on **your own documents** (e.g., PDFs, text files) instead of just its training data.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 Libraries Required:\n",
        "\n",
        "```bash\n",
        "pip install langchain openai faiss-cpu tiktoken PyMuPDF\n",
        "```\n",
        "\n",
        "> You’ll also need your **OpenAI API key**.\n",
        "\n",
        "---\n",
        "\n",
        "## 📦 Project Structure:\n",
        "\n",
        "```\n",
        "/rag_project\n",
        "  ├── document.pdf\n",
        "  └── rag_script.py\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Step-by-Step Code (`rag_script.py`):\n",
        "\n",
        "```python\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "import os\n",
        "\n",
        "# 1. Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-openai-api-key\"\n",
        "\n",
        "# 2. Load the document\n",
        "loader = PyMuPDFLoader(\"document.pdf\")\n",
        "documents = loader.load()\n",
        "\n",
        "# 3. Split text into manageable chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "\n",
        "# 4. Convert chunks into embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# 5. Create the RAG chain\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    chain_type=\"stuff\",  # or map_reduce, refine\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")\n",
        "\n",
        "# 6. Ask a question\n",
        "question = \"What is the main topic discussed in the document?\"\n",
        "response = qa_chain.run(question)\n",
        "\n",
        "# 7. Output the answer\n",
        "print(\"📘 Answer:\", response)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 What Each Step Does:\n",
        "\n",
        "| Step | Description                                        |\n",
        "| ---- | -------------------------------------------------- |\n",
        "| 1    | Sets your OpenAI key for accessing the API         |\n",
        "| 2    | Loads the PDF and extracts text                    |\n",
        "| 3    | Splits long texts into smaller chunks              |\n",
        "| 4    | Embeds each chunk into a vector (semantic meaning) |\n",
        "| 5    | Builds a retrieval-based QA chain                  |\n",
        "| 6    | Sends your question, retrieves relevant docs       |\n",
        "| 7    | Prints the answer generated by the LLM             |\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Sample Output:\n",
        "\n",
        "```\n",
        "📘 Answer: The document discusses invoice generation and payment terms...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Use Cases:\n",
        "\n",
        "* Chat over PDFs / manuals / research papers\n",
        "* Internal knowledge base Q\\&A\n",
        "* Legal & policy document assistants\n",
        "* Customer support on product guides\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Want to Extend This?\n",
        "\n",
        "* Use **Chroma** instead of FAISS\n",
        "* Swap `OpenAI()` with `ChatOpenAI(model=\"gpt-4\")`\n",
        "* Add **LangGraph** for multi-step reasoning\n",
        "* Use **Guardrails AI** for safe responses\n",
        "* Build a UI using **Streamlit or FastAPI**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LztO0XrlKIIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o5kSRl7_KJln"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}