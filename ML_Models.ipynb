{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Supervised**"
      ],
      "metadata": {
        "id": "0zWHZeqb3vpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# load Data\n",
        "df = pd.read_csv(\"Dataset.csv\")\n",
        "df.head()\n",
        "\n",
        "# EDA\n",
        "df.shape\n",
        "df.info()\n",
        "df.describe()\n",
        "df.isnull().sum()         # Total missing per column\n",
        "df.isnull().mean()*100    # Percentage missing\n",
        "df.dropna(inplace=True)  # Drop rows with any missing values\n",
        "df['numerics_columns'].fillna(df['numerics_columns'].mean(), inplace=True)        # Numeric: mean\n",
        "df['categorical_columns'].fillna(df['categorical_columns'].mode()[0], inplace=True)  # Categorical: mode\n",
        "df.duplicated().sum()\n",
        "df.drop_duplicates(inplace=True)\n",
        "df.corr()\n",
        "\n",
        "# Outlier detection\n",
        "for i in df.columns:\n",
        "    if df[i].dtype != 'O':\n",
        "        plt.boxplot(df[i])\n",
        "        plt.title(i)\n",
        "        plt.show()\n",
        "out_col = ['age','campaign','cons.conf.idx'] # columns with outliers\n",
        "\n",
        "for col in out_col:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    LL = Q1 - IQR * 1.5\n",
        "    UL = Q3 + IQR * 1.5\n",
        "    df = df[(df[col] <= UL) & (df[col] >=LL)]\n",
        "\n",
        "\n",
        "# Label Encoding if column is categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "for i in df.columns:\n",
        "    if df[i].dtype == 'object':\n",
        "        df[i] = le.fit_transform(df[i])\n",
        "        print(f'Column Name : {i}')\n",
        "        print(\"Original ---> Encoded\")\n",
        "        for original_class,encoded_value in zip(le.classes_,le.transform(le.classes_)):\n",
        "            print(f'{original_class}->{encoded_value}')\n",
        "        print()\n",
        "\n",
        "# data split\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']"
      ],
      "metadata": {
        "id": "PUUSWVvjurXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Linear Regression**"
      ],
      "metadata": {
        "id": "ACvJrvLDq-uk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XvMdW5Stq7xf"
      },
      "outputs": [],
      "source": [
        "# Standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "\n",
        "# best fit line\n",
        "sns.regplot(x=y_test, y=y_pred)\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Best fit line for linear regression model\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Logistic Regression**"
      ],
      "metadata": {
        "id": "Sq_uuEPOuzRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# not all columns are important\n",
        "# correlation\n",
        "correlation_matrix = df.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n",
        "\n",
        "# VIF and multicolinearilty\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.columns)]\n",
        "vif_data\n",
        "# drop the columns with high multicolinearity (vif >5) and check again\n",
        "X.drop(['col1'], axis=1, inplace=True)\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"Feature\"] = X.columns\n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.columns)]\n",
        "vif_data\n",
        "# until all columns with vif > 5 are removed\n",
        "\n",
        "# Standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "# best fit line\n",
        "sns.regplot(x=y_test, y=y_pred)\n",
        "plt.xlabel(\"Actual\")\n",
        "plt.ylabel(\"Predicted\")\n",
        "plt.title(\"Best fit line for logistic regression model\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HVOGHQ7lstHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Decision Trees**"
      ],
      "metadata": {
        "id": "8qZli5M5xTfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no standardisation\n",
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "# plot the model\n",
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(model, filled=True, feature_names=X.columns)\n",
        "plt.show()\n",
        "\n",
        "# hyper-parameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model = DecisionTreeClassifier()\n",
        "param_grid = {\"max_depth\": [None, 5, 10, 15],\n",
        "              \"min_samples_split\": [2, 5, 10],\n",
        "              \"min_samples_leaf\": [1, 2, 4],\n",
        "              \"criterion\":[\"gini\", \"entropy\"],\n",
        "              \"random_state\" : [0,42,60,100]}\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "final_model = DecisionTreeClassifier(**best_params)\n",
        "final_model.fit(x_train, y_train)\n",
        "y_pred = final_model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# plot tree\n",
        "plot_tree(final_model, fontsize = 3)"
      ],
      "metadata": {
        "id": "T08P2WauxdDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Random Forest**"
      ],
      "metadata": {
        "id": "o_51O_htzCSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no standardisation\n",
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "model = RandomForestClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "# plot model\n",
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(15, 10))\n",
        "plot_tree(model.estimators_[0], filled=True, feature_names=X.columns)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2YH_10X7zExt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **SVM : Support Vector Machine**"
      ],
      "metadata": {
        "id": "A79R256YznAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model\n",
        "from sklearn.svm import SVC, SVR\n",
        "model = SVC()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "px7zTU910I_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **KNN : K-Nearest Neighbours**"
      ],
      "metadata": {
        "id": "0Yxnm5NP0J2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "44vYtRFt0Ngc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LDA : Linear Discriminant Analysis**"
      ],
      "metadata": {
        "id": "W1pNks4A0OEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = pd.read_csv(\"dataset.scv\")\n",
        "\n",
        "# split features\n",
        "X = df.drop('target_column', axis=1)\n",
        "y = df['target_column']\n",
        "\n",
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\"\"\"\n",
        "Standardisation :\n",
        "If we standardize before splitting, we leak information about the entire dataset, potentially leading to data leakage.\n",
        "Therefore, we must standardize using only the training data so the test set remains unbiased.\n",
        "\"\"\"\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.transform(x_test)\n",
        "\n",
        "# n_components\n",
        "n_components = min(no. of features, no. of classes - 1)\n",
        "n_components\n",
        "\n",
        "# LDA model\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
        "x_train = lda.fit_transform(x_train, y_train)\n",
        "x_test = lda.transform(x_test)\n",
        "\n",
        "\"\"\"\n",
        "'LDA.explained_variance_ratio_' :\n",
        "How much of the class-discriminative information (variance between classes) is preserved in each Linear Discriminant (LD) component.\n",
        "no.of elements shown in the printed array : LD1,LD2,etc\n",
        "\"\"\"\n",
        "LDA.explained_variance_ratio_\n",
        "\n",
        "\"\"\"\n",
        "sum(lda.explained_variance_ratio_) :\n",
        "This tells the total class-discriminative variance explained by the selected components.\n",
        "\"\"\"\n",
        "sum(LDA.explained_variance_ratio_)\n",
        "\n",
        "# ML model\n",
        "model = RandomForestClassifier()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)"
      ],
      "metadata": {
        "id": "9ZcTsCpS0TTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Unsupervised**"
      ],
      "metadata": {
        "id": "wVba4Jp-3six"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **K-means Clustering**"
      ],
      "metadata": {
        "id": "cXLT6_yd4CbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# calculate K\n",
        "# elbow method :\n",
        "from sklearn.cluster import KMeans\n",
        "wcss = [] # WCSS (Within-Cluster Sum of Squares) measures how close points are to their cluster center\n",
        "for i in range(1,11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)\n",
        "    kmeans.fit(X_scaled)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(k_values,ssd) # Sharp \"elbow\" point where WCSS decrease slows\n",
        "plt.title('Elbow Method - Optimal number of clusters')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Silhouette method\n",
        "from sklearn.metrics import silhouette_score\n",
        "k_values = range(2,8)\n",
        "silhouette_scores=[]\n",
        "for k in k_values:\n",
        "  km = KMeans(n_clusters=k, max_iter=150, random_state=32)\n",
        "  km.fit(df_scaled)\n",
        "  silhouette_scores.append(silhouette_score(df_scaled,km.labels_))\n",
        "  print(f\"Silhouette score for k={k}: {silhouette_scores[-1]}\")\n",
        "\n",
        "plt.subplot(1,2,2) # Peak point where silhouette score is highest\n",
        "plt.plot(k_values,silhouette_scores,marker='o',color='green')\n",
        "plt.title('Silhouette Score')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# model\n",
        "kmeans = KMeans(n_clusters= k, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "y_kmeans = kmeans.predict(X_scaled)\n",
        "\n",
        "# evaluate\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, confusion_matrix, classification_report\n",
        "silhouette_score(X_scaled, y_kmeans)\n",
        "accuracy_score(y, y_kmeans)\n",
        "confusion_matrix(y, y_kmeans)\n",
        "print(classification_report(y, y_kmeans))\n",
        "print(accuracy_score(y, y_kmeans))\n",
        "print(confusion_matrix(y, y_kmeans))\n",
        "print(silhouette_score(X_scaled, y_kmeans))"
      ],
      "metadata": {
        "id": "ZffhwI9p3uoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Agglomerative Clustering**"
      ],
      "metadata": {
        "id": "ngS6gQS86zm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "# Plot dendrogram\n",
        "# linkage matrix using scipy\n",
        "linked = linkage(X, method='ward')\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "dendrogram(linked,\n",
        "           orientation='top',\n",
        "           distance_sort='descending',\n",
        "           show_leaf_counts=True)\n",
        "plt.title('Hierarchical Clustering Dendrogram (Ward Linkage)')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "li8EOoMF7CdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PCA : Principal Component Analysis**"
      ],
      "metadata": {
        "id": "aFL8FAh78Wuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# there is no need to do following steps if you are not plotting a visualisation, directly use pca = PCA(0.95)\n",
        "x_transposed = pd.DataFrame(x_scaled).T # transposing the matrix for covariance\n",
        "# Compute covariance matrix to identify correlated features and determine directions of maximum variance.\n",
        "cov_matrix = np.cov(x_transposed)\n",
        "pd.DataFrame(cov_matrix)\n",
        "\n",
        "'''\n",
        "Eigen Values and Eigen Vectors\n",
        "- Eigen Vectors : Directiosn in which data is pread the most\n",
        "- Eigen Values  : Magnitude of spread, i.e how much info or variance is captured in that direction\n",
        "\n",
        "in pca we selct the pc eith the highest variance\n",
        "'''\n",
        "eig_values, eig_vectors = np.linalg.eig(cov_matrix)\n",
        "\n",
        "# information gain\n",
        "total = sum(eig_values) # i.e total variance, information gain\n",
        "total\n",
        "\n",
        "# Variance explained by each feature\n",
        "variance_explained = [(i/total)*100 for i in sorted(eig_values, reverse = True)]\n",
        "variance_explained\n",
        "\n",
        "# Cumulative Explained Variance will show the total variance explained untill that feature and we generally take 95% or more so that .\n",
        "cumulative_variance_explained = np.cumsum(variance_explained)\n",
        "pd.DataFrame(cumulative_variance_explained)\n",
        "n_components_95 = np.where(cumulative_variance_explained >= 95)[0][0] + 1\n",
        "print(f\"n_components_95 : {n_components_95}\")\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize = (10,5))\n",
        "plt.bar(range(len(variance_explained)), variance_explained,label = 'Individual Explained Variance', color = 'g')\n",
        "plt.step(range(len(cumulative_variance_explained)), cumulative_variance_explained,label = 'Cumulative Explained Variance', color = 'r')\n",
        "plt.axhline(y=95, color='blue', linestyle='--', label='95% Variance Threshold')\n",
        "plt.axvline(x=n_components_95, color='purple', linestyle='--', label=f'{n_components_95} Components explain 95%')\n",
        "plt.legend()\n",
        "\n",
        "# PCA model\n",
        "from sklearn.decomposition import PCA\n",
        "# pca = PCA(0.95) will give the same\n",
        "pca = PCA(n_components = 40)\n",
        "x_pca = pca.fit_transform(x_scaled)\n",
        "\n",
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ml model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# evaluation\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "class_report = classification_report(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n"
      ],
      "metadata": {
        "id": "nB-tVxR28drR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}