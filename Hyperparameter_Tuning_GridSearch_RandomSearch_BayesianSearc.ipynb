{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda9a876-e5a2-46e1-b278-642e4a8442b2",
   "metadata": {},
   "source": [
    "Here are the implementations for **Grid Search, Random Search, and Bayesian Search** for hyperparameter tuning using **Scikit-Learn** and **Optuna**.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **1. Grid Search (Exhaustive Search)**\r\n",
    "Grid Search systematically tests all possible hyperparameter combinations.\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "\r\n",
    "# Load dataset\r\n",
    "X, y = load_iris(return_X_y=True)\r\n",
    "\r\n",
    "# Define model\r\n",
    "model = RandomForestClassifier()\r\n",
    "\r\n",
    "# Define hyperparameter grid\r\n",
    "param_grid = {\r\n",
    "    'n_estimators': [10, 50, 100],\r\n",
    "    'max_depth': [3, 5, 10],\r\n",
    "    'min_samples_split': [2, 5, 10]\r\n",
    "}\r\n",
    "\r\n",
    "# Perform Grid Search\r\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\r\n",
    "grid_search.fit(X, y)\r\n",
    "\r\n",
    "# Best parameters and score\r\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\r\n",
    "print(\"Best Score:\", grid_search.best_score_)\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **2. Random Search (Randomized Parameter Search)**\r\n",
    "Random Search randomly samples hyperparameters instead of testing all combinations.\r\n",
    "\r\n",
    "```python\r\n",
    "from sklearn.model_selection import RandomizedSearchCV\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Define hyperparameter distribution\r\n",
    "param_dist = {\r\n",
    "    'n_estimators': np.arange(10, 200, 10),\r\n",
    "    'max_depth': np.arange(3, 20, 1),\r\n",
    "    'min_samples_split': np.arange(2, 20, 1)\r\n",
    "}\r\n",
    "\r\n",
    "# Perform Random Search\r\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\r\n",
    "random_search.fit(X, y)\r\n",
    "\r\n",
    "# Best parameters and score\r\n",
    "print(\"Best Parameters:\", random_search.best_params_)\r\n",
    "print(\"Best Score:\", random_search.best_score_)\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **3. Bayesian Optimization (Using Optuna)**\r\n",
    "Bayesian Search uses past evaluations to suggest better hyperparameters iteratively.\r\n",
    "\r\n",
    "```python\r\n",
    "import optuna\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "\r\n",
    "# Define objective function\r\n",
    "def objective(trial):\r\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200, step=10)\r\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\r\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\r\n",
    "\r\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\r\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\r\n",
    "    return score\r\n",
    "\r\n",
    "# Perform Bayesian Optimization\r\n",
    "study = optuna.create_study(direction='maximize')\r\n",
    "study.optimize(objective, n_trials=20)\r\n",
    "\r\n",
    "# Best parameters and score\r\n",
    "print(\"Best Parameters:\", study.best_params)\r\n",
    "print(\"Best Score:\", study.best_value)\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Comparison**\r\n",
    "| Method | Exploration | Computation Time | Efficiency |\r\n",
    "|--------|------------|-----------------|------------|\r\n",
    "| **Grid Search** | Exhaustive | High | Inefficient for large spaces |\r\n",
    "| **Random Search** | Random | Medium | Good for large spaces |\r\n",
    "| **Bayesian Search** | Adaptive | Low-Medium | Best for optimizing efficiently |\r\n",
    "\r\n",
    "Let me know if you need any modifications or explanations! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4f798f-164d-4a7c-bf15-395272cad0a8",
   "metadata": {},
   "source": [
    "## **Bayesian Optimization for Hyperparameter Tuning**\r\n",
    "Bayesian Optimization is an advanced method for **hyperparameter tuning** that intelligently explores the search space by learning from past evaluations. Unlike **Grid Search** (which exhaustively tests all possibilities) and **Random Search** (which picks random combinations), Bayesian Optimization **predicts** promising hyperparameter values based on previous results.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Why Bayesian Optimization?**\r\n",
    "Traditional methods like Grid Search and Random Search become inefficient when dealing with high-dimensional hyperparameter spaces. Bayesian Optimization:\r\n",
    "1. **Uses Prior Knowledge** → Learns from past trials to find better hyperparameters.\r\n",
    "2. **Explores Intelligently** → Focuses on promising areas of the search space.\r\n",
    "3. **Faster Convergence** → Requires fewer evaluations compared to brute-force methods.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **How Bayesian Optimization Works**\r\n",
    "Bayesian Optimization follows these key steps:\r\n",
    "\r\n",
    "1. **Define an Objective Function**  \r\n",
    "   - This is the function we want to optimize (e.g., accuracy, F1-score).\r\n",
    "   - The function takes hyperparameters as input and returns a score.\r\n",
    "\r\n",
    "2. **Build a Probabilistic Model (Surrogate Function)**  \r\n",
    "   - Uses a **Gaussian Process (GP)** to estimate the objective function.\r\n",
    "   - This is a statistical model that predicts the score for unseen hyperparameters.\r\n",
    "\r\n",
    "3. **Select the Next Hyperparameter Set (Acquisition Function)**  \r\n",
    "   - Uses an **acquisition function** to decide the next best set of hyperparameters to evaluate.\r\n",
    "   - Balances **exploration** (testing new areas) and **exploitation** (focusing on promising regions).\r\n",
    "\r\n",
    "4. **Update the Model**  \r\n",
    "   - After evaluating the new hyperparameters, the GP model is updated.\r\n",
    "   - This cycle continues until a stopping criterion is met (e.g., max trials reached).\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Implementation Using Optuna**\r\n",
    "Now, let's see **Bayesian Optimization in action** using the **Optuna** library.\r\n",
    "\r\n",
    "### **Step 1: Install Optuna (if not installed)**\r\n",
    "```bash\r\n",
    "pip install optuna\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### **Step 2: Define the Optimization Process**\r\n",
    "We'll optimize a **Random Forest Classifier** using Bayesian Search.\r\n",
    "\r\n",
    "```python\r\n",
    "import optuna\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.model_selection import cross_val_score\r\n",
    "from sklearn.datasets import load_iris\r\n",
    "\r\n",
    "# Load dataset\r\n",
    "X, y = load_iris(return_X_y=True)\r\n",
    "\r\n",
    "# Define the objective function\r\n",
    "def objective(trial):\r\n",
    "    # Define hyperparameter search space\r\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200, step=10)\r\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\r\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\r\n",
    "\r\n",
    "    # Initialize the model with suggested hyperparameters\r\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, \r\n",
    "                                   max_depth=max_depth, \r\n",
    "                                   min_samples_split=min_samples_split)\r\n",
    "\r\n",
    "    # Evaluate model performance using cross-validation\r\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\r\n",
    "    \r\n",
    "    return score  # Higher accuracy is better\r\n",
    "\r\n",
    "# Create a study and optimize the hyperparameters\r\n",
    "study = optuna.create_study(direction='maximize')  # We want to maximize accuracy\r\n",
    "study.optimize(objective, n_trials=20)  # Run for 20 trials\r\n",
    "\r\n",
    "# Print the best hyperparameters and score\r\n",
    "print(\"Best Parameters:\", study.best_params)\r\n",
    "print(\"Best Score:\", study.best_value)\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Explanation of the Code**\r\n",
    "- **`trial.suggest_int('n_estimators', 10, 200, step=10)`**  \r\n",
    "  - The search space for `n_estimators` is between **10 and 200**, with a step size of **10**.\r\n",
    "- **`trial.suggest_int('max_depth', 3, 20)`**  \r\n",
    "  - `max_depth` varies from **3 to 20**.\r\n",
    "- **`trial.suggest_int('min_samples_split', 2, 20)`**  \r\n",
    "  - The number of samples required to split an internal node ranges from **2 to 20**.\r\n",
    "- **`cross_val_score()`**  \r\n",
    "  - Performs **5-fold cross-validation** to get a robust accuracy estimate.\r\n",
    "- **`optuna.create_study(direction='maximize')`**  \r\n",
    "  - We optimize for **maximum accuracy**.\r\n",
    "- **`study.optimize(objective, n_trials=20)`**  \r\n",
    "  - Runs **20 trials**, evaluating different hyperparameter sets.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Visualization of Optimization**\r\n",
    "Optuna provides built-in tools for visualizing the optimization process.\r\n",
    "\r\n",
    "### **Plot the Optimization History**\r\n",
    "```python\r\n",
    "import optuna.visualization as vis\r\n",
    "\r\n",
    "vis.plot_optimization_history(study)\r\n",
    "```\r\n",
    "\r\n",
    "### **Plot Hyperparameter Importances**\r\n",
    "```python\r\n",
    "vis.plot_param_importances(study)\r\n",
    "```\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Comparison with Other Methods**\r\n",
    "| Method | **Exploration Strategy** | **Computational Cost** | **Efficiency** |\r\n",
    "|--------|------------------------|--------------------|-------------|\r\n",
    "| **Grid Search** | Exhaustive | High | Low for large spaces |\r\n",
    "| **Random Search** | Random Sampling | Medium | Good for large spaces |\r\n",
    "| **Bayesian Optimization** | Smart Exploration | Low-Medium | Best for large spaces |\r\n",
    "\r\n",
    "- **Grid Search**: Tests all possible hyperparameter combinations → **Computationally expensive** 🚀🔴  \r\n",
    "- **Random Search**: Randomly picks values → **Faster but still inefficient** ⏳  \r\n",
    "- **Bayesian Optimization**: Learns from previous trials → **Finds the best hyperparameters with fewer trials** ✅🔥  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Advantages of Bayesian Optimization**\r\n",
    "✅ **Faster Convergence** → Finds optimal hyperparameters in fewer iterations.  \r\n",
    "✅ **More Efficient** → Focuses on promising areas instead of wasting trials.  \r\n",
    "✅ **Works Well with Limited Resources** → Reduces unnecessary evaluations.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **When to Use Bayesian Optimization?**\r\n",
    "✔ If your hyperparameter space is **large and continuous**.  \r\n",
    "✔ If **evaluating the model is expensive** (e.g., deep learning).  \r\n",
    "✔ If **you need an optimal model with fewer trials**.  \r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## **Conclusion**\r\n",
    "Bayesian Optimization (via Optuna) **outperforms Grid Search and Random Search** by intelligently selecting hyperparameter values. It is the best choice for complex machine learning models where **computational efficiency matters**.\r\n",
    "\r\n",
    "Would you like to see a **deep learning example** using TensorFlow/Keras? 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1a82bf-4d08-48c2-9050-f689a8a91697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 50}\n",
      "Best Score: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61b127d7-1e15-4df9-bf54-746cd3f22fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 110, 'min_samples_split': 2, 'max_depth': 14}\n",
      "Best Score: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': np.arange(10, 200, 10),\n",
    "    'max_depth': np.arange(3, 20, 1),\n",
    "    'min_samples_split': np.arange(2, 20, 1)\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=20, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28d62e96-be17-4e7a-8ce3-fb97ed635969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-30 10:01:22,152] A new study created in memory with name: no-name-a73410c0-7940-465e-8559-901381399405\n",
      "[I 2025-03-30 10:01:24,088] Trial 0 finished with value: 0.96 and parameters: {'n_estimators': 190, 'max_depth': 15, 'min_samples_split': 5}. Best is trial 0 with value: 0.96.\n",
      "[I 2025-03-30 10:01:25,561] Trial 1 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 150, 'max_depth': 19, 'min_samples_split': 14}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:27,494] Trial 2 finished with value: 0.96 and parameters: {'n_estimators': 180, 'max_depth': 17, 'min_samples_split': 12}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:28,791] Trial 3 finished with value: 0.9533333333333334 and parameters: {'n_estimators': 130, 'max_depth': 15, 'min_samples_split': 5}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:29,980] Trial 4 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 120, 'max_depth': 17, 'min_samples_split': 19}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:31,666] Trial 5 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 170, 'max_depth': 13, 'min_samples_split': 13}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:33,552] Trial 6 finished with value: 0.96 and parameters: {'n_estimators': 190, 'max_depth': 5, 'min_samples_split': 10}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:34,320] Trial 7 finished with value: 0.9533333333333334 and parameters: {'n_estimators': 70, 'max_depth': 20, 'min_samples_split': 15}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:35,149] Trial 8 finished with value: 0.96 and parameters: {'n_estimators': 60, 'max_depth': 15, 'min_samples_split': 10}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:36,735] Trial 9 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 150, 'max_depth': 13, 'min_samples_split': 18}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:37,112] Trial 10 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 30, 'max_depth': 8, 'min_samples_split': 8}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:38,261] Trial 11 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 110, 'max_depth': 20, 'min_samples_split': 20}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:39,314] Trial 12 finished with value: 0.9533333333333334 and parameters: {'n_estimators': 100, 'max_depth': 18, 'min_samples_split': 16}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:41,029] Trial 13 finished with value: 0.96 and parameters: {'n_estimators': 140, 'max_depth': 10, 'min_samples_split': 20}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:42,050] Trial 14 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 100, 'max_depth': 18, 'min_samples_split': 16}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:43,667] Trial 15 finished with value: 0.96 and parameters: {'n_estimators': 160, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:44,927] Trial 16 finished with value: 0.96 and parameters: {'n_estimators': 120, 'max_depth': 10, 'min_samples_split': 14}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:45,788] Trial 17 finished with value: 0.9666666666666668 and parameters: {'n_estimators': 80, 'max_depth': 20, 'min_samples_split': 18}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:45,933] Trial 18 finished with value: 0.96 and parameters: {'n_estimators': 10, 'max_depth': 4, 'min_samples_split': 18}. Best is trial 1 with value: 0.9666666666666668.\n",
      "[I 2025-03-30 10:01:47,431] Trial 19 finished with value: 0.96 and parameters: {'n_estimators': 140, 'max_depth': 13, 'min_samples_split': 17}. Best is trial 1 with value: 0.9666666666666668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'n_estimators': 150, 'max_depth': 19, 'min_samples_split': 14}\n",
      "Best Score: 0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define objective function\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 200, step=10)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
    "    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "    return score\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", study.best_params)\n",
    "print(\"Best Score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac64584-c14b-48af-b7b3-1b1661e0e296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting optuna\n",
      "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in c:\\programdata\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.12 (from alembic>=1.5.0->optuna)\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
      "   ---------------------------------------- 0.0/383.6 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/383.6 kB 1.3 MB/s eta 0:00:01\n",
      "   ------ -------------------------------- 61.4/383.6 kB 812.7 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 102.4/383.6 kB 837.8 kB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 153.6/383.6 kB 1.0 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 184.3/383.6 kB 1.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 225.3/383.6 kB 1.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 266.2/383.6 kB 1.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 317.4/383.6 kB 1.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 368.6/383.6 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  378.9/383.6 kB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- 383.6/383.6 kB 954.8 kB/s eta 0:00:00\n",
      "Downloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "   ---------------------------------------- 0.0/231.9 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/231.9 kB 991.0 kB/s eta 0:00:01\n",
      "   --------------- ------------------------ 92.2/231.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 163.8/231.9 kB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 194.6/231.9 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/231.9 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 225.3/231.9 kB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- 231.9/231.9 kB 886.7 kB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.7 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 41.0/45.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 41.0/45.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 41.0/45.7 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.7/45.7 kB 251.4 kB/s eta 0:00:00\n",
      "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 71.7/78.5 kB 4.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 71.7/78.5 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 78.5/78.5 kB 874.6 kB/s eta 0:00:00\n",
      "Installing collected packages: typing-extensions, Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.9 alembic-1.15.2 colorlog-6.9.0 optuna-4.2.1 typing-extensions-4.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script mako-render.exe is installed in 'C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script alembic.exe is installed in 'C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script optuna.exe is installed in 'C:\\Users\\Dell\\AppData\\Roaming\\Python\\Python312\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa49a26b-1af7-4a14-b074-f7cc925e66b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
